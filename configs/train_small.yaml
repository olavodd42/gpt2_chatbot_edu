project_name: chatbot-edu-gpt2
seed: 42

data:
  train_path: ../data/processed/train.jsonl
  val_path:   ../data/processed/val.jsonl
  block_size: 512

model:
  base_ckpt: gpt2
  add_special_tokens: ["<|user|>", "<|assistant|>"]
  load_in_8bit: True
  device_map: "auto"
  gradient_checkpointing: True
  cache: False

lora:
  enabled: True
  r: 16
  alpha: 32
  dropout: 0.05
  task_type: CAUSAL_LM
  bias: "none"
  target_modules: ["c_attn","c_proj"]

train:
  epochs: 3
  lr: 5e-5
  wd: 0.01
  train_batch_size: 1
  grad_accum_steps: 16
  eval_steps: 500
  save_steps: 500
  dataloader_num_workers: 2
  warmup_ratio: 0.1
  fp16: True
  group_by_length: True

logging:
  report_to: "wandb"          # <- era "none"
  output_dir: ../experiments/runs
  logging_steps: 5
  run_name: "E1-gpt2-lora"    # <- nome do run no W&B (opcional mas útil)
  entity: ""                  # <- seu usuário/org do W&B (opcional; pode deixar vazio e setar via env)
  tags: ["E1","gpt2","lora"]  # <- opcional

infer:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
