100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 205/205 [00:15<00:00, 13.31it/s]
[BASE] eval_loss=85.1189 | ppl=9260842151569193566206983445665546240.00
DEBUG tipos (train): {'train_batch_size': 'int', 'grad_accum_steps': 'int', 'eval_steps': 'int', 'save_steps': 'int', 'dataloader_num_workers': 'int', 'warmup_ratio': 'float', 'lr': 'float', 'wd': 'float', 'fp16': 'bool', 'group_by_length': 'bool'}
DEBUG tipos (logging): {'report_to': 'str', 'output_dir': 'str', 'logging_steps': 'int'}
  0%|                                                                                                                                        | 0/14706 [00:00<?, ?it/s]/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 88.705, 'grad_norm': nan, 'learning_rate': 1.0197144799456154e-07, 'epoch': 0.0}
{'loss': 87.8782, 'grad_norm': 21.053056716918945, 'learning_rate': 2.0394289598912307e-07, 'epoch': 0.0}
{'loss': 84.5261, 'grad_norm': 19.878402709960938, 'learning_rate': 3.738953093133923e-07, 'epoch': 0.0}
{'loss': 82.8576, 'grad_norm': 19.8817138671875, 'learning_rate': 5.438477226376614e-07, 'epoch': 0.0}
{'loss': 79.1361, 'grad_norm': 18.757139205932617, 'learning_rate': 7.138001359619306e-07, 'epoch': 0.01}
{'loss': 76.8511, 'grad_norm': 19.85665512084961, 'learning_rate': 8.837525492861999e-07, 'epoch': 0.01}
{'loss': 76.1722, 'grad_norm': 16.038414001464844, 'learning_rate': 1.053704962610469e-06, 'epoch': 0.01}
{'loss': 75.2365, 'grad_norm': 17.107824325561523, 'learning_rate': 1.2236573759347382e-06, 'epoch': 0.01}
{'loss': 75.0002, 'grad_norm': 16.12984275817871, 'learning_rate': 1.3936097892590076e-06, 'epoch': 0.01}
{'loss': 74.1649, 'grad_norm': 18.927581787109375, 'learning_rate': 1.5635622025832769e-06, 'epoch': 0.01}
{'loss': 89.2188, 'grad_norm': 23.839488983154297, 'learning_rate': 1.733514615907546e-06, 'epoch': 0.01}
{'loss': 89.4913, 'grad_norm': 21.228818893432617, 'learning_rate': 1.9034670292318153e-06, 'epoch': 0.01}
{'loss': 85.5826, 'grad_norm': 19.691509246826172, 'learning_rate': 2.0734194425560847e-06, 'epoch': 0.01}
{'loss': 81.8304, 'grad_norm': 22.240854263305664, 'learning_rate': 2.2433718558803536e-06, 'epoch': 0.01}
{'loss': 78.4155, 'grad_norm': 18.471080780029297, 'learning_rate': 2.413324269204623e-06, 'epoch': 0.02}
{'loss': 76.7262, 'grad_norm': 19.223085403442383, 'learning_rate': 2.5832766825288923e-06, 'epoch': 0.02}
{'loss': 73.9708, 'grad_norm': 17.62273597717285, 'learning_rate': 2.7532290958531616e-06, 'epoch': 0.02}
{'loss': 75.4874, 'grad_norm': 18.15907096862793, 'learning_rate': 2.9231815091774305e-06, 'epoch': 0.02}
{'loss': 75.0335, 'grad_norm': 18.740110397338867, 'learning_rate': 3.0931339225017e-06, 'epoch': 0.02}
{'loss': 72.5956, 'grad_norm': 19.249576568603516, 'learning_rate': 3.263086335825969e-06, 'epoch': 0.02}
{'loss': 88.8873, 'grad_norm': 22.474557876586914, 'learning_rate': 3.433038749150238e-06, 'epoch': 0.02}
{'loss': 85.7729, 'grad_norm': 21.2171688079834, 'learning_rate': 3.6029911624745074e-06, 'epoch': 0.02}
{'loss': 84.1508, 'grad_norm': 21.948875427246094, 'learning_rate': 3.7729435757987763e-06, 'epoch': 0.02}
{'loss': 80.946, 'grad_norm': 24.344148635864258, 'learning_rate': 3.942895989123046e-06, 'epoch': 0.02}
{'loss': 76.4795, 'grad_norm': 24.304868698120117, 'learning_rate': 4.1128484024473146e-06, 'epoch': 0.03}
{'loss': 75.606, 'grad_norm': 23.02046775817871, 'learning_rate': 4.282800815771584e-06, 'epoch': 0.03}
{'loss': 74.9304, 'grad_norm': 22.81097412109375, 'learning_rate': 4.452753229095853e-06, 'epoch': 0.03}
{'loss': 72.147, 'grad_norm': 21.02488899230957, 'learning_rate': 4.622705642420123e-06, 'epoch': 0.03}
{'loss': 72.5174, 'grad_norm': 22.619102478027344, 'learning_rate': 4.792658055744392e-06, 'epoch': 0.03}
{'loss': 70.8553, 'grad_norm': 29.58873748779297, 'learning_rate': 4.962610469068661e-06, 'epoch': 0.03}
{'loss': 84.8307, 'grad_norm': 28.37607765197754, 'learning_rate': 5.13256288239293e-06, 'epoch': 0.03}
{'loss': 83.5381, 'grad_norm': 30.939390182495117, 'learning_rate': 5.3025152957171995e-06, 'epoch': 0.03}
{'loss': 78.9912, 'grad_norm': 30.520532608032227, 'learning_rate': 5.472467709041468e-06, 'epoch': 0.03}
{'loss': 75.9661, 'grad_norm': 32.69453430175781, 'learning_rate': 5.642420122365738e-06, 'epoch': 0.03}
{'loss': 75.5745, 'grad_norm': 32.85432815551758, 'learning_rate': 5.812372535690007e-06, 'epoch': 0.04}
{'loss': 73.5424, 'grad_norm': 30.654735565185547, 'learning_rate': 5.982324949014277e-06, 'epoch': 0.04}
{'loss': 69.6769, 'grad_norm': 31.34101676940918, 'learning_rate': 6.152277362338546e-06, 'epoch': 0.04}
{'loss': 69.8668, 'grad_norm': 33.005130767822266, 'learning_rate': 6.322229775662815e-06, 'epoch': 0.04}
{'loss': 69.1094, 'grad_norm': 30.922801971435547, 'learning_rate': 6.4921821889870835e-06, 'epoch': 0.04}
{'loss': 67.8294, 'grad_norm': 31.70675277709961, 'learning_rate': 6.6621346023113524e-06, 'epoch': 0.04}
{'loss': 78.7537, 'grad_norm': 41.88623809814453, 'learning_rate': 6.832087015635623e-06, 'epoch': 0.04}
{'loss': 76.9729, 'grad_norm': 42.388580322265625, 'learning_rate': 7.002039428959892e-06, 'epoch': 0.04}
{'loss': 74.6225, 'grad_norm': 45.632080078125, 'learning_rate': 7.171991842284161e-06, 'epoch': 0.04}
{'loss': 71.5546, 'grad_norm': 47.791725158691406, 'learning_rate': 7.34194425560843e-06, 'epoch': 0.04}
{'loss': 67.0318, 'grad_norm': 48.080650329589844, 'learning_rate': 7.5118966689326995e-06, 'epoch': 0.05}
{'loss': 62.5713, 'grad_norm': 44.71641540527344, 'learning_rate': 7.68184908225697e-06, 'epoch': 0.05}
{'loss': 64.0256, 'grad_norm': 40.1230583190918, 'learning_rate': 7.851801495581238e-06, 'epoch': 0.05}
{'loss': 62.9607, 'grad_norm': 42.16843032836914, 'learning_rate': 7.987763426240654e-06, 'epoch': 0.05}
{'loss': 61.764, 'grad_norm': 46.60397720336914, 'learning_rate': 8.157715839564923e-06, 'epoch': 0.05}
{'loss': 59.2077, 'grad_norm': 44.15697479248047, 'learning_rate': 8.327668252889192e-06, 'epoch': 0.05}
{'loss': 68.1825, 'grad_norm': 68.32262420654297, 'learning_rate': 8.49762066621346e-06, 'epoch': 0.05}
{'loss': 66.7141, 'grad_norm': 66.69918060302734, 'learning_rate': 8.66757307953773e-06, 'epoch': 0.05}
{'loss': 59.8592, 'grad_norm': 68.48106384277344, 'learning_rate': 8.837525492861999e-06, 'epoch': 0.05}
{'loss': 57.1996, 'grad_norm': 65.86009979248047, 'learning_rate': 9.007477906186267e-06, 'epoch': 0.06}
{'loss': 51.7682, 'grad_norm': 69.921875, 'learning_rate': 9.177430319510536e-06, 'epoch': 0.06}
{'loss': 47.1989, 'grad_norm': 70.03852844238281, 'learning_rate': 9.347382732834807e-06, 'epoch': 0.06}
{'loss': 50.5356, 'grad_norm': 62.906497955322266, 'learning_rate': 9.517335146159076e-06, 'epoch': 0.06}
{'loss': 50.2416, 'grad_norm': 63.244998931884766, 'learning_rate': 9.687287559483345e-06, 'epoch': 0.06}
{'loss': 49.2359, 'grad_norm': 65.65673828125, 'learning_rate': 9.857239972807614e-06, 'epoch': 0.06}
{'loss': 47.4343, 'grad_norm': 60.282039642333984, 'learning_rate': 1.0027192386131884e-05, 'epoch': 0.06}
{'loss': 44.3806, 'grad_norm': 114.28610229492188, 'learning_rate': 1.0197144799456153e-05, 'epoch': 0.06}
{'loss': 38.1232, 'grad_norm': 112.73677825927734, 'learning_rate': 1.0367097212780422e-05, 'epoch': 0.06}
{'loss': 34.5876, 'grad_norm': 94.34059143066406, 'learning_rate': 1.0537049626104691e-05, 'epoch': 0.06}
{'loss': 32.6366, 'grad_norm': 115.04905700683594, 'learning_rate': 1.070700203942896e-05, 'epoch': 0.07}
{'loss': 28.2196, 'grad_norm': 107.37167358398438, 'learning_rate': 1.0876954452753229e-05, 'epoch': 0.07}
{'loss': 29.2258, 'grad_norm': 88.35647583007812, 'learning_rate': 1.1046906866077498e-05, 'epoch': 0.07}
{'loss': 28.8839, 'grad_norm': 90.5987548828125, 'learning_rate': 1.1216859279401767e-05, 'epoch': 0.07}
{'loss': 27.8608, 'grad_norm': 94.80984497070312, 'learning_rate': 1.1352821210061182e-05, 'epoch': 0.07}
{'loss': 26.7341, 'grad_norm': 97.48939514160156, 'learning_rate': 1.1522773623385451e-05, 'epoch': 0.07}
{'loss': 26.6096, 'grad_norm': 108.97573852539062, 'learning_rate': 1.1692726036709722e-05, 'epoch': 0.07}
{'loss': 13.954, 'grad_norm': 85.97958374023438, 'learning_rate': 1.1862678450033991e-05, 'epoch': 0.07}
{'loss': 13.2517, 'grad_norm': 116.37136840820312, 'learning_rate': 1.203263086335826e-05, 'epoch': 0.07}
{'loss': 13.3342, 'grad_norm': 97.82450866699219, 'learning_rate': 1.2202583276682529e-05, 'epoch': 0.07}
{'loss': 11.8234, 'grad_norm': 58.107391357421875, 'learning_rate': 1.23725356900068e-05, 'epoch': 0.08}
{'loss': 11.8213, 'grad_norm': 55.87822723388672, 'learning_rate': 1.2542488103331068e-05, 'epoch': 0.08}
{'loss': 11.865, 'grad_norm': 54.16217041015625, 'learning_rate': 1.2712440516655339e-05, 'epoch': 0.08}
{'loss': 12.4113, 'grad_norm': 49.68141555786133, 'learning_rate': 1.2882392929979606e-05, 'epoch': 0.08}
{'loss': 13.6187, 'grad_norm': 59.391117095947266, 'learning_rate': 1.3052345343303877e-05, 'epoch': 0.08}
{'loss': 11.8483, 'grad_norm': 56.55659484863281, 'learning_rate': 1.3222297756628144e-05, 'epoch': 0.08}
{'loss': 11.4746, 'grad_norm': 35.82514190673828, 'learning_rate': 1.3392250169952414e-05, 'epoch': 0.08}
{'loss': 5.6372, 'grad_norm': 10.27627944946289, 'learning_rate': 1.3562202583276683e-05, 'epoch': 0.08}
{'loss': 6.0902, 'grad_norm': 11.848478317260742, 'learning_rate': 1.3732154996600952e-05, 'epoch': 0.08}
{'loss': 5.8474, 'grad_norm': 9.318279266357422, 'learning_rate': 1.3902107409925221e-05, 'epoch': 0.08}
{'loss': 5.8145, 'grad_norm': 10.528655052185059, 'learning_rate': 1.4072059823249492e-05, 'epoch': 0.09}
{'loss': 6.3315, 'grad_norm': 20.640047073364258, 'learning_rate': 1.4242012236573759e-05, 'epoch': 0.09}
{'loss': 6.9077, 'grad_norm': 22.5700740814209, 'learning_rate': 1.441196464989803e-05, 'epoch': 0.09}
{'loss': 7.6351, 'grad_norm': 17.97286033630371, 'learning_rate': 1.4581917063222297e-05, 'epoch': 0.09}
{'loss': 8.0145, 'grad_norm': 32.48872756958008, 'learning_rate': 1.4751869476546567e-05, 'epoch': 0.09}
{'loss': 7.6487, 'grad_norm': 21.552913665771484, 'learning_rate': 1.4921821889870838e-05, 'epoch': 0.09}
{'loss': 7.9647, 'grad_norm': 46.02519607543945, 'learning_rate': 1.5091774303195105e-05, 'epoch': 0.09}
{'loss': 4.8652, 'grad_norm': 9.382233619689941, 'learning_rate': 1.5261726716519374e-05, 'epoch': 0.09}
{'loss': 5.0014, 'grad_norm': 11.430237770080566, 'learning_rate': 1.5431679129843645e-05, 'epoch': 0.09}
{'loss': 5.2448, 'grad_norm': 4.850231170654297, 'learning_rate': 1.5601631543167912e-05, 'epoch': 0.09}
{'loss': 5.1128, 'grad_norm': 7.237546920776367, 'learning_rate': 1.5771583956492183e-05, 'epoch': 0.1}
{'loss': 5.6519, 'grad_norm': 4.829804420471191, 'learning_rate': 1.594153636981645e-05, 'epoch': 0.1}
{'loss': 5.9015, 'grad_norm': 8.746416091918945, 'learning_rate': 1.611148878314072e-05, 'epoch': 0.1}
{'loss': 6.1869, 'grad_norm': 5.2342305183410645, 'learning_rate': 1.628144119646499e-05, 'epoch': 0.1}
{'loss': 6.8668, 'grad_norm': 5.136504173278809, 'learning_rate': 1.6451393609789258e-05, 'epoch': 0.1}
{'loss': 6.5748, 'grad_norm': 6.001937389373779, 'learning_rate': 1.662134602311353e-05, 'epoch': 0.1}
{'loss': 7.2603, 'grad_norm': 10.927319526672363, 'learning_rate': 1.67912984364378e-05, 'epoch': 0.1}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 7.440108776092529, 'eval_runtime': 12.5804, 'eval_samples_per_second': 129.884, 'eval_steps_per_second': 16.295, 'epoch': 0.1}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 4.3317, 'grad_norm': 3.422048807144165, 'learning_rate': 1.6961250849762067e-05, 'epoch': 0.1}
{'loss': 4.4129, 'grad_norm': 8.009466171264648, 'learning_rate': 1.7131203263086337e-05, 'epoch': 0.1}
{'loss': 4.2773, 'grad_norm': 5.319063663482666, 'learning_rate': 1.7301155676410605e-05, 'epoch': 0.11}
{'loss': 4.7797, 'grad_norm': 3.6145052909851074, 'learning_rate': 1.7471108089734875e-05, 'epoch': 0.11}
{'loss': 4.9372, 'grad_norm': 5.284934043884277, 'learning_rate': 1.7641060503059146e-05, 'epoch': 0.11}
{'loss': 5.2719, 'grad_norm': 4.372105598449707, 'learning_rate': 1.7811012916383413e-05, 'epoch': 0.11}
{'loss': 5.8083, 'grad_norm': 4.470000267028809, 'learning_rate': 1.7980965329707684e-05, 'epoch': 0.11}
{'loss': 5.838, 'grad_norm': 5.121641159057617, 'learning_rate': 1.8150917743031954e-05, 'epoch': 0.11}
{'loss': 6.3298, 'grad_norm': 4.852851867675781, 'learning_rate': 1.832087015635622e-05, 'epoch': 0.11}
{'loss': 6.1987, 'grad_norm': 14.167869567871094, 'learning_rate': 1.8490822569680492e-05, 'epoch': 0.11}
{'loss': 4.1413, 'grad_norm': 9.011024475097656, 'learning_rate': 1.866077498300476e-05, 'epoch': 0.11}
{'loss': 4.1643, 'grad_norm': 4.143851280212402, 'learning_rate': 1.883072739632903e-05, 'epoch': 0.11}
{'loss': 3.9952, 'grad_norm': 9.357696533203125, 'learning_rate': 1.90006798096533e-05, 'epoch': 0.12}
{'loss': 4.2484, 'grad_norm': 3.9205949306488037, 'learning_rate': 1.9170632222977568e-05, 'epoch': 0.12}
{'loss': 4.63, 'grad_norm': 9.597735404968262, 'learning_rate': 1.9340584636301838e-05, 'epoch': 0.12}
{'loss': 4.8775, 'grad_norm': 6.708508014678955, 'learning_rate': 1.9510537049626105e-05, 'epoch': 0.12}
{'loss': 5.3854, 'grad_norm': 4.605306625366211, 'learning_rate': 1.9680489462950376e-05, 'epoch': 0.12}
{'loss': 5.5247, 'grad_norm': 6.602455139160156, 'learning_rate': 1.9850441876274643e-05, 'epoch': 0.12}
{'loss': 5.707, 'grad_norm': 5.841760158538818, 'learning_rate': 2.0020394289598914e-05, 'epoch': 0.12}
{'loss': 5.5981, 'grad_norm': 4.907317638397217, 'learning_rate': 2.019034670292318e-05, 'epoch': 0.12}
{'loss': 3.7513, 'grad_norm': 5.268993854522705, 'learning_rate': 2.036029911624745e-05, 'epoch': 0.12}
{'loss': 3.8065, 'grad_norm': 7.4318928718566895, 'learning_rate': 2.053025152957172e-05, 'epoch': 0.12}
{'loss': 3.8564, 'grad_norm': 2.776442527770996, 'learning_rate': 2.070020394289599e-05, 'epoch': 0.13}
{'loss': 4.1169, 'grad_norm': 10.627806663513184, 'learning_rate': 2.087015635622026e-05, 'epoch': 0.13}
{'loss': 4.2389, 'grad_norm': 4.760598182678223, 'learning_rate': 2.1040108769544527e-05, 'epoch': 0.13}
{'loss': 4.3943, 'grad_norm': 7.907107353210449, 'learning_rate': 2.1210061182868798e-05, 'epoch': 0.13}
{'loss': 4.8544, 'grad_norm': 2.5822997093200684, 'learning_rate': 2.1380013596193065e-05, 'epoch': 0.13}
{'loss': 5.5292, 'grad_norm': 3.9157724380493164, 'learning_rate': 2.1549966009517336e-05, 'epoch': 0.13}
{'loss': 5.3511, 'grad_norm': 11.378535270690918, 'learning_rate': 2.1719918422841606e-05, 'epoch': 0.13}
{'loss': 5.4256, 'grad_norm': 4.67001485824585, 'learning_rate': 2.1889870836165874e-05, 'epoch': 0.13}
{'loss': 3.6686, 'grad_norm': 2.2422428131103516, 'learning_rate': 2.2059823249490144e-05, 'epoch': 0.13}
{'loss': 3.6052, 'grad_norm': 2.578693389892578, 'learning_rate': 2.2229775662814415e-05, 'epoch': 0.13}
{'loss': 3.7182, 'grad_norm': 2.44415020942688, 'learning_rate': 2.2399728076138682e-05, 'epoch': 0.14}
{'loss': 3.834, 'grad_norm': 6.82523250579834, 'learning_rate': 2.2569680489462953e-05, 'epoch': 0.14}
{'loss': 4.2626, 'grad_norm': 5.086855411529541, 'learning_rate': 2.273963290278722e-05, 'epoch': 0.14}
{'loss': 4.2234, 'grad_norm': 6.160099983215332, 'learning_rate': 2.290958531611149e-05, 'epoch': 0.14}
{'loss': 4.7879, 'grad_norm': 6.222545146942139, 'learning_rate': 2.307953772943576e-05, 'epoch': 0.14}
{'loss': 5.0109, 'grad_norm': 22.91529655456543, 'learning_rate': 2.3249490142760028e-05, 'epoch': 0.14}
{'loss': 5.067, 'grad_norm': 4.608329772949219, 'learning_rate': 2.34194425560843e-05, 'epoch': 0.14}
{'loss': 5.1044, 'grad_norm': 5.1887946128845215, 'learning_rate': 2.358939496940857e-05, 'epoch': 0.14}
{'loss': 3.5311, 'grad_norm': 2.1428377628326416, 'learning_rate': 2.3759347382732837e-05, 'epoch': 0.14}
{'loss': 3.5526, 'grad_norm': 2.11330509185791, 'learning_rate': 2.3929299796057107e-05, 'epoch': 0.14}
{'loss': 3.5681, 'grad_norm': 1.9845682382583618, 'learning_rate': 2.4099252209381374e-05, 'epoch': 0.15}
{'loss': 3.7296, 'grad_norm': 3.7291572093963623, 'learning_rate': 2.4269204622705645e-05, 'epoch': 0.15}
{'loss': 3.8901, 'grad_norm': 3.3597326278686523, 'learning_rate': 2.4439157036029912e-05, 'epoch': 0.15}
{'loss': 4.044, 'grad_norm': 4.50559139251709, 'learning_rate': 2.4609109449354183e-05, 'epoch': 0.15}
{'loss': 4.654, 'grad_norm': 2.384276866912842, 'learning_rate': 2.477906186267845e-05, 'epoch': 0.15}
{'loss': 4.9446, 'grad_norm': 3.4090609550476074, 'learning_rate': 2.494901427600272e-05, 'epoch': 0.15}
{'loss': 5.1623, 'grad_norm': 3.54439640045166, 'learning_rate': 2.5118966689326988e-05, 'epoch': 0.15}
{'loss': 4.8505, 'grad_norm': 3.8485186100006104, 'learning_rate': 2.528891910265126e-05, 'epoch': 0.15}
{'loss': 3.4032, 'grad_norm': 2.490727663040161, 'learning_rate': 2.545887151597553e-05, 'epoch': 0.15}
{'loss': 3.4669, 'grad_norm': 2.3563365936279297, 'learning_rate': 2.56288239292998e-05, 'epoch': 0.16}
{'loss': 3.4508, 'grad_norm': 3.8735811710357666, 'learning_rate': 2.5798776342624064e-05, 'epoch': 0.16}
{'loss': 3.7017, 'grad_norm': 2.4855029582977295, 'learning_rate': 2.5968728755948334e-05, 'epoch': 0.16}
{'loss': 3.9042, 'grad_norm': 2.776331663131714, 'learning_rate': 2.6138681169272605e-05, 'epoch': 0.16}
{'loss': 4.1687, 'grad_norm': 2.226611852645874, 'learning_rate': 2.6308633582596875e-05, 'epoch': 0.16}
{'loss': 4.503, 'grad_norm': 3.2019150257110596, 'learning_rate': 2.6478585995921146e-05, 'epoch': 0.16}
{'loss': 4.9038, 'grad_norm': 3.0747759342193604, 'learning_rate': 2.664853840924541e-05, 'epoch': 0.16}
{'loss': 4.8307, 'grad_norm': 7.541593074798584, 'learning_rate': 2.681849082256968e-05, 'epoch': 0.16}
{'loss': 4.4977, 'grad_norm': 3.4135732650756836, 'learning_rate': 2.698844323589395e-05, 'epoch': 0.16}
{'loss': 3.4443, 'grad_norm': 1.3550437688827515, 'learning_rate': 2.715839564921822e-05, 'epoch': 0.16}
{'loss': 3.3478, 'grad_norm': 1.6158593893051147, 'learning_rate': 2.7328348062542492e-05, 'epoch': 0.17}
{'loss': 3.3471, 'grad_norm': 2.081214666366577, 'learning_rate': 2.7498300475866756e-05, 'epoch': 0.17}
{'loss': 3.4974, 'grad_norm': 1.69139564037323, 'learning_rate': 2.7668252889191027e-05, 'epoch': 0.17}
{'loss': 3.5938, 'grad_norm': 1.8524636030197144, 'learning_rate': 2.7838205302515297e-05, 'epoch': 0.17}
{'loss': 3.8537, 'grad_norm': 4.182199954986572, 'learning_rate': 2.8008157715839568e-05, 'epoch': 0.17}
{'loss': 4.4128, 'grad_norm': 2.9546117782592773, 'learning_rate': 2.817811012916384e-05, 'epoch': 0.17}
{'loss': 4.7608, 'grad_norm': 2.8739049434661865, 'learning_rate': 2.834806254248811e-05, 'epoch': 0.17}
{'loss': 4.6322, 'grad_norm': 3.0694899559020996, 'learning_rate': 2.8518014955812373e-05, 'epoch': 0.17}
{'loss': 4.5758, 'grad_norm': 3.3741953372955322, 'learning_rate': 2.8687967369136643e-05, 'epoch': 0.17}
{'loss': 3.2927, 'grad_norm': 3.590888023376465, 'learning_rate': 2.8857919782460914e-05, 'epoch': 0.17}
{'loss': 3.3516, 'grad_norm': 1.122002124786377, 'learning_rate': 2.902787219578518e-05, 'epoch': 0.18}
{'loss': 3.2291, 'grad_norm': 2.3822684288024902, 'learning_rate': 2.9197824609109452e-05, 'epoch': 0.18}
{'loss': 3.3911, 'grad_norm': 1.3910702466964722, 'learning_rate': 2.936777702243372e-05, 'epoch': 0.18}
{'loss': 3.6775, 'grad_norm': 1.662377119064331, 'learning_rate': 2.953772943575799e-05, 'epoch': 0.18}
{'loss': 4.0482, 'grad_norm': 1.8899873495101929, 'learning_rate': 2.9707681849082257e-05, 'epoch': 0.18}
{'loss': 4.2493, 'grad_norm': 2.215226173400879, 'learning_rate': 2.9877634262406528e-05, 'epoch': 0.18}
{'loss': 4.5149, 'grad_norm': 3.0760531425476074, 'learning_rate': 3.0047586675730798e-05, 'epoch': 0.18}
{'loss': 4.4834, 'grad_norm': 3.385650157928467, 'learning_rate': 3.0217539089055062e-05, 'epoch': 0.18}
{'loss': 4.5879, 'grad_norm': 5.062711715698242, 'learning_rate': 3.0387491502379333e-05, 'epoch': 0.18}
{'loss': 3.3345, 'grad_norm': 1.6826937198638916, 'learning_rate': 3.0557443915703607e-05, 'epoch': 0.18}
{'loss': 3.2793, 'grad_norm': 1.81114661693573, 'learning_rate': 3.072739632902788e-05, 'epoch': 0.19}
{'loss': 3.2851, 'grad_norm': 1.8609163761138916, 'learning_rate': 3.089734874235215e-05, 'epoch': 0.19}
{'loss': 3.4264, 'grad_norm': 2.1702399253845215, 'learning_rate': 3.106730115567641e-05, 'epoch': 0.19}
{'loss': 3.5082, 'grad_norm': 1.691402554512024, 'learning_rate': 3.123725356900068e-05, 'epoch': 0.19}
{'loss': 3.7777, 'grad_norm': 2.244243860244751, 'learning_rate': 3.140720598232495e-05, 'epoch': 0.19}
{'loss': 4.1789, 'grad_norm': 2.9279205799102783, 'learning_rate': 3.1577158395649223e-05, 'epoch': 0.19}
{'loss': 4.4469, 'grad_norm': 7.157029151916504, 'learning_rate': 3.174711080897349e-05, 'epoch': 0.19}
{'loss': 4.4917, 'grad_norm': 3.490633487701416, 'learning_rate': 3.191706322229776e-05, 'epoch': 0.19}
{'loss': 4.3459, 'grad_norm': 2.7798807621002197, 'learning_rate': 3.208701563562203e-05, 'epoch': 0.19}
{'loss': 3.2023, 'grad_norm': 1.2035548686981201, 'learning_rate': 3.225696804894629e-05, 'epoch': 0.19}
{'loss': 3.1283, 'grad_norm': 1.0490295886993408, 'learning_rate': 3.242692046227056e-05, 'epoch': 0.2}
{'loss': 3.2943, 'grad_norm': 5.304506778717041, 'learning_rate': 3.2596872875594833e-05, 'epoch': 0.2}
{'loss': 3.5158, 'grad_norm': 2.1033127307891846, 'learning_rate': 3.2766825288919104e-05, 'epoch': 0.2}
{'loss': 3.5883, 'grad_norm': 2.5527215003967285, 'learning_rate': 3.293677770224337e-05, 'epoch': 0.2}
{'loss': 3.6413, 'grad_norm': 3.4058899879455566, 'learning_rate': 3.310673011556764e-05, 'epoch': 0.2}
{'loss': 4.2651, 'grad_norm': 3.2171473503112793, 'learning_rate': 3.327668252889191e-05, 'epoch': 0.2}
{'loss': 4.3885, 'grad_norm': 2.453853130340576, 'learning_rate': 3.344663494221618e-05, 'epoch': 0.2}
{'loss': 4.3482, 'grad_norm': 2.871532440185547, 'learning_rate': 3.361658735554045e-05, 'epoch': 0.2}
{'loss': 4.2101, 'grad_norm': 3.0596721172332764, 'learning_rate': 3.3786539768864714e-05, 'epoch': 0.2}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.6800358295440674, 'eval_runtime': 12.5532, 'eval_samples_per_second': 130.166, 'eval_steps_per_second': 16.33, 'epoch': 0.2}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.2057, 'grad_norm': 1.1292600631713867, 'learning_rate': 3.3956492182188985e-05, 'epoch': 0.21}
{'loss': 3.0755, 'grad_norm': 1.1810612678527832, 'learning_rate': 3.4126444595513255e-05, 'epoch': 0.21}
{'loss': 3.1673, 'grad_norm': 2.1726701259613037, 'learning_rate': 3.4296397008837526e-05, 'epoch': 0.21}
{'loss': 3.3227, 'grad_norm': 1.5429116487503052, 'learning_rate': 3.4466349422161797e-05, 'epoch': 0.21}
{'loss': 3.4837, 'grad_norm': 1.7325156927108765, 'learning_rate': 3.463630183548607e-05, 'epoch': 0.21}
{'loss': 3.9283, 'grad_norm': 1.9836934804916382, 'learning_rate': 3.480625424881033e-05, 'epoch': 0.21}
{'loss': 4.0607, 'grad_norm': 2.1716525554656982, 'learning_rate': 3.49762066621346e-05, 'epoch': 0.21}
{'loss': 4.2349, 'grad_norm': 2.6666252613067627, 'learning_rate': 3.514615907545887e-05, 'epoch': 0.21}
{'loss': 4.1708, 'grad_norm': 2.4128189086914062, 'learning_rate': 3.531611148878314e-05, 'epoch': 0.21}
{'loss': 4.1505, 'grad_norm': 9.72935676574707, 'learning_rate': 3.5486063902107413e-05, 'epoch': 0.21}
{'loss': 3.2082, 'grad_norm': 0.9342060685157776, 'learning_rate': 3.565601631543168e-05, 'epoch': 0.22}
{'loss': 3.1268, 'grad_norm': 1.2281497716903687, 'learning_rate': 3.582596872875595e-05, 'epoch': 0.22}
{'loss': 3.0827, 'grad_norm': 1.2498565912246704, 'learning_rate': 3.599592114208022e-05, 'epoch': 0.22}
{'loss': 3.3809, 'grad_norm': 1.3114193677902222, 'learning_rate': 3.616587355540449e-05, 'epoch': 0.22}
{'loss': 3.1874, 'grad_norm': 1.4026020765304565, 'learning_rate': 3.633582596872876e-05, 'epoch': 0.22}
{'loss': 3.6239, 'grad_norm': 2.0867788791656494, 'learning_rate': 3.6505778382053024e-05, 'epoch': 0.22}
{'loss': 4.0789, 'grad_norm': 2.606879949569702, 'learning_rate': 3.6675730795377294e-05, 'epoch': 0.22}
{'loss': 4.0709, 'grad_norm': 2.262571096420288, 'learning_rate': 3.6845683208701565e-05, 'epoch': 0.22}
{'loss': 4.263, 'grad_norm': 4.1397199630737305, 'learning_rate': 3.7015635622025835e-05, 'epoch': 0.22}
{'loss': 4.226, 'grad_norm': 2.5462472438812256, 'learning_rate': 3.7185588035350106e-05, 'epoch': 0.22}
{'loss': 3.1982, 'grad_norm': 0.9907558560371399, 'learning_rate': 3.7355540448674377e-05, 'epoch': 0.23}
{'loss': 3.0504, 'grad_norm': 1.0283255577087402, 'learning_rate': 3.752549286199864e-05, 'epoch': 0.23}
{'loss': 3.0833, 'grad_norm': 1.8214417695999146, 'learning_rate': 3.769544527532291e-05, 'epoch': 0.23}
{'loss': 3.1842, 'grad_norm': 1.396195888519287, 'learning_rate': 3.786539768864718e-05, 'epoch': 0.23}
{'loss': 3.5516, 'grad_norm': 1.624321460723877, 'learning_rate': 3.803535010197145e-05, 'epoch': 0.23}
{'loss': 3.6868, 'grad_norm': 1.7874884605407715, 'learning_rate': 3.820530251529572e-05, 'epoch': 0.23}
{'loss': 4.0953, 'grad_norm': 4.132045269012451, 'learning_rate': 3.8375254928619987e-05, 'epoch': 0.23}
{'loss': 4.1221, 'grad_norm': 2.4948573112487793, 'learning_rate': 3.854520734194426e-05, 'epoch': 0.23}
{'loss': 4.1911, 'grad_norm': 2.589507818222046, 'learning_rate': 3.871515975526853e-05, 'epoch': 0.23}
{'loss': 4.0827, 'grad_norm': 2.848787546157837, 'learning_rate': 3.88851121685928e-05, 'epoch': 0.23}
{'loss': 3.0674, 'grad_norm': 1.4187043905258179, 'learning_rate': 3.905506458191707e-05, 'epoch': 0.24}
{'loss': 3.0743, 'grad_norm': 2.2532317638397217, 'learning_rate': 3.922501699524133e-05, 'epoch': 0.24}
{'loss': 3.035, 'grad_norm': 1.4157435894012451, 'learning_rate': 3.9394969408565603e-05, 'epoch': 0.24}
{'loss': 3.1935, 'grad_norm': 1.3285504579544067, 'learning_rate': 3.9564921821889874e-05, 'epoch': 0.24}
{'loss': 3.3087, 'grad_norm': 1.4304546117782593, 'learning_rate': 3.9734874235214145e-05, 'epoch': 0.24}
{'loss': 3.6213, 'grad_norm': 1.7298228740692139, 'learning_rate': 3.9904826648538415e-05, 'epoch': 0.24}
{'loss': 4.0192, 'grad_norm': 2.1011929512023926, 'learning_rate': 4.0074779061862686e-05, 'epoch': 0.24}
{'loss': 4.1123, 'grad_norm': 4.7744317054748535, 'learning_rate': 4.024473147518695e-05, 'epoch': 0.24}
{'loss': 4.2455, 'grad_norm': 2.4780008792877197, 'learning_rate': 4.041468388851122e-05, 'epoch': 0.24}
{'loss': 4.0509, 'grad_norm': 2.5898659229278564, 'learning_rate': 4.058463630183549e-05, 'epoch': 0.24}
{'loss': 3.0942, 'grad_norm': 1.6068496704101562, 'learning_rate': 4.075458871515976e-05, 'epoch': 0.25}
{'loss': 2.9994, 'grad_norm': 1.2585315704345703, 'learning_rate': 4.0924541128484025e-05, 'epoch': 0.25}
{'loss': 3.0351, 'grad_norm': 1.0570566654205322, 'learning_rate': 4.1094493541808296e-05, 'epoch': 0.25}
{'loss': 3.0841, 'grad_norm': 1.7680718898773193, 'learning_rate': 4.1264445955132567e-05, 'epoch': 0.25}
{'loss': 3.2094, 'grad_norm': 1.4816120862960815, 'learning_rate': 4.143439836845684e-05, 'epoch': 0.25}
{'loss': 3.6178, 'grad_norm': 1.8739529848098755, 'learning_rate': 4.16043507817811e-05, 'epoch': 0.25}
{'loss': 3.9747, 'grad_norm': 5.482357025146484, 'learning_rate': 4.177430319510537e-05, 'epoch': 0.25}
{'loss': 4.2088, 'grad_norm': 2.574751377105713, 'learning_rate': 4.194425560842964e-05, 'epoch': 0.25}
{'loss': 4.1002, 'grad_norm': 2.316608190536499, 'learning_rate': 4.2114208021753906e-05, 'epoch': 0.25}
{'loss': 4.0887, 'grad_norm': 2.7837724685668945, 'learning_rate': 4.2284160435078177e-05, 'epoch': 0.26}
{'loss': 3.0972, 'grad_norm': 2.047135591506958, 'learning_rate': 4.245411284840245e-05, 'epoch': 0.26}
{'loss': 3.0797, 'grad_norm': 0.9244813323020935, 'learning_rate': 4.262406526172672e-05, 'epoch': 0.26}
{'loss': 3.1284, 'grad_norm': 1.1590158939361572, 'learning_rate': 4.279401767505099e-05, 'epoch': 0.26}
{'loss': 3.0983, 'grad_norm': 1.6982961893081665, 'learning_rate': 4.296397008837525e-05, 'epoch': 0.26}
{'loss': 3.3751, 'grad_norm': 4.9316277503967285, 'learning_rate': 4.313392250169952e-05, 'epoch': 0.26}
{'loss': 3.4435, 'grad_norm': 3.142366647720337, 'learning_rate': 4.3303874915023793e-05, 'epoch': 0.26}
{'loss': 3.9139, 'grad_norm': 2.0156731605529785, 'learning_rate': 4.3473827328348064e-05, 'epoch': 0.26}
{'loss': 4.022, 'grad_norm': 2.5196826457977295, 'learning_rate': 4.3643779741672335e-05, 'epoch': 0.26}
{'loss': 4.0876, 'grad_norm': 2.430975914001465, 'learning_rate': 4.38137321549966e-05, 'epoch': 0.26}
{'loss': 3.9562, 'grad_norm': 2.645885705947876, 'learning_rate': 4.398368456832087e-05, 'epoch': 0.27}
{'loss': 3.1808, 'grad_norm': 1.0138568878173828, 'learning_rate': 4.415363698164514e-05, 'epoch': 0.27}
{'loss': 3.0278, 'grad_norm': 2.495265483856201, 'learning_rate': 4.432358939496941e-05, 'epoch': 0.27}
{'loss': 2.96, 'grad_norm': 1.3790643215179443, 'learning_rate': 4.449354180829368e-05, 'epoch': 0.27}
{'loss': 3.0475, 'grad_norm': 6.006187438964844, 'learning_rate': 4.4663494221617945e-05, 'epoch': 0.27}
{'loss': 3.2266, 'grad_norm': 1.5146443843841553, 'learning_rate': 4.4833446634942215e-05, 'epoch': 0.27}
{'loss': 3.4561, 'grad_norm': 1.7431480884552002, 'learning_rate': 4.5003399048266486e-05, 'epoch': 0.27}
{'loss': 3.7952, 'grad_norm': 2.446104049682617, 'learning_rate': 4.5173351461590757e-05, 'epoch': 0.27}
{'loss': 3.9505, 'grad_norm': 2.9822211265563965, 'learning_rate': 4.534330387491503e-05, 'epoch': 0.27}
{'loss': 4.0966, 'grad_norm': 2.73982310295105, 'learning_rate': 4.55132562882393e-05, 'epoch': 0.27}
{'loss': 4.0127, 'grad_norm': 5.083624839782715, 'learning_rate': 4.568320870156356e-05, 'epoch': 0.28}
{'loss': 3.1208, 'grad_norm': 0.8268083333969116, 'learning_rate': 4.585316111488783e-05, 'epoch': 0.28}
{'loss': 2.962, 'grad_norm': 0.8877442479133606, 'learning_rate': 4.60231135282121e-05, 'epoch': 0.28}
{'loss': 2.9984, 'grad_norm': 1.3490794897079468, 'learning_rate': 4.6193065941536373e-05, 'epoch': 0.28}
{'loss': 3.1451, 'grad_norm': 2.0092573165893555, 'learning_rate': 4.6363018354860644e-05, 'epoch': 0.28}
{'loss': 3.1464, 'grad_norm': 1.6520971059799194, 'learning_rate': 4.653297076818491e-05, 'epoch': 0.28}
{'loss': 3.4866, 'grad_norm': 1.6831284761428833, 'learning_rate': 4.670292318150918e-05, 'epoch': 0.28}
{'loss': 3.9558, 'grad_norm': 2.0424041748046875, 'learning_rate': 4.687287559483345e-05, 'epoch': 0.28}
{'loss': 4.1574, 'grad_norm': 2.1842613220214844, 'learning_rate': 4.704282800815772e-05, 'epoch': 0.28}
{'loss': 3.9503, 'grad_norm': 2.229950428009033, 'learning_rate': 4.721278042148199e-05, 'epoch': 0.28}
{'loss': 3.9923, 'grad_norm': 2.5582637786865234, 'learning_rate': 4.7382732834806254e-05, 'epoch': 0.29}
{'loss': 3.0757, 'grad_norm': 0.7650417685508728, 'learning_rate': 4.7552685248130525e-05, 'epoch': 0.29}
{'loss': 2.979, 'grad_norm': 2.184556245803833, 'learning_rate': 4.7722637661454795e-05, 'epoch': 0.29}
{'loss': 2.9155, 'grad_norm': 1.109447956085205, 'learning_rate': 4.7892590074779066e-05, 'epoch': 0.29}
{'loss': 3.089, 'grad_norm': 10.278468132019043, 'learning_rate': 4.8062542488103336e-05, 'epoch': 0.29}
{'loss': 3.0637, 'grad_norm': 1.2574474811553955, 'learning_rate': 4.823249490142761e-05, 'epoch': 0.29}
{'loss': 3.3625, 'grad_norm': 2.061692476272583, 'learning_rate': 4.840244731475187e-05, 'epoch': 0.29}
{'loss': 3.8461, 'grad_norm': 2.2095959186553955, 'learning_rate': 4.857239972807614e-05, 'epoch': 0.29}
{'loss': 4.0007, 'grad_norm': 2.374359130859375, 'learning_rate': 4.874235214140041e-05, 'epoch': 0.29}
{'loss': 3.868, 'grad_norm': 2.5181281566619873, 'learning_rate': 4.891230455472468e-05, 'epoch': 0.29}
{'loss': 3.7359, 'grad_norm': 2.8402931690216064, 'learning_rate': 4.908225696804895e-05, 'epoch': 0.3}
{'loss': 3.0344, 'grad_norm': 0.8602608442306519, 'learning_rate': 4.925220938137322e-05, 'epoch': 0.3}
{'loss': 3.039, 'grad_norm': 0.8841608762741089, 'learning_rate': 4.942216179469749e-05, 'epoch': 0.3}
{'loss': 2.992, 'grad_norm': 1.572033166885376, 'learning_rate': 4.959211420802176e-05, 'epoch': 0.3}
{'loss': 3.0952, 'grad_norm': 1.4276182651519775, 'learning_rate': 4.976206662134603e-05, 'epoch': 0.3}
{'loss': 3.3, 'grad_norm': 3.3837037086486816, 'learning_rate': 4.99320190346703e-05, 'epoch': 0.3}
{'loss': 3.3916, 'grad_norm': 4.487947463989258, 'learning_rate': 4.999999366123928e-05, 'epoch': 0.3}
{'loss': 3.9638, 'grad_norm': 2.1086490154266357, 'learning_rate': 4.999995492437984e-05, 'epoch': 0.3}
{'loss': 3.8951, 'grad_norm': 2.107173204421997, 'learning_rate': 4.999988097224919e-05, 'epoch': 0.3}
{'loss': 4.0416, 'grad_norm': 2.929407835006714, 'learning_rate': 4.99997718049515e-05, 'epoch': 0.31}
{'loss': 4.0345, 'grad_norm': 3.4387876987457275, 'learning_rate': 4.999962742264055e-05, 'epoch': 0.31}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.3714053630828857, 'eval_runtime': 12.4947, 'eval_samples_per_second': 130.775, 'eval_steps_per_second': 16.407, 'epoch': 0.31}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.9564, 'grad_norm': 0.7188017964363098, 'learning_rate': 4.999944782551971e-05, 'epoch': 0.31}
{'loss': 2.9373, 'grad_norm': 0.8477083444595337, 'learning_rate': 4.999923301384197e-05, 'epoch': 0.31}
{'loss': 3.0009, 'grad_norm': 1.1285938024520874, 'learning_rate': 4.999898298790991e-05, 'epoch': 0.31}
{'loss': 2.9712, 'grad_norm': 1.8550959825515747, 'learning_rate': 4.999869774807573e-05, 'epoch': 0.31}
{'loss': 3.1174, 'grad_norm': 1.5394519567489624, 'learning_rate': 4.999837729474122e-05, 'epoch': 0.31}
{'loss': 3.4779, 'grad_norm': 1.6737589836120605, 'learning_rate': 4.9998021628357774e-05, 'epoch': 0.31}
{'loss': 3.9128, 'grad_norm': 2.0949900150299072, 'learning_rate': 4.999763074942637e-05, 'epoch': 0.31}
{'loss': 3.9449, 'grad_norm': 2.3476994037628174, 'learning_rate': 4.999720465849764e-05, 'epoch': 0.31}
{'loss': 3.9091, 'grad_norm': 2.262805700302124, 'learning_rate': 4.999674335617176e-05, 'epoch': 0.32}
{'loss': 3.7906, 'grad_norm': 2.416318416595459, 'learning_rate': 4.999624684309851e-05, 'epoch': 0.32}
{'loss': 3.1006, 'grad_norm': 2.3824198246002197, 'learning_rate': 4.9995715119977325e-05, 'epoch': 0.32}
{'loss': 2.8457, 'grad_norm': 1.6784584522247314, 'learning_rate': 4.999514818755718e-05, 'epoch': 0.32}
{'loss': 2.8589, 'grad_norm': 1.1940107345581055, 'learning_rate': 4.999454604663665e-05, 'epoch': 0.32}
{'loss': 3.0474, 'grad_norm': 1.1444429159164429, 'learning_rate': 4.999390869806393e-05, 'epoch': 0.32}
{'loss': 3.0544, 'grad_norm': 1.5683650970458984, 'learning_rate': 4.99932361427368e-05, 'epoch': 0.32}
{'loss': 3.4641, 'grad_norm': 1.7481813430786133, 'learning_rate': 4.999252838160263e-05, 'epoch': 0.32}
{'loss': 3.857, 'grad_norm': 2.0813422203063965, 'learning_rate': 4.9991785415658376e-05, 'epoch': 0.32}
{'loss': 3.9381, 'grad_norm': 2.068434000015259, 'learning_rate': 4.9991007245950594e-05, 'epoch': 0.32}
{'loss': 3.8398, 'grad_norm': 2.190469980239868, 'learning_rate': 4.999019387357542e-05, 'epoch': 0.33}
{'loss': 4.0311, 'grad_norm': 2.7368152141571045, 'learning_rate': 4.9989345299678585e-05, 'epoch': 0.33}
{'loss': 2.9997, 'grad_norm': 0.7152958512306213, 'learning_rate': 4.99884615254554e-05, 'epoch': 0.33}
{'loss': 2.7646, 'grad_norm': 0.789802074432373, 'learning_rate': 4.998754255215077e-05, 'epoch': 0.33}
{'loss': 2.8531, 'grad_norm': 0.9208889603614807, 'learning_rate': 4.9986588381059165e-05, 'epoch': 0.33}
{'loss': 3.0775, 'grad_norm': 1.262656807899475, 'learning_rate': 4.998559901352463e-05, 'epoch': 0.33}
{'loss': 3.043, 'grad_norm': 5.820215702056885, 'learning_rate': 4.9984574450940823e-05, 'epoch': 0.33}
{'loss': 3.3079, 'grad_norm': 1.5179334878921509, 'learning_rate': 4.998351469475096e-05, 'epoch': 0.33}
{'loss': 3.6376, 'grad_norm': 2.2842421531677246, 'learning_rate': 4.99824197464478e-05, 'epoch': 0.33}
{'loss': 3.936, 'grad_norm': 5.490171432495117, 'learning_rate': 4.998128960757372e-05, 'epoch': 0.33}
{'loss': 3.8841, 'grad_norm': 2.1455559730529785, 'learning_rate': 4.9980124279720656e-05, 'epoch': 0.34}
{'loss': 3.7943, 'grad_norm': 2.7939939498901367, 'learning_rate': 4.9978923764530095e-05, 'epoch': 0.34}
{'loss': 3.0018, 'grad_norm': 1.2721620798110962, 'learning_rate': 4.997768806369309e-05, 'epoch': 0.34}
{'loss': 2.903, 'grad_norm': 0.8216383457183838, 'learning_rate': 4.997641717895028e-05, 'epoch': 0.34}
{'loss': 2.9918, 'grad_norm': 0.9962595105171204, 'learning_rate': 4.9975111112091846e-05, 'epoch': 0.34}
{'loss': 2.9571, 'grad_norm': 1.062766671180725, 'learning_rate': 4.9973769864957534e-05, 'epoch': 0.34}
{'loss': 3.1515, 'grad_norm': 1.3881908655166626, 'learning_rate': 4.9972393439436624e-05, 'epoch': 0.34}
{'loss': 3.4115, 'grad_norm': 1.4751172065734863, 'learning_rate': 4.9970981837467995e-05, 'epoch': 0.34}
{'loss': 3.7758, 'grad_norm': 1.8848276138305664, 'learning_rate': 4.996953506104003e-05, 'epoch': 0.34}
{'loss': 3.9339, 'grad_norm': 2.0436224937438965, 'learning_rate': 4.996805311219069e-05, 'epoch': 0.34}
{'loss': 3.9219, 'grad_norm': 2.3169972896575928, 'learning_rate': 4.996653599300746e-05, 'epoch': 0.35}
{'loss': 3.8614, 'grad_norm': 2.489147186279297, 'learning_rate': 4.996498370562737e-05, 'epoch': 0.35}
{'loss': 3.026, 'grad_norm': 0.9734664559364319, 'learning_rate': 4.9963396252237005e-05, 'epoch': 0.35}
{'loss': 2.8208, 'grad_norm': 0.9280369281768799, 'learning_rate': 4.996177363507246e-05, 'epoch': 0.35}
{'loss': 2.8963, 'grad_norm': 1.2786897420883179, 'learning_rate': 4.9960115856419384e-05, 'epoch': 0.35}
{'loss': 3.1507, 'grad_norm': 3.0202364921569824, 'learning_rate': 4.995842291861296e-05, 'epoch': 0.35}
{'loss': 3.114, 'grad_norm': 1.4702280759811401, 'learning_rate': 4.9956694824037845e-05, 'epoch': 0.35}
{'loss': 3.2699, 'grad_norm': 1.6175377368927002, 'learning_rate': 4.9954931575128286e-05, 'epoch': 0.35}
{'loss': 3.986, 'grad_norm': 2.5947883129119873, 'learning_rate': 4.995313317436802e-05, 'epoch': 0.35}
{'loss': 3.9363, 'grad_norm': 2.1478612422943115, 'learning_rate': 4.995129962429027e-05, 'epoch': 0.36}
{'loss': 3.971, 'grad_norm': 2.1416358947753906, 'learning_rate': 4.994943092747784e-05, 'epoch': 0.36}
{'loss': 3.8629, 'grad_norm': 5.136025428771973, 'learning_rate': 4.994752708656297e-05, 'epoch': 0.36}
{'loss': 2.9157, 'grad_norm': 0.8025735020637512, 'learning_rate': 4.994558810422745e-05, 'epoch': 0.36}
{'loss': 2.9359, 'grad_norm': 3.6045448780059814, 'learning_rate': 4.994361398320254e-05, 'epoch': 0.36}
{'loss': 2.9827, 'grad_norm': 3.849283456802368, 'learning_rate': 4.994160472626904e-05, 'epoch': 0.36}
{'loss': 2.9952, 'grad_norm': 1.14816415309906, 'learning_rate': 4.9939560336257195e-05, 'epoch': 0.36}
{'loss': 3.1353, 'grad_norm': 1.3272039890289307, 'learning_rate': 4.993748081604677e-05, 'epoch': 0.36}
{'loss': 3.4914, 'grad_norm': 1.7896287441253662, 'learning_rate': 4.9935366168567e-05, 'epoch': 0.36}
{'loss': 3.921, 'grad_norm': 2.109297752380371, 'learning_rate': 4.993321639679661e-05, 'epoch': 0.36}
{'loss': 3.7015, 'grad_norm': 2.1857690811157227, 'learning_rate': 4.993103150376379e-05, 'epoch': 0.37}
{'loss': 3.6359, 'grad_norm': 2.483076810836792, 'learning_rate': 4.992881149254622e-05, 'epoch': 0.37}
{'loss': 3.8043, 'grad_norm': 2.3743910789489746, 'learning_rate': 4.9926556366271036e-05, 'epoch': 0.37}
{'loss': 3.0652, 'grad_norm': 0.7900597453117371, 'learning_rate': 4.992426612811483e-05, 'epoch': 0.37}
{'loss': 2.8172, 'grad_norm': 0.8385236859321594, 'learning_rate': 4.992194078130367e-05, 'epoch': 0.37}
{'loss': 3.0537, 'grad_norm': 1.063664197921753, 'learning_rate': 4.991958032911307e-05, 'epoch': 0.37}
{'loss': 3.0908, 'grad_norm': 1.247226595878601, 'learning_rate': 4.991718477486799e-05, 'epoch': 0.37}
{'loss': 3.2863, 'grad_norm': 1.4515013694763184, 'learning_rate': 4.991475412194283e-05, 'epoch': 0.37}
{'loss': 3.4448, 'grad_norm': 1.610817551612854, 'learning_rate': 4.9912288373761474e-05, 'epoch': 0.37}
{'loss': 3.7059, 'grad_norm': 2.0305655002593994, 'learning_rate': 4.9909787533797174e-05, 'epoch': 0.37}
{'loss': 3.8296, 'grad_norm': 2.017162322998047, 'learning_rate': 4.990725160557266e-05, 'epoch': 0.38}
{'loss': 3.7966, 'grad_norm': 2.054044246673584, 'learning_rate': 4.990468059266008e-05, 'epoch': 0.38}
{'loss': 3.6991, 'grad_norm': 2.5040090084075928, 'learning_rate': 4.9902074498680985e-05, 'epoch': 0.38}
{'loss': 2.967, 'grad_norm': 0.7244731783866882, 'learning_rate': 4.989943332730636e-05, 'epoch': 0.38}
{'loss': 2.8691, 'grad_norm': 0.8542428612709045, 'learning_rate': 4.98967570822566e-05, 'epoch': 0.38}
{'loss': 2.9194, 'grad_norm': 0.9524308443069458, 'learning_rate': 4.989404576730149e-05, 'epoch': 0.38}
{'loss': 3.0906, 'grad_norm': 1.3954566717147827, 'learning_rate': 4.989129938626023e-05, 'epoch': 0.38}
{'loss': 3.1916, 'grad_norm': 1.312841773033142, 'learning_rate': 4.98885179430014e-05, 'epoch': 0.38}
{'loss': 3.2471, 'grad_norm': 1.6298083066940308, 'learning_rate': 4.988570144144299e-05, 'epoch': 0.38}
{'loss': 3.7525, 'grad_norm': 1.989475131034851, 'learning_rate': 4.9882849885552364e-05, 'epoch': 0.38}
{'loss': 3.8279, 'grad_norm': 2.3366057872772217, 'learning_rate': 4.9879963279346244e-05, 'epoch': 0.39}
{'loss': 4.0559, 'grad_norm': 2.3848090171813965, 'learning_rate': 4.987704162689076e-05, 'epoch': 0.39}
{'loss': 3.8142, 'grad_norm': 2.5371649265289307, 'learning_rate': 4.987408493230138e-05, 'epoch': 0.39}
{'loss': 2.9783, 'grad_norm': 0.6526737809181213, 'learning_rate': 4.987109319974296e-05, 'epoch': 0.39}
{'loss': 2.7874, 'grad_norm': 1.5872652530670166, 'learning_rate': 4.9868066433429664e-05, 'epoch': 0.39}
{'loss': 2.8084, 'grad_norm': 0.9238144755363464, 'learning_rate': 4.9865004637625066e-05, 'epoch': 0.39}
{'loss': 3.0973, 'grad_norm': 1.4988856315612793, 'learning_rate': 4.986190781664204e-05, 'epoch': 0.39}
{'loss': 3.1393, 'grad_norm': 2.0486326217651367, 'learning_rate': 4.985877597484281e-05, 'epoch': 0.39}
{'loss': 3.3386, 'grad_norm': 1.712785243988037, 'learning_rate': 4.9855609116638926e-05, 'epoch': 0.39}
{'loss': 3.6274, 'grad_norm': 2.0251364707946777, 'learning_rate': 4.985240724649127e-05, 'epoch': 0.39}
{'loss': 3.8613, 'grad_norm': 2.036149740219116, 'learning_rate': 4.984917036891004e-05, 'epoch': 0.4}
{'loss': 3.7953, 'grad_norm': 2.2501251697540283, 'learning_rate': 4.9845898488454754e-05, 'epoch': 0.4}
{'loss': 3.7295, 'grad_norm': 2.720836877822876, 'learning_rate': 4.9842591609734205e-05, 'epoch': 0.4}
{'loss': 2.9949, 'grad_norm': 0.7027559280395508, 'learning_rate': 4.983924973740653e-05, 'epoch': 0.4}
{'loss': 2.8547, 'grad_norm': 0.877156674861908, 'learning_rate': 4.9835872876179126e-05, 'epoch': 0.4}
{'loss': 2.8941, 'grad_norm': 0.9335431456565857, 'learning_rate': 4.9832461030808676e-05, 'epoch': 0.4}
{'loss': 2.9453, 'grad_norm': 1.8664823770523071, 'learning_rate': 4.982901420610115e-05, 'epoch': 0.4}
{'loss': 3.1216, 'grad_norm': 1.5049176216125488, 'learning_rate': 4.9825532406911816e-05, 'epoch': 0.4}
{'loss': 3.4256, 'grad_norm': 1.6073641777038574, 'learning_rate': 4.982201563814516e-05, 'epoch': 0.4}
{'loss': 3.7814, 'grad_norm': 2.182152271270752, 'learning_rate': 4.9818463904754966e-05, 'epoch': 0.4}
{'loss': 3.8243, 'grad_norm': 2.0248844623565674, 'learning_rate': 4.981487721174424e-05, 'epoch': 0.41}
{'loss': 3.7789, 'grad_norm': 1.958869218826294, 'learning_rate': 4.9811255564165236e-05, 'epoch': 0.41}
{'loss': 3.815, 'grad_norm': 2.093808889389038, 'learning_rate': 4.980759896711949e-05, 'epoch': 0.41}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.263923168182373, 'eval_runtime': 12.456, 'eval_samples_per_second': 131.182, 'eval_steps_per_second': 16.458, 'epoch': 0.41}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.9912, 'grad_norm': 0.7787101864814758, 'learning_rate': 4.9803907425757694e-05, 'epoch': 0.41}
{'loss': 2.8773, 'grad_norm': 1.770081639289856, 'learning_rate': 4.980018094527983e-05, 'epoch': 0.41}
{'loss': 2.8122, 'grad_norm': 0.8582913875579834, 'learning_rate': 4.979641953093506e-05, 'epoch': 0.41}
{'loss': 2.9121, 'grad_norm': 1.0938897132873535, 'learning_rate': 4.9792623188021756e-05, 'epoch': 0.41}
{'loss': 3.2187, 'grad_norm': 1.8469082117080688, 'learning_rate': 4.9788791921887514e-05, 'epoch': 0.41}
{'loss': 3.2573, 'grad_norm': 1.7341125011444092, 'learning_rate': 4.9784925737929086e-05, 'epoch': 0.41}
{'loss': 3.8258, 'grad_norm': 1.9323612451553345, 'learning_rate': 4.978102464159245e-05, 'epoch': 0.42}
{'loss': 3.9459, 'grad_norm': 3.4307868480682373, 'learning_rate': 4.977708863837272e-05, 'epoch': 0.42}
{'loss': 3.8621, 'grad_norm': 2.2035152912139893, 'learning_rate': 4.977311773381422e-05, 'epoch': 0.42}
{'loss': 3.6815, 'grad_norm': 2.3522191047668457, 'learning_rate': 4.976911193351042e-05, 'epoch': 0.42}
{'loss': 3.0579, 'grad_norm': 0.6280657649040222, 'learning_rate': 4.9765071243103934e-05, 'epoch': 0.42}
{'loss': 2.8426, 'grad_norm': 0.7876995801925659, 'learning_rate': 4.9760995668286545e-05, 'epoch': 0.42}
{'loss': 2.8617, 'grad_norm': 1.3534411191940308, 'learning_rate': 4.9756885214799156e-05, 'epoch': 0.42}
{'loss': 2.812, 'grad_norm': 1.0512449741363525, 'learning_rate': 4.97527398884318e-05, 'epoch': 0.42}
{'loss': 3.2052, 'grad_norm': 1.989815354347229, 'learning_rate': 4.974855969502364e-05, 'epoch': 0.42}
{'loss': 3.2346, 'grad_norm': 1.573451280593872, 'learning_rate': 4.974434464046297e-05, 'epoch': 0.42}
{'loss': 3.7339, 'grad_norm': 1.9788844585418701, 'learning_rate': 4.974009473068715e-05, 'epoch': 0.43}
{'loss': 3.7806, 'grad_norm': 1.9672657251358032, 'learning_rate': 4.973580997168267e-05, 'epoch': 0.43}
{'loss': 3.7023, 'grad_norm': 2.2155065536499023, 'learning_rate': 4.97314903694851e-05, 'epoch': 0.43}
{'loss': 3.763, 'grad_norm': 2.4225358963012695, 'learning_rate': 4.972713593017909e-05, 'epoch': 0.43}
{'loss': 2.9821, 'grad_norm': 0.635657012462616, 'learning_rate': 4.972274665989837e-05, 'epoch': 0.43}
{'loss': 2.8516, 'grad_norm': 0.9900002479553223, 'learning_rate': 4.97183225648257e-05, 'epoch': 0.43}
{'loss': 2.9549, 'grad_norm': 0.9438961148262024, 'learning_rate': 4.971386365119294e-05, 'epoch': 0.43}
{'loss': 3.0197, 'grad_norm': 1.1060791015625, 'learning_rate': 4.970936992528096e-05, 'epoch': 0.43}
{'loss': 3.0711, 'grad_norm': 1.3550230264663696, 'learning_rate': 4.9704841393419697e-05, 'epoch': 0.43}
{'loss': 3.283, 'grad_norm': 1.6173707246780396, 'learning_rate': 4.97002780619881e-05, 'epoch': 0.43}
{'loss': 3.7879, 'grad_norm': 2.02091383934021, 'learning_rate': 4.969567993741413e-05, 'epoch': 0.44}
{'loss': 3.7727, 'grad_norm': 2.2176575660705566, 'learning_rate': 4.969104702617477e-05, 'epoch': 0.44}
{'loss': 3.8242, 'grad_norm': 1.9241623878479004, 'learning_rate': 4.9686379334796015e-05, 'epoch': 0.44}
{'loss': 3.8225, 'grad_norm': 2.4009904861450195, 'learning_rate': 4.9681676869852825e-05, 'epoch': 0.44}
{'loss': 3.0408, 'grad_norm': 0.6712176203727722, 'learning_rate': 4.967693963796914e-05, 'epoch': 0.44}
{'loss': 2.8775, 'grad_norm': 0.7541842460632324, 'learning_rate': 4.967216764581792e-05, 'epoch': 0.44}
{'loss': 2.7909, 'grad_norm': 0.9376831650733948, 'learning_rate': 4.966736090012103e-05, 'epoch': 0.44}
{'loss': 2.9117, 'grad_norm': 1.1517846584320068, 'learning_rate': 4.966251940764933e-05, 'epoch': 0.44}
{'loss': 3.0915, 'grad_norm': 2.087851047515869, 'learning_rate': 4.965764317522261e-05, 'epoch': 0.44}
{'loss': 3.3288, 'grad_norm': 1.8061676025390625, 'learning_rate': 4.96527322097096e-05, 'epoch': 0.44}
{'loss': 3.7717, 'grad_norm': 1.9528496265411377, 'learning_rate': 4.964778651802793e-05, 'epoch': 0.45}
{'loss': 3.6539, 'grad_norm': 1.9535353183746338, 'learning_rate': 4.9642806107144194e-05, 'epoch': 0.45}
{'loss': 3.8446, 'grad_norm': 2.0513997077941895, 'learning_rate': 4.963779098407386e-05, 'epoch': 0.45}
{'loss': 3.6843, 'grad_norm': 2.570065498352051, 'learning_rate': 4.9632741155881285e-05, 'epoch': 0.45}
{'loss': 3.0302, 'grad_norm': 0.6681931018829346, 'learning_rate': 4.962765662967973e-05, 'epoch': 0.45}
{'loss': 2.8922, 'grad_norm': 0.7594034671783447, 'learning_rate': 4.962253741263134e-05, 'epoch': 0.45}
{'loss': 2.9308, 'grad_norm': 2.1380465030670166, 'learning_rate': 4.961738351194709e-05, 'epoch': 0.45}
{'loss': 3.0647, 'grad_norm': 1.1523926258087158, 'learning_rate': 4.9612194934886855e-05, 'epoch': 0.45}
{'loss': 3.1058, 'grad_norm': 1.6098533868789673, 'learning_rate': 4.960697168875932e-05, 'epoch': 0.45}
{'loss': 3.2816, 'grad_norm': 1.4383771419525146, 'learning_rate': 4.9601713780922024e-05, 'epoch': 0.45}
{'loss': 3.7379, 'grad_norm': 1.9518043994903564, 'learning_rate': 4.959642121878132e-05, 'epoch': 0.46}
{'loss': 3.7883, 'grad_norm': 2.416990041732788, 'learning_rate': 4.9591094009792396e-05, 'epoch': 0.46}
{'loss': 3.7305, 'grad_norm': 2.3048946857452393, 'learning_rate': 4.958573216145922e-05, 'epoch': 0.46}
{'loss': 3.7709, 'grad_norm': 2.404205560684204, 'learning_rate': 4.9580335681334566e-05, 'epoch': 0.46}
{'loss': 2.9263, 'grad_norm': 0.7044367790222168, 'learning_rate': 4.957490457701999e-05, 'epoch': 0.46}
{'loss': 2.8587, 'grad_norm': 0.8176078796386719, 'learning_rate': 4.95694388561658e-05, 'epoch': 0.46}
{'loss': 2.8388, 'grad_norm': 1.0290340185165405, 'learning_rate': 4.956393852647112e-05, 'epoch': 0.46}
{'loss': 2.9683, 'grad_norm': 1.0506367683410645, 'learning_rate': 4.955840359568376e-05, 'epoch': 0.46}
{'loss': 3.1925, 'grad_norm': 1.3277456760406494, 'learning_rate': 4.9552834071600295e-05, 'epoch': 0.46}
{'loss': 3.3587, 'grad_norm': 2.5456395149230957, 'learning_rate': 4.954722996206606e-05, 'epoch': 0.47}
{'loss': 3.9328, 'grad_norm': 1.873786449432373, 'learning_rate': 4.954159127497504e-05, 'epoch': 0.47}
{'loss': 3.646, 'grad_norm': 1.970720887184143, 'learning_rate': 4.953591801827e-05, 'epoch': 0.47}
{'loss': 3.7336, 'grad_norm': 2.029588460922241, 'learning_rate': 4.953021019994234e-05, 'epoch': 0.47}
{'loss': 3.7686, 'grad_norm': 2.225228786468506, 'learning_rate': 4.952446782803218e-05, 'epoch': 0.47}
{'loss': 2.9196, 'grad_norm': 0.6514212489128113, 'learning_rate': 4.9518690910628306e-05, 'epoch': 0.47}
{'loss': 2.794, 'grad_norm': 0.8723588585853577, 'learning_rate': 4.951287945586816e-05, 'epoch': 0.47}
{'loss': 2.9103, 'grad_norm': 0.9474808573722839, 'learning_rate': 4.9507033471937826e-05, 'epoch': 0.47}
{'loss': 2.9681, 'grad_norm': 1.0731208324432373, 'learning_rate': 4.950115296707204e-05, 'epoch': 0.47}
{'loss': 3.1796, 'grad_norm': 1.4015284776687622, 'learning_rate': 4.949523794955417e-05, 'epoch': 0.47}
{'loss': 3.173, 'grad_norm': 1.4100008010864258, 'learning_rate': 4.948928842771617e-05, 'epoch': 0.48}
{'loss': 3.5958, 'grad_norm': 2.209207057952881, 'learning_rate': 4.948330440993863e-05, 'epoch': 0.48}
{'loss': 3.799, 'grad_norm': 2.0083425045013428, 'learning_rate': 4.947728590465071e-05, 'epoch': 0.48}
{'loss': 4.0478, 'grad_norm': 2.1223268508911133, 'learning_rate': 4.947123292033016e-05, 'epoch': 0.48}
{'loss': 3.7028, 'grad_norm': 2.3683810234069824, 'learning_rate': 4.9465145465503305e-05, 'epoch': 0.48}
{'loss': 3.0277, 'grad_norm': 0.6352224946022034, 'learning_rate': 4.9459023548745e-05, 'epoch': 0.48}
{'loss': 2.8572, 'grad_norm': 0.7492267489433289, 'learning_rate': 4.945286717867866e-05, 'epoch': 0.48}
{'loss': 2.7932, 'grad_norm': 0.8747745156288147, 'learning_rate': 4.9446676363976234e-05, 'epoch': 0.48}
{'loss': 2.9707, 'grad_norm': 1.1788957118988037, 'learning_rate': 4.944045111335819e-05, 'epoch': 0.48}
{'loss': 3.0284, 'grad_norm': 2.6556906700134277, 'learning_rate': 4.94341914355935e-05, 'epoch': 0.48}
{'loss': 3.0841, 'grad_norm': 1.5903071165084839, 'learning_rate': 4.942789733949962e-05, 'epoch': 0.49}
{'loss': 3.5542, 'grad_norm': 2.176635980606079, 'learning_rate': 4.9421568833942514e-05, 'epoch': 0.49}
{'loss': 3.7099, 'grad_norm': 2.180992364883423, 'learning_rate': 4.9415205927836585e-05, 'epoch': 0.49}
{'loss': 3.7477, 'grad_norm': 2.024993419647217, 'learning_rate': 4.940880863014471e-05, 'epoch': 0.49}
{'loss': 3.5567, 'grad_norm': 2.330469846725464, 'learning_rate': 4.940237694987822e-05, 'epoch': 0.49}
{'loss': 2.9056, 'grad_norm': 1.0703922510147095, 'learning_rate': 4.939591089609684e-05, 'epoch': 0.49}
{'loss': 2.8437, 'grad_norm': 0.8684384822845459, 'learning_rate': 4.938941047790877e-05, 'epoch': 0.49}
{'loss': 2.8568, 'grad_norm': 0.9654069542884827, 'learning_rate': 4.9382875704470576e-05, 'epoch': 0.49}
{'loss': 2.9011, 'grad_norm': 1.0183876752853394, 'learning_rate': 4.937630658498722e-05, 'epoch': 0.49}
{'loss': 3.0377, 'grad_norm': 1.3642077445983887, 'learning_rate': 4.9369703128712054e-05, 'epoch': 0.49}
{'loss': 3.3664, 'grad_norm': 1.410327672958374, 'learning_rate': 4.93630653449468e-05, 'epoch': 0.5}
{'loss': 3.5902, 'grad_norm': 2.041741132736206, 'learning_rate': 4.935639324304152e-05, 'epoch': 0.5}
{'loss': 3.8565, 'grad_norm': 2.05829119682312, 'learning_rate': 4.934968683239464e-05, 'epoch': 0.5}
{'loss': 3.6764, 'grad_norm': 2.130560874938965, 'learning_rate': 4.934294612245289e-05, 'epoch': 0.5}
{'loss': 3.8367, 'grad_norm': 2.626716136932373, 'learning_rate': 4.933617112271132e-05, 'epoch': 0.5}
{'loss': 2.9597, 'grad_norm': 0.6038496494293213, 'learning_rate': 4.93293618427133e-05, 'epoch': 0.5}
{'loss': 2.8196, 'grad_norm': 1.514629602432251, 'learning_rate': 4.932251829205046e-05, 'epoch': 0.5}
{'loss': 2.8848, 'grad_norm': 0.9396697282791138, 'learning_rate': 4.931564048036273e-05, 'epoch': 0.5}
{'loss': 2.9102, 'grad_norm': 1.0861350297927856, 'learning_rate': 4.9308728417338264e-05, 'epoch': 0.5}
{'loss': 2.9268, 'grad_norm': 1.2208731174468994, 'learning_rate': 4.9301782112713516e-05, 'epoch': 0.5}
{'loss': 3.0388, 'grad_norm': 1.7273244857788086, 'learning_rate': 4.9294801576273116e-05, 'epoch': 0.51}
{'loss': 3.453, 'grad_norm': 1.6299299001693726, 'learning_rate': 4.928778681784996e-05, 'epoch': 0.51}
{'loss': 3.7339, 'grad_norm': 2.1574668884277344, 'learning_rate': 4.928073784732513e-05, 'epoch': 0.51}
{'loss': 3.7496, 'grad_norm': 2.1121938228607178, 'learning_rate': 4.927365467462789e-05, 'epoch': 0.51}
{'loss': 3.7234, 'grad_norm': 2.561298131942749, 'learning_rate': 4.92665373097357e-05, 'epoch': 0.51}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.1922223567962646, 'eval_runtime': 12.6006, 'eval_samples_per_second': 129.676, 'eval_steps_per_second': 16.269, 'epoch': 0.51}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.8402, 'grad_norm': 0.7263063192367554, 'learning_rate': 4.925938576267417e-05, 'epoch': 0.51}
{'loss': 2.7535, 'grad_norm': 0.7477288246154785, 'learning_rate': 4.925220004351707e-05, 'epoch': 0.51}
{'loss': 2.8941, 'grad_norm': 0.9401690363883972, 'learning_rate': 4.92449801623863e-05, 'epoch': 0.51}
{'loss': 2.9651, 'grad_norm': 1.244489312171936, 'learning_rate': 4.923772612945188e-05, 'epoch': 0.51}
{'loss': 3.1619, 'grad_norm': 5.531866073608398, 'learning_rate': 4.923043795493193e-05, 'epoch': 0.52}
{'loss': 3.3361, 'grad_norm': 1.7358253002166748, 'learning_rate': 4.9223115649092685e-05, 'epoch': 0.52}
{'loss': 3.7014, 'grad_norm': 1.9334207773208618, 'learning_rate': 4.921575922224843e-05, 'epoch': 0.52}
{'loss': 3.7237, 'grad_norm': 1.9438347816467285, 'learning_rate': 4.920836868476153e-05, 'epoch': 0.52}
{'loss': 3.7382, 'grad_norm': 2.0986521244049072, 'learning_rate': 4.92009440470424e-05, 'epoch': 0.52}
{'loss': 3.7171, 'grad_norm': 2.737393856048584, 'learning_rate': 4.919348531954948e-05, 'epoch': 0.52}
{'loss': 3.0283, 'grad_norm': 0.677030622959137, 'learning_rate': 4.918599251278924e-05, 'epoch': 0.52}
{'loss': 2.8171, 'grad_norm': 2.5283243656158447, 'learning_rate': 4.9178465637316136e-05, 'epoch': 0.52}
{'loss': 2.7892, 'grad_norm': 1.1561264991760254, 'learning_rate': 4.9170904703732636e-05, 'epoch': 0.52}
{'loss': 2.7699, 'grad_norm': 1.0979228019714355, 'learning_rate': 4.916330972268916e-05, 'epoch': 0.52}
{'loss': 3.0954, 'grad_norm': 1.6674010753631592, 'learning_rate': 4.915568070488411e-05, 'epoch': 0.53}
{'loss': 3.1205, 'grad_norm': 1.494470477104187, 'learning_rate': 4.9148017661063825e-05, 'epoch': 0.53}
{'loss': 3.7092, 'grad_norm': 1.9634385108947754, 'learning_rate': 4.914032060202256e-05, 'epoch': 0.53}
{'loss': 3.792, 'grad_norm': 2.102932929992676, 'learning_rate': 4.9132589538602516e-05, 'epoch': 0.53}
{'loss': 3.8792, 'grad_norm': 2.1808016300201416, 'learning_rate': 4.9124824481693746e-05, 'epoch': 0.53}
{'loss': 3.8108, 'grad_norm': 3.0990419387817383, 'learning_rate': 4.911702544223423e-05, 'epoch': 0.53}
{'loss': 2.9149, 'grad_norm': 0.6839823126792908, 'learning_rate': 4.9109192431209795e-05, 'epoch': 0.53}
{'loss': 2.7793, 'grad_norm': 0.7209641933441162, 'learning_rate': 4.910132545965412e-05, 'epoch': 0.53}
{'loss': 2.8542, 'grad_norm': 0.8936854600906372, 'learning_rate': 4.9093424538648735e-05, 'epoch': 0.53}
{'loss': 2.8545, 'grad_norm': 1.1108206510543823, 'learning_rate': 4.9085489679322985e-05, 'epoch': 0.53}
{'loss': 3.1309, 'grad_norm': 1.338134765625, 'learning_rate': 4.907752089285401e-05, 'epoch': 0.54}
{'loss': 3.2021, 'grad_norm': 1.6860839128494263, 'learning_rate': 4.906951819046675e-05, 'epoch': 0.54}
{'loss': 3.6314, 'grad_norm': 1.8738298416137695, 'learning_rate': 4.906148158343392e-05, 'epoch': 0.54}
{'loss': 3.808, 'grad_norm': 1.9783852100372314, 'learning_rate': 4.9053411083076006e-05, 'epoch': 0.54}
{'loss': 3.562, 'grad_norm': 2.079349994659424, 'learning_rate': 4.90453067007612e-05, 'epoch': 0.54}
{'loss': 3.6104, 'grad_norm': 3.5007004737854004, 'learning_rate': 4.903716844790546e-05, 'epoch': 0.54}
{'loss': 2.8647, 'grad_norm': 1.9154008626937866, 'learning_rate': 4.902899633597243e-05, 'epoch': 0.54}
{'loss': 2.802, 'grad_norm': 0.7736636996269226, 'learning_rate': 4.902079037647346e-05, 'epoch': 0.54}
{'loss': 2.7323, 'grad_norm': 0.897685170173645, 'learning_rate': 4.901255058096757e-05, 'epoch': 0.54}
{'loss': 2.9823, 'grad_norm': 1.4634634256362915, 'learning_rate': 4.900427696106145e-05, 'epoch': 0.54}
{'loss': 3.0647, 'grad_norm': 1.346622109413147, 'learning_rate': 4.8995969528409434e-05, 'epoch': 0.55}
{'loss': 3.3888, 'grad_norm': 1.619271993637085, 'learning_rate': 4.898762829471348e-05, 'epoch': 0.55}
{'loss': 3.5977, 'grad_norm': 1.8791544437408447, 'learning_rate': 4.897925327172316e-05, 'epoch': 0.55}
{'loss': 3.6842, 'grad_norm': 2.4075729846954346, 'learning_rate': 4.897084447123564e-05, 'epoch': 0.55}
{'loss': 3.7508, 'grad_norm': 1.9559251070022583, 'learning_rate': 4.896240190509569e-05, 'epoch': 0.55}
{'loss': 3.8448, 'grad_norm': 2.7713537216186523, 'learning_rate': 4.895392558519559e-05, 'epoch': 0.55}
{'loss': 2.8409, 'grad_norm': 0.6629713177680969, 'learning_rate': 4.894541552347521e-05, 'epoch': 0.55}
{'loss': 2.8372, 'grad_norm': 0.7855944037437439, 'learning_rate': 4.893687173192195e-05, 'epoch': 0.55}
{'loss': 2.8511, 'grad_norm': 0.9301161766052246, 'learning_rate': 4.89282942225707e-05, 'epoch': 0.55}
{'loss': 2.8683, 'grad_norm': 1.2463310956954956, 'learning_rate': 4.891968300750385e-05, 'epoch': 0.55}
{'loss': 3.1175, 'grad_norm': 1.496172308921814, 'learning_rate': 4.891103809885128e-05, 'epoch': 0.56}
{'loss': 3.3162, 'grad_norm': 1.802823781967163, 'learning_rate': 4.890235950879032e-05, 'epoch': 0.56}
{'loss': 3.5105, 'grad_norm': 1.8561822175979614, 'learning_rate': 4.889364724954574e-05, 'epoch': 0.56}
{'loss': 3.7387, 'grad_norm': 1.989076018333435, 'learning_rate': 4.888490133338977e-05, 'epoch': 0.56}
{'loss': 3.598, 'grad_norm': 2.1816625595092773, 'learning_rate': 4.887612177264199e-05, 'epoch': 0.56}
{'loss': 3.742, 'grad_norm': 2.330615758895874, 'learning_rate': 4.886730857966944e-05, 'epoch': 0.56}
{'loss': 2.9515, 'grad_norm': 0.6746682524681091, 'learning_rate': 4.885846176688648e-05, 'epoch': 0.56}
{'loss': 2.8808, 'grad_norm': 0.747062623500824, 'learning_rate': 4.884958134675486e-05, 'epoch': 0.56}
{'loss': 2.8947, 'grad_norm': 0.9031075239181519, 'learning_rate': 4.884066733178365e-05, 'epoch': 0.56}
{'loss': 2.9447, 'grad_norm': 1.768028974533081, 'learning_rate': 4.883171973452926e-05, 'epoch': 0.57}
{'loss': 2.9556, 'grad_norm': 1.235521674156189, 'learning_rate': 4.882273856759538e-05, 'epoch': 0.57}
{'loss': 3.1601, 'grad_norm': 1.6053211688995361, 'learning_rate': 4.881372384363302e-05, 'epoch': 0.57}
{'loss': 3.607, 'grad_norm': 2.0190248489379883, 'learning_rate': 4.8804675575340437e-05, 'epoch': 0.57}
{'loss': 3.7152, 'grad_norm': 2.1878302097320557, 'learning_rate': 4.8795593775463134e-05, 'epoch': 0.57}
{'loss': 3.5484, 'grad_norm': 2.6063718795776367, 'learning_rate': 4.878647845679386e-05, 'epoch': 0.57}
{'loss': 3.768, 'grad_norm': 2.9043378829956055, 'learning_rate': 4.877732963217259e-05, 'epoch': 0.57}
{'loss': 2.9526, 'grad_norm': 0.6058057546615601, 'learning_rate': 4.8768147314486454e-05, 'epoch': 0.57}
{'loss': 2.7604, 'grad_norm': 0.7785670757293701, 'learning_rate': 4.87589315166698e-05, 'epoch': 0.57}
{'loss': 2.7716, 'grad_norm': 0.9193582534790039, 'learning_rate': 4.8749682251704123e-05, 'epoch': 0.57}
{'loss': 2.8511, 'grad_norm': 1.0157467126846313, 'learning_rate': 4.874039953261807e-05, 'epoch': 0.58}
{'loss': 3.0139, 'grad_norm': 1.417785406112671, 'learning_rate': 4.87310833724874e-05, 'epoch': 0.58}
{'loss': 3.1073, 'grad_norm': 1.5900256633758545, 'learning_rate': 4.8721733784434964e-05, 'epoch': 0.58}
{'loss': 3.6914, 'grad_norm': 1.8941313028335571, 'learning_rate': 4.871235078163074e-05, 'epoch': 0.58}
{'loss': 3.7999, 'grad_norm': 1.9521760940551758, 'learning_rate': 4.870293437729174e-05, 'epoch': 0.58}
{'loss': 3.5327, 'grad_norm': 2.0625147819519043, 'learning_rate': 4.869348458468204e-05, 'epoch': 0.58}
{'loss': 3.5892, 'grad_norm': 2.7923004627227783, 'learning_rate': 4.868400141711275e-05, 'epoch': 0.58}
{'loss': 2.8814, 'grad_norm': 0.57940274477005, 'learning_rate': 4.867448488794196e-05, 'epoch': 0.58}
{'loss': 2.7841, 'grad_norm': 0.8122175335884094, 'learning_rate': 4.866493501057482e-05, 'epoch': 0.58}
{'loss': 2.8804, 'grad_norm': 0.8692875504493713, 'learning_rate': 4.865535179846339e-05, 'epoch': 0.58}
{'loss': 2.8592, 'grad_norm': 1.118024230003357, 'learning_rate': 4.864573526510671e-05, 'epoch': 0.59}
{'loss': 2.9775, 'grad_norm': 1.2685470581054688, 'learning_rate': 4.863608542405077e-05, 'epoch': 0.59}
{'loss': 3.124, 'grad_norm': 1.9557931423187256, 'learning_rate': 4.862640228888846e-05, 'epoch': 0.59}
{'loss': 3.3979, 'grad_norm': 2.052368402481079, 'learning_rate': 4.861668587325957e-05, 'epoch': 0.59}
{'loss': 3.7112, 'grad_norm': 2.037809371948242, 'learning_rate': 4.860693619085078e-05, 'epoch': 0.59}
{'loss': 3.8705, 'grad_norm': 1.9887217283248901, 'learning_rate': 4.859715325539562e-05, 'epoch': 0.59}
{'loss': 3.6072, 'grad_norm': 2.04911732673645, 'learning_rate': 4.858733708067446e-05, 'epoch': 0.59}
{'loss': 2.9932, 'grad_norm': 0.5663296580314636, 'learning_rate': 4.857748768051451e-05, 'epoch': 0.59}
{'loss': 2.7684, 'grad_norm': 0.7046048045158386, 'learning_rate': 4.856760506878975e-05, 'epoch': 0.59}
{'loss': 2.8346, 'grad_norm': 0.9163241386413574, 'learning_rate': 4.855768925942096e-05, 'epoch': 0.59}
{'loss': 2.9507, 'grad_norm': 1.201330542564392, 'learning_rate': 4.854774026637568e-05, 'epoch': 0.6}
{'loss': 2.9576, 'grad_norm': 1.2012766599655151, 'learning_rate': 4.8537758103668206e-05, 'epoch': 0.6}
{'loss': 3.1242, 'grad_norm': 1.6191385984420776, 'learning_rate': 4.852774278535955e-05, 'epoch': 0.6}
{'loss': 3.6028, 'grad_norm': 1.8359801769256592, 'learning_rate': 4.8517694325557406e-05, 'epoch': 0.6}
{'loss': 3.5533, 'grad_norm': 2.2867844104766846, 'learning_rate': 4.850761273841619e-05, 'epoch': 0.6}
{'loss': 3.8245, 'grad_norm': 1.9872294664382935, 'learning_rate': 4.8497498038136936e-05, 'epoch': 0.6}
{'loss': 3.7355, 'grad_norm': 2.3485419750213623, 'learning_rate': 4.848735023896736e-05, 'epoch': 0.6}
{'loss': 3.055, 'grad_norm': 0.6520758271217346, 'learning_rate': 4.847716935520179e-05, 'epoch': 0.6}
{'loss': 2.7732, 'grad_norm': 1.7689186334609985, 'learning_rate': 4.846695540118115e-05, 'epoch': 0.6}
{'loss': 2.8813, 'grad_norm': 0.7794094085693359, 'learning_rate': 4.845670839129296e-05, 'epoch': 0.6}
{'loss': 2.9471, 'grad_norm': 1.093864917755127, 'learning_rate': 4.844642833997128e-05, 'epoch': 0.61}
{'loss': 2.9581, 'grad_norm': 1.2856990098953247, 'learning_rate': 4.8436115261696734e-05, 'epoch': 0.61}
{'loss': 3.2098, 'grad_norm': 1.7411751747131348, 'learning_rate': 4.8425769170996485e-05, 'epoch': 0.61}
{'loss': 3.3839, 'grad_norm': 1.8874995708465576, 'learning_rate': 4.841539008244413e-05, 'epoch': 0.61}
{'loss': 3.7858, 'grad_norm': 3.17751407623291, 'learning_rate': 4.840497801065984e-05, 'epoch': 0.61}
{'loss': 3.7042, 'grad_norm': 2.2293882369995117, 'learning_rate': 4.839453297031016e-05, 'epoch': 0.61}
{'loss': 3.6047, 'grad_norm': 2.637516736984253, 'learning_rate': 4.838405497610814e-05, 'epoch': 0.61}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.1592161655426025, 'eval_runtime': 12.6749, 'eval_samples_per_second': 128.916, 'eval_steps_per_second': 16.174, 'epoch': 0.61}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.8618, 'grad_norm': 0.6237866878509521, 'learning_rate': 4.837354404281321e-05, 'epoch': 0.61}
{'loss': 2.7752, 'grad_norm': 0.7649616599082947, 'learning_rate': 4.836300018523121e-05, 'epoch': 0.61}
{'loss': 2.7937, 'grad_norm': 0.8386452198028564, 'learning_rate': 4.8352423418214363e-05, 'epoch': 0.62}
{'loss': 2.9385, 'grad_norm': 1.1409330368041992, 'learning_rate': 4.8341813756661255e-05, 'epoch': 0.62}
{'loss': 3.0718, 'grad_norm': 1.319663643836975, 'learning_rate': 4.833117121551678e-05, 'epoch': 0.62}
{'loss': 3.1453, 'grad_norm': 1.5963196754455566, 'learning_rate': 4.832049580977217e-05, 'epoch': 0.62}
{'loss': 3.6303, 'grad_norm': 1.9944871664047241, 'learning_rate': 4.830978755446495e-05, 'epoch': 0.62}
{'loss': 3.6674, 'grad_norm': 1.9429184198379517, 'learning_rate': 4.82990464646789e-05, 'epoch': 0.62}
{'loss': 3.6318, 'grad_norm': 2.2506895065307617, 'learning_rate': 4.828827255554407e-05, 'epoch': 0.62}
{'loss': 3.5947, 'grad_norm': 2.786362886428833, 'learning_rate': 4.827746584223673e-05, 'epoch': 0.62}
{'loss': 2.95, 'grad_norm': 0.6154093146324158, 'learning_rate': 4.826662633997937e-05, 'epoch': 0.62}
{'loss': 2.6638, 'grad_norm': 0.8338093161582947, 'learning_rate': 4.825575406404066e-05, 'epoch': 0.62}
{'loss': 2.8264, 'grad_norm': 0.9666579961776733, 'learning_rate': 4.8244849029735413e-05, 'epoch': 0.63}
{'loss': 2.9667, 'grad_norm': 1.2219033241271973, 'learning_rate': 4.823391125242462e-05, 'epoch': 0.63}
{'loss': 3.0792, 'grad_norm': 1.4311898946762085, 'learning_rate': 4.8222940747515376e-05, 'epoch': 0.63}
{'loss': 3.142, 'grad_norm': 1.612226963043213, 'learning_rate': 4.8211937530460895e-05, 'epoch': 0.63}
{'loss': 3.7179, 'grad_norm': 3.433901786804199, 'learning_rate': 4.820090161676044e-05, 'epoch': 0.63}
{'loss': 3.6716, 'grad_norm': 2.2565836906433105, 'learning_rate': 4.8189833021959344e-05, 'epoch': 0.63}
{'loss': 3.6489, 'grad_norm': 2.0594396591186523, 'learning_rate': 4.8178731761648995e-05, 'epoch': 0.63}
{'loss': 3.5869, 'grad_norm': 2.164430618286133, 'learning_rate': 4.816759785146676e-05, 'epoch': 0.63}
{'loss': 2.8491, 'grad_norm': 0.6384804844856262, 'learning_rate': 4.815643130709603e-05, 'epoch': 0.63}
{'loss': 2.8151, 'grad_norm': 0.7278299927711487, 'learning_rate': 4.814523214426614e-05, 'epoch': 0.63}
{'loss': 2.8227, 'grad_norm': 0.861382246017456, 'learning_rate': 4.813400037875238e-05, 'epoch': 0.64}
{'loss': 2.7638, 'grad_norm': 1.0412694215774536, 'learning_rate': 4.812273602637596e-05, 'epoch': 0.64}
{'loss': 3.0549, 'grad_norm': 1.372894048690796, 'learning_rate': 4.8111439103004006e-05, 'epoch': 0.64}
{'loss': 3.2835, 'grad_norm': 1.6398831605911255, 'learning_rate': 4.810010962454951e-05, 'epoch': 0.64}
{'loss': 3.6129, 'grad_norm': 2.197343587875366, 'learning_rate': 4.808874760697134e-05, 'epoch': 0.64}
{'loss': 3.8193, 'grad_norm': 2.10314679145813, 'learning_rate': 4.807735306627417e-05, 'epoch': 0.64}
{'loss': 3.7657, 'grad_norm': 2.1335558891296387, 'learning_rate': 4.806592601850851e-05, 'epoch': 0.64}
{'loss': 3.607, 'grad_norm': 2.547731876373291, 'learning_rate': 4.8054466479770655e-05, 'epoch': 0.64}
{'loss': 3.0297, 'grad_norm': 0.6538695693016052, 'learning_rate': 4.8042974466202654e-05, 'epoch': 0.64}
{'loss': 2.7489, 'grad_norm': 0.7438532710075378, 'learning_rate': 4.8031449993992326e-05, 'epoch': 0.64}
{'loss': 2.8017, 'grad_norm': 0.8807994723320007, 'learning_rate': 4.8019893079373186e-05, 'epoch': 0.65}
{'loss': 2.8132, 'grad_norm': 1.0771808624267578, 'learning_rate': 4.800830373862446e-05, 'epoch': 0.65}
{'loss': 2.9975, 'grad_norm': 1.6925359964370728, 'learning_rate': 4.799668198807105e-05, 'epoch': 0.65}
{'loss': 3.0653, 'grad_norm': 1.7409534454345703, 'learning_rate': 4.7985027844083514e-05, 'epoch': 0.65}
{'loss': 3.429, 'grad_norm': 1.8444011211395264, 'learning_rate': 4.7973341323078026e-05, 'epoch': 0.65}
{'loss': 3.6561, 'grad_norm': 2.190192937850952, 'learning_rate': 4.796162244151638e-05, 'epoch': 0.65}
{'loss': 3.6004, 'grad_norm': 2.225855827331543, 'learning_rate': 4.794987121590596e-05, 'epoch': 0.65}
{'loss': 3.6749, 'grad_norm': 2.3749516010284424, 'learning_rate': 4.793808766279969e-05, 'epoch': 0.65}
{'loss': 3.0267, 'grad_norm': 0.6295204162597656, 'learning_rate': 4.792627179879603e-05, 'epoch': 0.65}
{'loss': 2.8371, 'grad_norm': 0.7149284482002258, 'learning_rate': 4.7914423640538986e-05, 'epoch': 0.65}
{'loss': 2.8393, 'grad_norm': 0.8699694871902466, 'learning_rate': 4.7902543204718006e-05, 'epoch': 0.66}
{'loss': 2.8445, 'grad_norm': 5.130504608154297, 'learning_rate': 4.789063050806806e-05, 'epoch': 0.66}
{'loss': 2.8979, 'grad_norm': 1.3512918949127197, 'learning_rate': 4.787868556736952e-05, 'epoch': 0.66}
{'loss': 3.1608, 'grad_norm': 1.595682978630066, 'learning_rate': 4.786670839944818e-05, 'epoch': 0.66}
{'loss': 3.6879, 'grad_norm': 1.9127449989318848, 'learning_rate': 4.785469902117525e-05, 'epoch': 0.66}
{'loss': 3.7417, 'grad_norm': 2.284026861190796, 'learning_rate': 4.78426574494673e-05, 'epoch': 0.66}
{'loss': 3.7025, 'grad_norm': 2.0407259464263916, 'learning_rate': 4.783058370128624e-05, 'epoch': 0.66}
{'loss': 3.4754, 'grad_norm': 2.7262954711914062, 'learning_rate': 4.7818477793639324e-05, 'epoch': 0.66}
{'loss': 2.9178, 'grad_norm': 0.6027998328208923, 'learning_rate': 4.78063397435791e-05, 'epoch': 0.66}
{'loss': 2.822, 'grad_norm': 0.761732816696167, 'learning_rate': 4.779416956820337e-05, 'epoch': 0.67}
{'loss': 2.7048, 'grad_norm': 0.8654400706291199, 'learning_rate': 4.7781967284655214e-05, 'epoch': 0.67}
{'loss': 2.8153, 'grad_norm': 1.1833813190460205, 'learning_rate': 4.776973291012294e-05, 'epoch': 0.67}
{'loss': 2.9853, 'grad_norm': 1.3141592741012573, 'learning_rate': 4.775746646184004e-05, 'epoch': 0.67}
{'loss': 2.9723, 'grad_norm': 1.4877007007598877, 'learning_rate': 4.774516795708521e-05, 'epoch': 0.67}
{'loss': 3.4376, 'grad_norm': 1.9913458824157715, 'learning_rate': 4.773283741318227e-05, 'epoch': 0.67}
{'loss': 3.7197, 'grad_norm': 1.9316496849060059, 'learning_rate': 4.772047484750021e-05, 'epoch': 0.67}
{'loss': 3.6729, 'grad_norm': 1.912162184715271, 'learning_rate': 4.770808027745309e-05, 'epoch': 0.67}
{'loss': 3.6892, 'grad_norm': 2.4860920906066895, 'learning_rate': 4.769565372050008e-05, 'epoch': 0.67}
{'loss': 2.8934, 'grad_norm': 0.6121442914009094, 'learning_rate': 4.7683195194145383e-05, 'epoch': 0.67}
{'loss': 2.6085, 'grad_norm': 2.930156946182251, 'learning_rate': 4.767070471593825e-05, 'epoch': 0.68}
{'loss': 2.8457, 'grad_norm': 0.8494144678115845, 'learning_rate': 4.765818230347294e-05, 'epoch': 0.68}
{'loss': 2.8629, 'grad_norm': 1.065671443939209, 'learning_rate': 4.764562797438869e-05, 'epoch': 0.68}
{'loss': 2.9706, 'grad_norm': 1.244216799736023, 'learning_rate': 4.763304174636969e-05, 'epoch': 0.68}
{'loss': 3.1747, 'grad_norm': 1.6892657279968262, 'learning_rate': 4.762042363714507e-05, 'epoch': 0.68}
{'loss': 3.5908, 'grad_norm': 1.8298940658569336, 'learning_rate': 4.760777366448888e-05, 'epoch': 0.68}
{'loss': 3.7434, 'grad_norm': 1.7859302759170532, 'learning_rate': 4.759509184622002e-05, 'epoch': 0.68}
{'loss': 3.5784, 'grad_norm': 2.047874689102173, 'learning_rate': 4.758237820020228e-05, 'epoch': 0.68}
{'loss': 3.6351, 'grad_norm': 2.7096312046051025, 'learning_rate': 4.756963274434428e-05, 'epoch': 0.68}
{'loss': 2.9577, 'grad_norm': 0.6505510210990906, 'learning_rate': 4.755685549659943e-05, 'epoch': 0.68}
{'loss': 2.766, 'grad_norm': 1.06242835521698, 'learning_rate': 4.754404647496593e-05, 'epoch': 0.69}
{'loss': 2.7349, 'grad_norm': 0.8109351992607117, 'learning_rate': 4.753120569748675e-05, 'epoch': 0.69}
{'loss': 2.7371, 'grad_norm': 1.0390599966049194, 'learning_rate': 4.7518333182249566e-05, 'epoch': 0.69}
{'loss': 2.9084, 'grad_norm': 1.5306793451309204, 'learning_rate': 4.7505428947386786e-05, 'epoch': 0.69}
{'loss': 3.3379, 'grad_norm': 1.7377272844314575, 'learning_rate': 4.749249301107549e-05, 'epoch': 0.69}
{'loss': 3.6146, 'grad_norm': 2.0067193508148193, 'learning_rate': 4.74795253915374e-05, 'epoch': 0.69}
{'loss': 3.8239, 'grad_norm': 2.0507726669311523, 'learning_rate': 4.746652610703889e-05, 'epoch': 0.69}
{'loss': 3.8164, 'grad_norm': 2.253537654876709, 'learning_rate': 4.745349517589093e-05, 'epoch': 0.69}
{'loss': 3.5454, 'grad_norm': 2.768692970275879, 'learning_rate': 4.7440432616449046e-05, 'epoch': 0.69}
{'loss': 2.9316, 'grad_norm': 0.5834919810295105, 'learning_rate': 4.7427338447113344e-05, 'epoch': 0.69}
{'loss': 2.7579, 'grad_norm': 0.7312009930610657, 'learning_rate': 4.7414212686328446e-05, 'epoch': 0.7}
{'loss': 2.8095, 'grad_norm': 0.8336912393569946, 'learning_rate': 4.740105535258348e-05, 'epoch': 0.7}
{'loss': 2.8105, 'grad_norm': 1.0260164737701416, 'learning_rate': 4.738786646441203e-05, 'epoch': 0.7}
{'loss': 3.0444, 'grad_norm': 1.3611723184585571, 'learning_rate': 4.7374646040392146e-05, 'epoch': 0.7}
{'loss': 3.0178, 'grad_norm': 1.5928521156311035, 'learning_rate': 4.7361394099146305e-05, 'epoch': 0.7}
{'loss': 3.5172, 'grad_norm': 1.9285222291946411, 'learning_rate': 4.734811065934135e-05, 'epoch': 0.7}
{'loss': 3.7293, 'grad_norm': 2.4512338638305664, 'learning_rate': 4.733479573968853e-05, 'epoch': 0.7}
{'loss': 3.6788, 'grad_norm': 2.150547981262207, 'learning_rate': 4.73214493589434e-05, 'epoch': 0.7}
{'loss': 3.5758, 'grad_norm': 2.5417892932891846, 'learning_rate': 4.7308071535905876e-05, 'epoch': 0.7}
{'loss': 2.8529, 'grad_norm': 0.7070690393447876, 'learning_rate': 4.729466228942011e-05, 'epoch': 0.7}
{'loss': 2.793, 'grad_norm': 0.8265367746353149, 'learning_rate': 4.728122163837457e-05, 'epoch': 0.71}
{'loss': 2.7401, 'grad_norm': 1.1128888130187988, 'learning_rate': 4.7267749601701925e-05, 'epoch': 0.71}
{'loss': 2.8719, 'grad_norm': 1.1390190124511719, 'learning_rate': 4.725424619837906e-05, 'epoch': 0.71}
{'loss': 3.0176, 'grad_norm': 1.3768309354782104, 'learning_rate': 4.724071144742707e-05, 'epoch': 0.71}
{'loss': 3.1808, 'grad_norm': 1.543092131614685, 'learning_rate': 4.722714536791116e-05, 'epoch': 0.71}
{'loss': 3.6075, 'grad_norm': 2.067089319229126, 'learning_rate': 4.721354797894072e-05, 'epoch': 0.71}
{'loss': 3.7369, 'grad_norm': 1.977592945098877, 'learning_rate': 4.719991929966919e-05, 'epoch': 0.71}
{'loss': 3.6985, 'grad_norm': 2.2480480670928955, 'learning_rate': 4.718625934929413e-05, 'epoch': 0.71}
{'loss': 3.6326, 'grad_norm': 2.3069956302642822, 'learning_rate': 4.7172568147057105e-05, 'epoch': 0.71}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.1361353397369385, 'eval_runtime': 12.5863, 'eval_samples_per_second': 129.824, 'eval_steps_per_second': 16.288, 'epoch': 0.71}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.9432, 'grad_norm': 0.6136707067489624, 'learning_rate': 4.715884571224375e-05, 'epoch': 0.72}
{'loss': 2.799, 'grad_norm': 0.8604207038879395, 'learning_rate': 4.714509206418366e-05, 'epoch': 0.72}
{'loss': 2.8051, 'grad_norm': 1.0202497243881226, 'learning_rate': 4.71313072222504e-05, 'epoch': 0.72}
{'loss': 2.9458, 'grad_norm': 1.0713024139404297, 'learning_rate': 4.71174912058615e-05, 'epoch': 0.72}
{'loss': 2.8701, 'grad_norm': 1.3689918518066406, 'learning_rate': 4.710364403447837e-05, 'epoch': 0.72}
{'loss': 3.235, 'grad_norm': 1.593753457069397, 'learning_rate': 4.7089765727606325e-05, 'epoch': 0.72}
{'loss': 3.5865, 'grad_norm': 1.8897703886032104, 'learning_rate': 4.707585630479454e-05, 'epoch': 0.72}
{'loss': 3.7122, 'grad_norm': 1.853363275527954, 'learning_rate': 4.706191578563601e-05, 'epoch': 0.72}
{'loss': 3.7296, 'grad_norm': 1.9949626922607422, 'learning_rate': 4.704794418976755e-05, 'epoch': 0.72}
{'loss': 3.7245, 'grad_norm': 2.6517815589904785, 'learning_rate': 4.7033941536869705e-05, 'epoch': 0.72}
{'loss': 2.9174, 'grad_norm': 0.5679797530174255, 'learning_rate': 4.7019907846666825e-05, 'epoch': 0.73}
{'loss': 2.7538, 'grad_norm': 0.7382780909538269, 'learning_rate': 4.700584313892695e-05, 'epoch': 0.73}
{'loss': 2.817, 'grad_norm': 0.8326514959335327, 'learning_rate': 4.6991747433461814e-05, 'epoch': 0.73}
{'loss': 2.8912, 'grad_norm': 2.933096408843994, 'learning_rate': 4.697762075012682e-05, 'epoch': 0.73}
{'loss': 3.0614, 'grad_norm': 1.3439762592315674, 'learning_rate': 4.696346310882098e-05, 'epoch': 0.73}
{'loss': 3.1379, 'grad_norm': 1.7893904447555542, 'learning_rate': 4.6949274529486973e-05, 'epoch': 0.73}
{'loss': 3.68, 'grad_norm': 1.9107797145843506, 'learning_rate': 4.6935055032111e-05, 'epoch': 0.73}
{'loss': 3.5806, 'grad_norm': 1.9829713106155396, 'learning_rate': 4.6920804636722836e-05, 'epoch': 0.73}
{'loss': 3.7054, 'grad_norm': 1.9849183559417725, 'learning_rate': 4.6906523363395795e-05, 'epoch': 0.73}
{'loss': 3.6377, 'grad_norm': 2.4509482383728027, 'learning_rate': 4.6892211232246644e-05, 'epoch': 0.73}
{'loss': 2.9368, 'grad_norm': 0.765306293964386, 'learning_rate': 4.687786826343566e-05, 'epoch': 0.74}
{'loss': 2.7292, 'grad_norm': 0.6980283856391907, 'learning_rate': 4.6863494477166544e-05, 'epoch': 0.74}
{'loss': 2.8909, 'grad_norm': 0.9232119917869568, 'learning_rate': 4.684908989368639e-05, 'epoch': 0.74}
{'loss': 2.8397, 'grad_norm': 1.0976061820983887, 'learning_rate': 4.683465453328569e-05, 'epoch': 0.74}
{'loss': 2.9588, 'grad_norm': 1.1949726343154907, 'learning_rate': 4.6820188416298306e-05, 'epoch': 0.74}
{'loss': 3.2101, 'grad_norm': 1.540831208229065, 'learning_rate': 4.680569156310137e-05, 'epoch': 0.74}
{'loss': 3.5645, 'grad_norm': 1.9012784957885742, 'learning_rate': 4.679116399411537e-05, 'epoch': 0.74}
{'loss': 3.5919, 'grad_norm': 2.0926523208618164, 'learning_rate': 4.677660572980404e-05, 'epoch': 0.74}
{'loss': 3.7036, 'grad_norm': 2.0568127632141113, 'learning_rate': 4.676201679067431e-05, 'epoch': 0.74}
{'loss': 3.7065, 'grad_norm': 2.4307525157928467, 'learning_rate': 4.674739719727638e-05, 'epoch': 0.74}
{'loss': 2.8904, 'grad_norm': 0.6670363545417786, 'learning_rate': 4.6732746970203606e-05, 'epoch': 0.75}
{'loss': 2.8492, 'grad_norm': 0.7583880424499512, 'learning_rate': 4.671806613009249e-05, 'epoch': 0.75}
{'loss': 2.8662, 'grad_norm': 0.9329025745391846, 'learning_rate': 4.6703354697622656e-05, 'epoch': 0.75}
{'loss': 2.8189, 'grad_norm': 0.9734316468238831, 'learning_rate': 4.668861269351683e-05, 'epoch': 0.75}
{'loss': 2.89, 'grad_norm': 1.4294558763504028, 'learning_rate': 4.66738401385408e-05, 'epoch': 0.75}
{'loss': 3.1974, 'grad_norm': 1.6037321090698242, 'learning_rate': 4.6659037053503374e-05, 'epoch': 0.75}
{'loss': 3.7316, 'grad_norm': 1.5731791257858276, 'learning_rate': 4.664420345925638e-05, 'epoch': 0.75}
{'loss': 3.768, 'grad_norm': 2.114415407180786, 'learning_rate': 4.6629339376694624e-05, 'epoch': 0.75}
{'loss': 3.5559, 'grad_norm': 2.129345655441284, 'learning_rate': 4.6614444826755845e-05, 'epoch': 0.75}
{'loss': 3.7049, 'grad_norm': 2.6368842124938965, 'learning_rate': 4.65995198304207e-05, 'epoch': 0.75}
{'loss': 2.9502, 'grad_norm': 0.6337764859199524, 'learning_rate': 4.658456440871275e-05, 'epoch': 0.76}
{'loss': 2.8494, 'grad_norm': 0.7490624189376831, 'learning_rate': 4.65695785826984e-05, 'epoch': 0.76}
{'loss': 2.6949, 'grad_norm': 0.86196368932724, 'learning_rate': 4.6554562373486885e-05, 'epoch': 0.76}
{'loss': 2.7823, 'grad_norm': 1.1521623134613037, 'learning_rate': 4.6539515802230235e-05, 'epoch': 0.76}
{'loss': 3.0273, 'grad_norm': 1.4856334924697876, 'learning_rate': 4.652443889012324e-05, 'epoch': 0.76}
{'loss': 3.0336, 'grad_norm': 1.5786374807357788, 'learning_rate': 4.650933165840347e-05, 'epoch': 0.76}
{'loss': 3.5178, 'grad_norm': 1.8055914640426636, 'learning_rate': 4.6494194128351156e-05, 'epoch': 0.76}
{'loss': 3.6955, 'grad_norm': 1.94878089427948, 'learning_rate': 4.647902632128922e-05, 'epoch': 0.76}
{'loss': 3.5596, 'grad_norm': 2.0293548107147217, 'learning_rate': 4.646382825858324e-05, 'epoch': 0.76}
{'loss': 3.5591, 'grad_norm': 2.2012572288513184, 'learning_rate': 4.644859996164142e-05, 'epoch': 0.77}
{'loss': 2.8713, 'grad_norm': 0.6715354919433594, 'learning_rate': 4.6433341451914534e-05, 'epoch': 0.77}
{'loss': 2.7779, 'grad_norm': 0.8162845969200134, 'learning_rate': 4.641805275089594e-05, 'epoch': 0.77}
{'loss': 2.8529, 'grad_norm': 0.9097868800163269, 'learning_rate': 4.640273388012148e-05, 'epoch': 0.77}
{'loss': 2.7761, 'grad_norm': 1.1600922346115112, 'learning_rate': 4.6387384861169546e-05, 'epoch': 0.77}
{'loss': 2.9488, 'grad_norm': 1.2311623096466064, 'learning_rate': 4.637200571566095e-05, 'epoch': 0.77}
{'loss': 3.1741, 'grad_norm': 2.870828151702881, 'learning_rate': 4.635659646525898e-05, 'epoch': 0.77}
{'loss': 3.2526, 'grad_norm': 1.9604696035385132, 'learning_rate': 4.63411571316693e-05, 'epoch': 0.77}
{'loss': 3.7947, 'grad_norm': 2.1578164100646973, 'learning_rate': 4.6325687736639964e-05, 'epoch': 0.77}
{'loss': 3.6264, 'grad_norm': 2.0529513359069824, 'learning_rate': 4.631018830196138e-05, 'epoch': 0.77}
{'loss': 3.5799, 'grad_norm': 2.595029592514038, 'learning_rate': 4.629465884946625e-05, 'epoch': 0.78}
{'loss': 2.905, 'grad_norm': 0.634874701499939, 'learning_rate': 4.627909940102957e-05, 'epoch': 0.78}
{'loss': 2.7617, 'grad_norm': 0.7393595576286316, 'learning_rate': 4.6263509978568585e-05, 'epoch': 0.78}
{'loss': 2.7423, 'grad_norm': 1.0512175559997559, 'learning_rate': 4.624789060404276e-05, 'epoch': 0.78}
{'loss': 2.9909, 'grad_norm': 1.130569338798523, 'learning_rate': 4.6232241299453773e-05, 'epoch': 0.78}
{'loss': 2.851, 'grad_norm': 1.4053415060043335, 'learning_rate': 4.6216562086845424e-05, 'epoch': 0.78}
{'loss': 3.0865, 'grad_norm': 1.639837622642517, 'learning_rate': 4.620085298830367e-05, 'epoch': 0.78}
{'loss': 3.4172, 'grad_norm': 2.057668924331665, 'learning_rate': 4.618511402595656e-05, 'epoch': 0.78}
{'loss': 3.6782, 'grad_norm': 2.1595535278320312, 'learning_rate': 4.616934522197421e-05, 'epoch': 0.78}
{'loss': 3.7083, 'grad_norm': 1.9967095851898193, 'learning_rate': 4.615354659856875e-05, 'epoch': 0.78}
{'loss': 3.462, 'grad_norm': 2.7954487800598145, 'learning_rate': 4.613771817799436e-05, 'epoch': 0.79}
{'loss': 2.9156, 'grad_norm': 0.7272639870643616, 'learning_rate': 4.6121859982547144e-05, 'epoch': 0.79}
{'loss': 2.7495, 'grad_norm': 0.6971654891967773, 'learning_rate': 4.610597203456518e-05, 'epoch': 0.79}
{'loss': 2.7078, 'grad_norm': 0.8394996523857117, 'learning_rate': 4.609005435642844e-05, 'epoch': 0.79}
{'loss': 2.7708, 'grad_norm': 1.047786831855774, 'learning_rate': 4.607410697055876e-05, 'epoch': 0.79}
{'loss': 3.0509, 'grad_norm': 1.311920166015625, 'learning_rate': 4.605812989941988e-05, 'epoch': 0.79}
{'loss': 3.0216, 'grad_norm': 1.513108491897583, 'learning_rate': 4.604212316551728e-05, 'epoch': 0.79}
{'loss': 3.4351, 'grad_norm': 1.717087745666504, 'learning_rate': 4.602608679139828e-05, 'epoch': 0.79}
{'loss': 3.626, 'grad_norm': 1.8806166648864746, 'learning_rate': 4.601002079965193e-05, 'epoch': 0.79}
{'loss': 3.6047, 'grad_norm': 2.0327062606811523, 'learning_rate': 4.5993925212909e-05, 'epoch': 0.79}
{'loss': 3.5182, 'grad_norm': 2.8596012592315674, 'learning_rate': 4.597780005384194e-05, 'epoch': 0.8}
{'loss': 2.983, 'grad_norm': 0.5852866768836975, 'learning_rate': 4.5961645345164876e-05, 'epoch': 0.8}
{'loss': 2.7671, 'grad_norm': 0.7457605600357056, 'learning_rate': 4.594546110963354e-05, 'epoch': 0.8}
{'loss': 2.7653, 'grad_norm': 0.9176009893417358, 'learning_rate': 4.592924737004527e-05, 'epoch': 0.8}
{'loss': 2.7881, 'grad_norm': 0.9849543571472168, 'learning_rate': 4.591300414923894e-05, 'epoch': 0.8}
{'loss': 3.0384, 'grad_norm': 1.536586046218872, 'learning_rate': 4.589673147009499e-05, 'epoch': 0.8}
{'loss': 3.1476, 'grad_norm': 1.7369508743286133, 'learning_rate': 4.588042935553532e-05, 'epoch': 0.8}
{'loss': 3.5879, 'grad_norm': 1.7142198085784912, 'learning_rate': 4.586409782852331e-05, 'epoch': 0.8}
{'loss': 3.6013, 'grad_norm': 1.908386468887329, 'learning_rate': 4.584773691206377e-05, 'epoch': 0.8}
{'loss': 3.7454, 'grad_norm': 2.3373208045959473, 'learning_rate': 4.583134662920291e-05, 'epoch': 0.8}
{'loss': 3.5336, 'grad_norm': 2.305793046951294, 'learning_rate': 4.5814927003028286e-05, 'epoch': 0.81}
{'loss': 3.0007, 'grad_norm': 0.6054437756538391, 'learning_rate': 4.579847805666881e-05, 'epoch': 0.81}
{'loss': 2.7134, 'grad_norm': 0.7205591201782227, 'learning_rate': 4.5781999813294704e-05, 'epoch': 0.81}
{'loss': 2.7623, 'grad_norm': 0.9039812088012695, 'learning_rate': 4.576549229611743e-05, 'epoch': 0.81}
{'loss': 2.9043, 'grad_norm': 1.0748164653778076, 'learning_rate': 4.574895552838969e-05, 'epoch': 0.81}
{'loss': 2.9782, 'grad_norm': 1.4417489767074585, 'learning_rate': 4.5732389533405405e-05, 'epoch': 0.81}
{'loss': 3.1158, 'grad_norm': 1.517536997795105, 'learning_rate': 4.571579433449966e-05, 'epoch': 0.81}
{'loss': 3.6195, 'grad_norm': 1.9345608949661255, 'learning_rate': 4.569916995504866e-05, 'epoch': 0.81}
{'loss': 3.7768, 'grad_norm': 2.009624719619751, 'learning_rate': 4.568251641846975e-05, 'epoch': 0.81}
{'loss': 3.7009, 'grad_norm': 2.1801373958587646, 'learning_rate': 4.566583374822131e-05, 'epoch': 0.82}
{'loss': 3.6383, 'grad_norm': 2.2860214710235596, 'learning_rate': 4.564912196780277e-05, 'epoch': 0.82}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.118004560470581, 'eval_runtime': 12.4825, 'eval_samples_per_second': 130.904, 'eval_steps_per_second': 16.423, 'epoch': 0.82}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.0058, 'grad_norm': 0.6395586133003235, 'learning_rate': 4.5632381100754584e-05, 'epoch': 0.82}
{'loss': 2.6953, 'grad_norm': 0.7560373544692993, 'learning_rate': 4.5615611170658146e-05, 'epoch': 0.82}
{'loss': 2.9172, 'grad_norm': 0.8471481800079346, 'learning_rate': 4.559881220113581e-05, 'epoch': 0.82}
{'loss': 2.8952, 'grad_norm': 1.0510343313217163, 'learning_rate': 4.5581984215850823e-05, 'epoch': 0.82}
{'loss': 3.0105, 'grad_norm': 1.2812345027923584, 'learning_rate': 4.556512723850732e-05, 'epoch': 0.82}
{'loss': 3.001, 'grad_norm': 1.5641124248504639, 'learning_rate': 4.5548241292850255e-05, 'epoch': 0.82}
{'loss': 3.722, 'grad_norm': 1.825327754020691, 'learning_rate': 4.553132640266542e-05, 'epoch': 0.82}
{'loss': 3.48, 'grad_norm': 2.300696849822998, 'learning_rate': 4.551438259177933e-05, 'epoch': 0.82}
{'loss': 3.6428, 'grad_norm': 2.030245304107666, 'learning_rate': 4.5497409884059265e-05, 'epoch': 0.83}
{'loss': 3.6042, 'grad_norm': 2.187586545944214, 'learning_rate': 4.548040830341323e-05, 'epoch': 0.83}
{'loss': 2.8911, 'grad_norm': 0.6296389102935791, 'learning_rate': 4.546337787378985e-05, 'epoch': 0.83}
{'loss': 2.748, 'grad_norm': 0.7816153764724731, 'learning_rate': 4.544631861917844e-05, 'epoch': 0.83}
{'loss': 2.8236, 'grad_norm': 0.8854503035545349, 'learning_rate': 4.542923056360888e-05, 'epoch': 0.83}
{'loss': 2.8118, 'grad_norm': 1.0227563381195068, 'learning_rate': 4.541211373115165e-05, 'epoch': 0.83}
{'loss': 2.8569, 'grad_norm': 1.393364667892456, 'learning_rate': 4.5394968145917736e-05, 'epoch': 0.83}
{'loss': 3.1753, 'grad_norm': 1.611688256263733, 'learning_rate': 4.537779383205864e-05, 'epoch': 0.83}
{'loss': 3.5465, 'grad_norm': 2.024747133255005, 'learning_rate': 4.536059081376633e-05, 'epoch': 0.83}
{'loss': 3.6752, 'grad_norm': 2.345036029815674, 'learning_rate': 4.5343359115273207e-05, 'epoch': 0.83}
{'loss': 3.5606, 'grad_norm': 2.0311927795410156, 'learning_rate': 4.532609876085208e-05, 'epoch': 0.84}
{'loss': 3.5134, 'grad_norm': 2.3080785274505615, 'learning_rate': 4.530880977481611e-05, 'epoch': 0.84}
{'loss': 2.8858, 'grad_norm': 0.6399831175804138, 'learning_rate': 4.5291492181518805e-05, 'epoch': 0.84}
{'loss': 2.7202, 'grad_norm': 0.7390464544296265, 'learning_rate': 4.527414600535393e-05, 'epoch': 0.84}
{'loss': 2.7631, 'grad_norm': 0.9382126927375793, 'learning_rate': 4.525677127075557e-05, 'epoch': 0.84}
{'loss': 2.9726, 'grad_norm': 1.1031010150909424, 'learning_rate': 4.5239368002198e-05, 'epoch': 0.84}
{'loss': 2.8975, 'grad_norm': 1.306373953819275, 'learning_rate': 4.5221936224195696e-05, 'epoch': 0.84}
{'loss': 3.1884, 'grad_norm': 1.817215919494629, 'learning_rate': 4.520447596130329e-05, 'epoch': 0.84}
{'loss': 3.538, 'grad_norm': 1.8867601156234741, 'learning_rate': 4.518698723811554e-05, 'epoch': 0.84}
{'loss': 3.6528, 'grad_norm': 2.067249298095703, 'learning_rate': 4.516947007926731e-05, 'epoch': 0.84}
{'loss': 3.6103, 'grad_norm': 1.9213237762451172, 'learning_rate': 4.515192450943349e-05, 'epoch': 0.85}
{'loss': 3.6097, 'grad_norm': 2.443707227706909, 'learning_rate': 4.513435055332901e-05, 'epoch': 0.85}
{'loss': 2.8741, 'grad_norm': 0.5661212205886841, 'learning_rate': 4.511674823570879e-05, 'epoch': 0.85}
{'loss': 2.8751, 'grad_norm': 0.7502778768539429, 'learning_rate': 4.5099117581367667e-05, 'epoch': 0.85}
{'loss': 2.8253, 'grad_norm': 0.8034898638725281, 'learning_rate': 4.5081458615140436e-05, 'epoch': 0.85}
{'loss': 2.8651, 'grad_norm': 0.9892033934593201, 'learning_rate': 4.506377136190175e-05, 'epoch': 0.85}
{'loss': 3.0049, 'grad_norm': 1.364070177078247, 'learning_rate': 4.50460558465661e-05, 'epoch': 0.85}
{'loss': 3.0635, 'grad_norm': 1.5392659902572632, 'learning_rate': 4.502831209408781e-05, 'epoch': 0.85}
{'loss': 3.4968, 'grad_norm': 1.871174693107605, 'learning_rate': 4.5010540129460964e-05, 'epoch': 0.85}
{'loss': 3.6957, 'grad_norm': 2.0450186729431152, 'learning_rate': 4.499273997771939e-05, 'epoch': 0.85}
{'loss': 3.6478, 'grad_norm': 2.0514745712280273, 'learning_rate': 4.497491166393662e-05, 'epoch': 0.86}
{'loss': 3.7082, 'grad_norm': 2.5715439319610596, 'learning_rate': 4.4957055213225854e-05, 'epoch': 0.86}
{'loss': 2.8715, 'grad_norm': 0.6259501576423645, 'learning_rate': 4.493917065073993e-05, 'epoch': 0.86}
{'loss': 2.6897, 'grad_norm': 0.6834598779678345, 'learning_rate': 4.492125800167127e-05, 'epoch': 0.86}
{'loss': 2.7985, 'grad_norm': 0.8848289251327515, 'learning_rate': 4.4903317291251887e-05, 'epoch': 0.86}
{'loss': 2.8999, 'grad_norm': 1.2134188413619995, 'learning_rate': 4.4885348544753295e-05, 'epoch': 0.86}
{'loss': 2.9442, 'grad_norm': 1.415235996246338, 'learning_rate': 4.486735178748652e-05, 'epoch': 0.86}
{'loss': 3.0456, 'grad_norm': 1.6656203269958496, 'learning_rate': 4.484932704480204e-05, 'epoch': 0.86}
{'loss': 3.4533, 'grad_norm': 1.69041907787323, 'learning_rate': 4.483127434208973e-05, 'epoch': 0.86}
{'loss': 3.6989, 'grad_norm': 2.154646396636963, 'learning_rate': 4.481319370477888e-05, 'epoch': 0.87}
{'loss': 3.7552, 'grad_norm': 2.0237436294555664, 'learning_rate': 4.479508515833811e-05, 'epoch': 0.87}
{'loss': 3.6362, 'grad_norm': 2.1006650924682617, 'learning_rate': 4.477694872827538e-05, 'epoch': 0.87}
{'loss': 2.9284, 'grad_norm': 0.5560910105705261, 'learning_rate': 4.475878444013789e-05, 'epoch': 0.87}
{'loss': 2.7312, 'grad_norm': 0.7110385894775391, 'learning_rate': 4.47405923195121e-05, 'epoch': 0.87}
{'loss': 2.6972, 'grad_norm': 0.7730712890625, 'learning_rate': 4.472237239202368e-05, 'epoch': 0.87}
{'loss': 2.7522, 'grad_norm': 0.9978805780410767, 'learning_rate': 4.470412468333747e-05, 'epoch': 0.87}
{'loss': 2.8089, 'grad_norm': 1.2623345851898193, 'learning_rate': 4.4685849219157435e-05, 'epoch': 0.87}
{'loss': 2.9644, 'grad_norm': 1.5291496515274048, 'learning_rate': 4.4667546025226625e-05, 'epoch': 0.87}
{'loss': 3.4398, 'grad_norm': 1.9350448846817017, 'learning_rate': 4.464921512732717e-05, 'epoch': 0.87}
{'loss': 3.6807, 'grad_norm': 1.8838441371917725, 'learning_rate': 4.463085655128023e-05, 'epoch': 0.88}
{'loss': 3.6549, 'grad_norm': 1.9494160413742065, 'learning_rate': 4.461247032294593e-05, 'epoch': 0.88}
{'loss': 3.6714, 'grad_norm': 2.5259666442871094, 'learning_rate': 4.4594056468223354e-05, 'epoch': 0.88}
{'loss': 2.8201, 'grad_norm': 0.5980493426322937, 'learning_rate': 4.457561501305052e-05, 'epoch': 0.88}
{'loss': 2.737, 'grad_norm': 0.7821279168128967, 'learning_rate': 4.455714598340429e-05, 'epoch': 0.88}
{'loss': 2.7833, 'grad_norm': 0.861491858959198, 'learning_rate': 4.4538649405300395e-05, 'epoch': 0.88}
{'loss': 2.8624, 'grad_norm': 1.904038906097412, 'learning_rate': 4.4520125304793364e-05, 'epoch': 0.88}
{'loss': 3.0523, 'grad_norm': 1.3312718868255615, 'learning_rate': 4.45015737079765e-05, 'epoch': 0.88}
{'loss': 3.0021, 'grad_norm': 1.5537225008010864, 'learning_rate': 4.44829946409818e-05, 'epoch': 0.88}
{'loss': 3.4842, 'grad_norm': 4.042230606079102, 'learning_rate': 4.4464388129980026e-05, 'epoch': 0.88}
{'loss': 3.6728, 'grad_norm': 2.0009593963623047, 'learning_rate': 4.444575420118054e-05, 'epoch': 0.89}
{'loss': 3.7477, 'grad_norm': 2.2882702350616455, 'learning_rate': 4.442709288083134e-05, 'epoch': 0.89}
{'loss': 3.5697, 'grad_norm': 2.288851022720337, 'learning_rate': 4.440840419521902e-05, 'epoch': 0.89}
{'loss': 2.8641, 'grad_norm': 0.6332223415374756, 'learning_rate': 4.438968817066871e-05, 'epoch': 0.89}
{'loss': 2.792, 'grad_norm': 0.7095689177513123, 'learning_rate': 4.437094483354405e-05, 'epoch': 0.89}
{'loss': 2.6891, 'grad_norm': 1.0310158729553223, 'learning_rate': 4.435217421024716e-05, 'epoch': 0.89}
{'loss': 2.7731, 'grad_norm': 0.9433982372283936, 'learning_rate': 4.433337632721858e-05, 'epoch': 0.89}
{'loss': 3.0053, 'grad_norm': 1.3930292129516602, 'learning_rate': 4.431455121093727e-05, 'epoch': 0.89}
{'loss': 3.0298, 'grad_norm': 1.41710364818573, 'learning_rate': 4.429569888792055e-05, 'epoch': 0.89}
{'loss': 3.512, 'grad_norm': 1.8158894777297974, 'learning_rate': 4.427681938472403e-05, 'epoch': 0.89}
{'loss': 3.6771, 'grad_norm': 1.8130639791488647, 'learning_rate': 4.4257912727941654e-05, 'epoch': 0.9}
{'loss': 3.5487, 'grad_norm': 2.1272952556610107, 'learning_rate': 4.423897894420558e-05, 'epoch': 0.9}
{'loss': 3.6177, 'grad_norm': 2.3628857135772705, 'learning_rate': 4.4220018060186186e-05, 'epoch': 0.9}
{'loss': 2.9462, 'grad_norm': 0.6706204414367676, 'learning_rate': 4.420103010259204e-05, 'epoch': 0.9}
{'loss': 2.8461, 'grad_norm': 0.7171093225479126, 'learning_rate': 4.4182015098169823e-05, 'epoch': 0.9}
{'loss': 2.9029, 'grad_norm': 0.8936415910720825, 'learning_rate': 4.416297307370433e-05, 'epoch': 0.9}
{'loss': 2.775, 'grad_norm': 0.9877598881721497, 'learning_rate': 4.4143904056018416e-05, 'epoch': 0.9}
{'loss': 2.8447, 'grad_norm': 1.125718116760254, 'learning_rate': 4.412480807197294e-05, 'epoch': 0.9}
{'loss': 3.0362, 'grad_norm': 1.6171389818191528, 'learning_rate': 4.410568514846677e-05, 'epoch': 0.9}
{'loss': 3.7045, 'grad_norm': 1.7281627655029297, 'learning_rate': 4.4086535312436716e-05, 'epoch': 0.9}
{'loss': 3.6246, 'grad_norm': 1.8769817352294922, 'learning_rate': 4.4067358590857485e-05, 'epoch': 0.91}
{'loss': 3.7632, 'grad_norm': 2.077876091003418, 'learning_rate': 4.4048155010741665e-05, 'epoch': 0.91}
{'loss': 3.6904, 'grad_norm': 2.0142276287078857, 'learning_rate': 4.4028924599139675e-05, 'epoch': 0.91}
{'loss': 2.8876, 'grad_norm': 0.7586492300033569, 'learning_rate': 4.4009667383139744e-05, 'epoch': 0.91}
{'loss': 2.7973, 'grad_norm': 0.6944968700408936, 'learning_rate': 4.399038338986783e-05, 'epoch': 0.91}
{'loss': 2.8496, 'grad_norm': 0.9325587153434753, 'learning_rate': 4.3971072646487624e-05, 'epoch': 0.91}
{'loss': 2.8222, 'grad_norm': 1.210330843925476, 'learning_rate': 4.3951735180200504e-05, 'epoch': 0.91}
{'loss': 2.9763, 'grad_norm': 1.2812886238098145, 'learning_rate': 4.393237101824548e-05, 'epoch': 0.91}
{'loss': 3.293, 'grad_norm': 1.620269775390625, 'learning_rate': 4.391298018789918e-05, 'epoch': 0.91}
{'loss': 3.5712, 'grad_norm': 1.784889817237854, 'learning_rate': 4.3893562716475767e-05, 'epoch': 0.92}
{'loss': 3.59, 'grad_norm': 1.8200509548187256, 'learning_rate': 4.3874118631326964e-05, 'epoch': 0.92}
{'loss': 3.5323, 'grad_norm': 1.9571342468261719, 'learning_rate': 4.3854647959841975e-05, 'epoch': 0.92}
{'loss': 3.5382, 'grad_norm': 2.466099977493286, 'learning_rate': 4.383515072944744e-05, 'epoch': 0.92}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.101323127746582, 'eval_runtime': 12.6262, 'eval_samples_per_second': 129.413, 'eval_steps_per_second': 16.236, 'epoch': 0.92}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 2.8967, 'grad_norm': 0.6098955869674683, 'learning_rate': 4.381562696760742e-05, 'epoch': 0.92}
{'loss': 2.7693, 'grad_norm': 0.7299177050590515, 'learning_rate': 4.379607670182335e-05, 'epoch': 0.92}
{'loss': 2.7243, 'grad_norm': 0.9565892815589905, 'learning_rate': 4.3776499959633984e-05, 'epoch': 0.92}
{'loss': 2.7868, 'grad_norm': 1.0413633584976196, 'learning_rate': 4.3756896768615406e-05, 'epoch': 0.92}
{'loss': 3.0012, 'grad_norm': 1.4053654670715332, 'learning_rate': 4.373726715638092e-05, 'epoch': 0.92}
{'loss': 3.0263, 'grad_norm': 1.5088613033294678, 'learning_rate': 4.371761115058105e-05, 'epoch': 0.92}
{'loss': 3.5382, 'grad_norm': 1.9708054065704346, 'learning_rate': 4.369792877890353e-05, 'epoch': 0.93}
{'loss': 3.6838, 'grad_norm': 2.3308634757995605, 'learning_rate': 4.3678220069073205e-05, 'epoch': 0.93}
{'loss': 3.6741, 'grad_norm': 1.9057164192199707, 'learning_rate': 4.3658485048852017e-05, 'epoch': 0.93}
{'loss': 3.482, 'grad_norm': 2.6147234439849854, 'learning_rate': 4.363872374603899e-05, 'epoch': 0.93}
{'loss': 2.9617, 'grad_norm': 0.5531965494155884, 'learning_rate': 4.361893618847015e-05, 'epoch': 0.93}
{'loss': 2.7864, 'grad_norm': 0.6680421233177185, 'learning_rate': 4.3599122404018535e-05, 'epoch': 0.93}
{'loss': 2.7021, 'grad_norm': 0.7586775422096252, 'learning_rate': 4.357928242059409e-05, 'epoch': 0.93}
{'loss': 2.8469, 'grad_norm': 1.0344303846359253, 'learning_rate': 4.355941626614368e-05, 'epoch': 0.93}
{'loss': 2.9543, 'grad_norm': 1.2731512784957886, 'learning_rate': 4.353952396865105e-05, 'epoch': 0.93}
{'loss': 3.0131, 'grad_norm': 1.4186092615127563, 'learning_rate': 4.351960555613675e-05, 'epoch': 0.93}
{'loss': 3.6347, 'grad_norm': 1.801815390586853, 'learning_rate': 4.349966105665812e-05, 'epoch': 0.94}
{'loss': 3.4834, 'grad_norm': 1.8289071321487427, 'learning_rate': 4.347969049830924e-05, 'epoch': 0.94}
{'loss': 3.5344, 'grad_norm': 1.9588955640792847, 'learning_rate': 4.3459693909220935e-05, 'epoch': 0.94}
{'loss': 3.5796, 'grad_norm': 2.238734006881714, 'learning_rate': 4.343967131756064e-05, 'epoch': 0.94}
{'loss': 2.91, 'grad_norm': 1.401093602180481, 'learning_rate': 4.341962275153245e-05, 'epoch': 0.94}
{'loss': 2.772, 'grad_norm': 0.6796796321868896, 'learning_rate': 4.3399548239377055e-05, 'epoch': 0.94}
{'loss': 2.685, 'grad_norm': 2.846982955932617, 'learning_rate': 4.3379447809371676e-05, 'epoch': 0.94}
{'loss': 2.8235, 'grad_norm': 1.059887170791626, 'learning_rate': 4.335932148983004e-05, 'epoch': 0.94}
{'loss': 2.8834, 'grad_norm': 1.5106480121612549, 'learning_rate': 4.333916930910236e-05, 'epoch': 0.94}
{'loss': 3.1175, 'grad_norm': 1.5260639190673828, 'learning_rate': 4.331899129557527e-05, 'epoch': 0.94}
{'loss': 3.5919, 'grad_norm': 1.8015766143798828, 'learning_rate': 4.329878747767177e-05, 'epoch': 0.95}
{'loss': 3.836, 'grad_norm': 2.238820791244507, 'learning_rate': 4.327855788385126e-05, 'epoch': 0.95}
{'loss': 3.5939, 'grad_norm': 2.086949348449707, 'learning_rate': 4.32583025426094e-05, 'epoch': 0.95}
{'loss': 3.4558, 'grad_norm': 2.378136157989502, 'learning_rate': 4.323802148247812e-05, 'epoch': 0.95}
{'loss': 2.8505, 'grad_norm': 0.6030862331390381, 'learning_rate': 4.3217714732025615e-05, 'epoch': 0.95}
{'loss': 2.7573, 'grad_norm': 0.6963781118392944, 'learning_rate': 4.319738231985624e-05, 'epoch': 0.95}
{'loss': 2.7751, 'grad_norm': 0.9363259077072144, 'learning_rate': 4.31770242746105e-05, 'epoch': 0.95}
{'loss': 2.8265, 'grad_norm': 0.9587712287902832, 'learning_rate': 4.3156640624965014e-05, 'epoch': 0.95}
{'loss': 2.8973, 'grad_norm': 1.194191336631775, 'learning_rate': 4.3136231399632455e-05, 'epoch': 0.95}
{'loss': 3.0106, 'grad_norm': 1.568913459777832, 'learning_rate': 4.311579662736154e-05, 'epoch': 0.95}
{'loss': 3.52, 'grad_norm': 2.0577480792999268, 'learning_rate': 4.309533633693695e-05, 'epoch': 0.96}
{'loss': 3.5684, 'grad_norm': 2.1070408821105957, 'learning_rate': 4.307485055717935e-05, 'epoch': 0.96}
{'loss': 3.6242, 'grad_norm': 2.3974103927612305, 'learning_rate': 4.305433931694526e-05, 'epoch': 0.96}
{'loss': 3.541, 'grad_norm': 2.2445669174194336, 'learning_rate': 4.30338026451271e-05, 'epoch': 0.96}
{'loss': 2.9569, 'grad_norm': 0.6048523783683777, 'learning_rate': 4.301324057065308e-05, 'epoch': 0.96}
{'loss': 2.7941, 'grad_norm': 0.7126567363739014, 'learning_rate': 4.299265312248724e-05, 'epoch': 0.96}
{'loss': 2.8135, 'grad_norm': 0.9339568614959717, 'learning_rate': 4.2972040329629326e-05, 'epoch': 0.96}
{'loss': 2.8624, 'grad_norm': 1.0713337659835815, 'learning_rate': 4.295140222111479e-05, 'epoch': 0.96}
{'loss': 2.8979, 'grad_norm': 1.526214838027954, 'learning_rate': 4.293073882601475e-05, 'epoch': 0.96}
{'loss': 3.0396, 'grad_norm': 1.4774363040924072, 'learning_rate': 4.291005017343594e-05, 'epoch': 0.97}
{'loss': 3.6117, 'grad_norm': 1.9462965726852417, 'learning_rate': 4.2889336292520675e-05, 'epoch': 0.97}
{'loss': 3.6685, 'grad_norm': 1.9821360111236572, 'learning_rate': 4.2868597212446803e-05, 'epoch': 0.97}
{'loss': 3.5702, 'grad_norm': 1.936326026916504, 'learning_rate': 4.2847832962427675e-05, 'epoch': 0.97}
{'loss': 3.5172, 'grad_norm': 2.49845552444458, 'learning_rate': 4.282704357171209e-05, 'epoch': 0.97}
{'loss': 2.8778, 'grad_norm': 1.1649458408355713, 'learning_rate': 4.280622906958425e-05, 'epoch': 0.97}
{'loss': 2.8105, 'grad_norm': 0.8495995998382568, 'learning_rate': 4.278538948536376e-05, 'epoch': 0.97}
{'loss': 2.7931, 'grad_norm': 0.9291573166847229, 'learning_rate': 4.2764524848405526e-05, 'epoch': 0.97}
{'loss': 2.7091, 'grad_norm': 0.9900373220443726, 'learning_rate': 4.274363518809975e-05, 'epoch': 0.97}
{'loss': 2.9154, 'grad_norm': 1.3776971101760864, 'learning_rate': 4.272272053387191e-05, 'epoch': 0.97}
{'loss': 2.9088, 'grad_norm': 1.6892341375350952, 'learning_rate': 4.270178091518264e-05, 'epoch': 0.98}
{'loss': 3.6189, 'grad_norm': 2.8627870082855225, 'learning_rate': 4.268081636152778e-05, 'epoch': 0.98}
{'loss': 3.6001, 'grad_norm': 2.0239498615264893, 'learning_rate': 4.265982690243828e-05, 'epoch': 0.98}
{'loss': 3.4962, 'grad_norm': 2.2859747409820557, 'learning_rate': 4.263881256748018e-05, 'epoch': 0.98}
{'loss': 3.5893, 'grad_norm': 2.649993896484375, 'learning_rate': 4.261777338625454e-05, 'epoch': 0.98}
{'loss': 2.8234, 'grad_norm': 0.5927807688713074, 'learning_rate': 4.259670938839745e-05, 'epoch': 0.98}
{'loss': 2.6873, 'grad_norm': 0.6833533048629761, 'learning_rate': 4.2575620603579924e-05, 'epoch': 0.98}
{'loss': 2.7369, 'grad_norm': 0.8517126441001892, 'learning_rate': 4.255450706150791e-05, 'epoch': 0.98}
{'loss': 2.9144, 'grad_norm': 1.0807141065597534, 'learning_rate': 4.2533368791922233e-05, 'epoch': 0.98}
{'loss': 2.9944, 'grad_norm': 1.3530703783035278, 'learning_rate': 4.251220582459853e-05, 'epoch': 0.98}
{'loss': 2.9491, 'grad_norm': 1.618251085281372, 'learning_rate': 4.2491018189347256e-05, 'epoch': 0.99}
{'loss': 3.4425, 'grad_norm': 1.9283068180084229, 'learning_rate': 4.246980591601358e-05, 'epoch': 0.99}
{'loss': 3.6082, 'grad_norm': 2.416530132293701, 'learning_rate': 4.2448569034477406e-05, 'epoch': 0.99}
{'loss': 3.6237, 'grad_norm': 1.9643356800079346, 'learning_rate': 4.2427307574653286e-05, 'epoch': 0.99}
{'loss': 3.5857, 'grad_norm': 2.570221185684204, 'learning_rate': 4.240602156649041e-05, 'epoch': 0.99}
{'loss': 2.9142, 'grad_norm': 0.5584243535995483, 'learning_rate': 4.23847110399725e-05, 'epoch': 0.99}
{'loss': 2.7443, 'grad_norm': 0.7077073454856873, 'learning_rate': 4.2363376025117884e-05, 'epoch': 0.99}
{'loss': 2.6952, 'grad_norm': 0.8333693146705627, 'learning_rate': 4.234201655197933e-05, 'epoch': 0.99}
{'loss': 2.8284, 'grad_norm': 6.256344318389893, 'learning_rate': 4.23206326506441e-05, 'epoch': 0.99}
{'loss': 2.9491, 'grad_norm': 1.3962937593460083, 'learning_rate': 4.2299224351233815e-05, 'epoch': 0.99}
{'loss': 3.0803, 'grad_norm': 1.7438973188400269, 'learning_rate': 4.227779168390451e-05, 'epoch': 1.0}
{'loss': 3.3647, 'grad_norm': 1.754940390586853, 'learning_rate': 4.225633467884652e-05, 'epoch': 1.0}
{'loss': 3.5112, 'grad_norm': 1.9534953832626343, 'learning_rate': 4.2234853366284466e-05, 'epoch': 1.0}
{'loss': 3.7264, 'grad_norm': 1.9700417518615723, 'learning_rate': 4.221334777647723e-05, 'epoch': 1.0}
{'loss': 3.6972, 'grad_norm': 2.5130441188812256, 'learning_rate': 4.219181793971786e-05, 'epoch': 1.0}
{'loss': 3.0349, 'grad_norm': 0.6217514872550964, 'learning_rate': 4.2170263886333566e-05, 'epoch': 1.0}
{'loss': 2.668, 'grad_norm': 0.6132234334945679, 'learning_rate': 4.2148685646685685e-05, 'epoch': 1.0}
{'loss': 2.7937, 'grad_norm': 0.8113442659378052, 'learning_rate': 4.212708325116962e-05, 'epoch': 1.0}
{'loss': 2.8427, 'grad_norm': 0.8627185821533203, 'learning_rate': 4.210545673021479e-05, 'epoch': 1.0}
{'loss': 2.9294, 'grad_norm': 1.3657686710357666, 'learning_rate': 4.2083806114284594e-05, 'epoch': 1.0}
{'loss': 2.9619, 'grad_norm': 1.3891688585281372, 'learning_rate': 4.2062131433876395e-05, 'epoch': 1.01}
{'loss': 3.2875, 'grad_norm': 1.599793791770935, 'learning_rate': 4.204043271952142e-05, 'epoch': 1.01}
{'loss': 3.7369, 'grad_norm': 1.8797881603240967, 'learning_rate': 4.201871000178478e-05, 'epoch': 1.01}
{'loss': 3.6043, 'grad_norm': 2.280427932739258, 'learning_rate': 4.199696331126538e-05, 'epoch': 1.01}
{'loss': 3.5261, 'grad_norm': 2.17482852935791, 'learning_rate': 4.197519267859591e-05, 'epoch': 1.01}
{'loss': 3.2902, 'grad_norm': 0.5591860413551331, 'learning_rate': 4.195339813444276e-05, 'epoch': 1.01}
{'loss': 2.8146, 'grad_norm': 0.8636063933372498, 'learning_rate': 4.193157970950603e-05, 'epoch': 1.01}
{'loss': 2.846, 'grad_norm': 0.7948408722877502, 'learning_rate': 4.1909737434519436e-05, 'epoch': 1.01}
{'loss': 2.7702, 'grad_norm': 1.616396188735962, 'learning_rate': 4.1887871340250306e-05, 'epoch': 1.01}
{'loss': 2.8343, 'grad_norm': 1.2802765369415283, 'learning_rate': 4.186598145749951e-05, 'epoch': 1.01}
{'loss': 2.9805, 'grad_norm': 1.4911320209503174, 'learning_rate': 4.184406781710142e-05, 'epoch': 1.02}
{'loss': 3.3033, 'grad_norm': 1.8845375776290894, 'learning_rate': 4.18221304499239e-05, 'epoch': 1.02}
{'loss': 3.6783, 'grad_norm': 2.0132343769073486, 'learning_rate': 4.180016938686821e-05, 'epoch': 1.02}
{'loss': 3.6451, 'grad_norm': 1.9511241912841797, 'learning_rate': 4.1778184658869005e-05, 'epoch': 1.02}
{'loss': 3.5434, 'grad_norm': 2.3129212856292725, 'learning_rate': 4.1756176296894256e-05, 'epoch': 1.02}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.091391086578369, 'eval_runtime': 12.5379, 'eval_samples_per_second': 130.324, 'eval_steps_per_second': 16.35, 'epoch': 1.02}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.1661, 'grad_norm': 0.5670532584190369, 'learning_rate': 4.1734144331945244e-05, 'epoch': 1.02}
{'loss': 2.7976, 'grad_norm': 0.6255324482917786, 'learning_rate': 4.171208879505649e-05, 'epoch': 1.02}
{'loss': 2.6454, 'grad_norm': 0.7685303092002869, 'learning_rate': 4.169000971729571e-05, 'epoch': 1.02}
{'loss': 2.8884, 'grad_norm': 1.1822516918182373, 'learning_rate': 4.16679071297638e-05, 'epoch': 1.02}
{'loss': 2.7705, 'grad_norm': 1.1924693584442139, 'learning_rate': 4.1645781063594754e-05, 'epoch': 1.03}
{'loss': 2.7969, 'grad_norm': 1.3430169820785522, 'learning_rate': 4.162363154995565e-05, 'epoch': 1.03}
{'loss': 3.1064, 'grad_norm': 1.7771753072738647, 'learning_rate': 4.160145862004657e-05, 'epoch': 1.03}
{'loss': 3.4106, 'grad_norm': 1.98087739944458, 'learning_rate': 4.157926230510064e-05, 'epoch': 1.03}
{'loss': 3.7382, 'grad_norm': 2.2643275260925293, 'learning_rate': 4.1557042636383846e-05, 'epoch': 1.03}
{'loss': 3.5038, 'grad_norm': 2.141211748123169, 'learning_rate': 4.153479964519513e-05, 'epoch': 1.03}
{'loss': 3.1604, 'grad_norm': 0.49355316162109375, 'learning_rate': 4.151253336286627e-05, 'epoch': 1.03}
{'loss': 2.7623, 'grad_norm': 0.6651879549026489, 'learning_rate': 4.149024382076184e-05, 'epoch': 1.03}
{'loss': 2.7076, 'grad_norm': 1.042584776878357, 'learning_rate': 4.1467931050279186e-05, 'epoch': 1.03}
{'loss': 2.8218, 'grad_norm': 0.9496789574623108, 'learning_rate': 4.144559508284839e-05, 'epoch': 1.03}
{'loss': 2.9019, 'grad_norm': 5.452874183654785, 'learning_rate': 4.142323594993218e-05, 'epoch': 1.04}
{'loss': 2.9347, 'grad_norm': 1.2628334760665894, 'learning_rate': 4.140085368302593e-05, 'epoch': 1.04}
{'loss': 3.2968, 'grad_norm': 1.815703272819519, 'learning_rate': 4.1378448313657616e-05, 'epoch': 1.04}
{'loss': 3.6394, 'grad_norm': 2.269444465637207, 'learning_rate': 4.135601987338773e-05, 'epoch': 1.04}
{'loss': 3.5179, 'grad_norm': 1.9678386449813843, 'learning_rate': 4.1333568393809266e-05, 'epoch': 1.04}
{'loss': 3.4967, 'grad_norm': 1.939615249633789, 'learning_rate': 4.131109390654769e-05, 'epoch': 1.04}
{'loss': 3.2504, 'grad_norm': 0.5722721219062805, 'learning_rate': 4.1288596443260865e-05, 'epoch': 1.04}
{'loss': 2.7324, 'grad_norm': 0.702772319316864, 'learning_rate': 4.1266076035639015e-05, 'epoch': 1.04}
{'loss': 2.7907, 'grad_norm': 0.7495595216751099, 'learning_rate': 4.124353271540469e-05, 'epoch': 1.04}
{'loss': 2.7235, 'grad_norm': 1.0983860492706299, 'learning_rate': 4.122096651431271e-05, 'epoch': 1.04}
{'loss': 2.8531, 'grad_norm': 3.087611436843872, 'learning_rate': 4.119837746415014e-05, 'epoch': 1.05}
{'loss': 3.1519, 'grad_norm': 1.5453683137893677, 'learning_rate': 4.1175765596736206e-05, 'epoch': 1.05}
{'loss': 3.2383, 'grad_norm': 1.7022390365600586, 'learning_rate': 4.115313094392228e-05, 'epoch': 1.05}
{'loss': 3.7165, 'grad_norm': 2.0338001251220703, 'learning_rate': 4.113047353759185e-05, 'epoch': 1.05}
{'loss': 3.5159, 'grad_norm': 3.885354518890381, 'learning_rate': 4.1107793409660445e-05, 'epoch': 1.05}
{'loss': 3.647, 'grad_norm': 2.4323785305023193, 'learning_rate': 4.108509059207558e-05, 'epoch': 1.05}
{'loss': 3.203, 'grad_norm': 1.7114895582199097, 'learning_rate': 4.106236511681677e-05, 'epoch': 1.05}
{'loss': 2.9014, 'grad_norm': 0.6572299599647522, 'learning_rate': 4.1039617015895406e-05, 'epoch': 1.05}
{'loss': 2.7198, 'grad_norm': 0.8367187976837158, 'learning_rate': 4.101684632135477e-05, 'epoch': 1.05}
{'loss': 2.7972, 'grad_norm': 1.8594390153884888, 'learning_rate': 4.099405306526998e-05, 'epoch': 1.05}
{'loss': 2.7672, 'grad_norm': 1.1670819520950317, 'learning_rate': 4.09712372797479e-05, 'epoch': 1.06}
{'loss': 2.845, 'grad_norm': 1.5400177240371704, 'learning_rate': 4.0948398996927174e-05, 'epoch': 1.06}
{'loss': 3.1906, 'grad_norm': 1.82765531539917, 'learning_rate': 4.092553824897809e-05, 'epoch': 1.06}
{'loss': 3.6196, 'grad_norm': 1.9689598083496094, 'learning_rate': 4.0902655068102636e-05, 'epoch': 1.06}
{'loss': 3.5581, 'grad_norm': 2.0050418376922607, 'learning_rate': 4.087974948653433e-05, 'epoch': 1.06}
{'loss': 3.6351, 'grad_norm': 2.0288445949554443, 'learning_rate': 4.0856821536538305e-05, 'epoch': 1.06}
{'loss': 3.2565, 'grad_norm': 0.5277248024940491, 'learning_rate': 4.083387125041117e-05, 'epoch': 1.06}
{'loss': 2.8296, 'grad_norm': 0.6923186182975769, 'learning_rate': 4.0810898660481e-05, 'epoch': 1.06}
{'loss': 2.6812, 'grad_norm': 0.7781555652618408, 'learning_rate': 4.07879037991073e-05, 'epoch': 1.06}
{'loss': 2.8515, 'grad_norm': 0.935347318649292, 'learning_rate': 4.076488669868095e-05, 'epoch': 1.06}
{'loss': 2.8874, 'grad_norm': 1.329883098602295, 'learning_rate': 4.074184739162412e-05, 'epoch': 1.07}
{'loss': 3.0384, 'grad_norm': 1.5248565673828125, 'learning_rate': 4.071878591039031e-05, 'epoch': 1.07}
{'loss': 3.2488, 'grad_norm': 1.625353455543518, 'learning_rate': 4.069570228746422e-05, 'epoch': 1.07}
{'loss': 3.5667, 'grad_norm': 1.7955976724624634, 'learning_rate': 4.067259655536175e-05, 'epoch': 1.07}
{'loss': 3.4557, 'grad_norm': 2.176081418991089, 'learning_rate': 4.064946874662995e-05, 'epoch': 1.07}
{'loss': 3.7375, 'grad_norm': 2.2599217891693115, 'learning_rate': 4.062631889384696e-05, 'epoch': 1.07}
{'loss': 3.2529, 'grad_norm': 0.5543920993804932, 'learning_rate': 4.060314702962196e-05, 'epoch': 1.07}
{'loss': 2.8463, 'grad_norm': 0.679962694644928, 'learning_rate': 4.0579953186595164e-05, 'epoch': 1.07}
{'loss': 2.593, 'grad_norm': 0.8014337420463562, 'learning_rate': 4.055673739743772e-05, 'epoch': 1.07}
{'loss': 2.8239, 'grad_norm': 1.0062624216079712, 'learning_rate': 4.0533499694851706e-05, 'epoch': 1.08}
{'loss': 2.7061, 'grad_norm': 1.2163382768630981, 'learning_rate': 4.0510240111570055e-05, 'epoch': 1.08}
{'loss': 2.8727, 'grad_norm': 1.4040635824203491, 'learning_rate': 4.0486958680356534e-05, 'epoch': 1.08}
{'loss': 3.5936, 'grad_norm': 2.166959524154663, 'learning_rate': 4.046365543400567e-05, 'epoch': 1.08}
{'loss': 3.5304, 'grad_norm': 2.0717833042144775, 'learning_rate': 4.044033040534273e-05, 'epoch': 1.08}
{'loss': 3.6677, 'grad_norm': 1.959045171737671, 'learning_rate': 4.041698362722368e-05, 'epoch': 1.08}
{'loss': 3.7366, 'grad_norm': 2.212432622909546, 'learning_rate': 4.0393615132535076e-05, 'epoch': 1.08}
{'loss': 3.1298, 'grad_norm': 0.5118005871772766, 'learning_rate': 4.03702249541941e-05, 'epoch': 1.08}
{'loss': 2.6859, 'grad_norm': 0.7064694762229919, 'learning_rate': 4.034681312514849e-05, 'epoch': 1.08}
{'loss': 2.6754, 'grad_norm': 0.8120209574699402, 'learning_rate': 4.032337967837644e-05, 'epoch': 1.08}
{'loss': 2.7862, 'grad_norm': 0.9257450103759766, 'learning_rate': 4.029992464688662e-05, 'epoch': 1.09}
{'loss': 2.8748, 'grad_norm': 1.3544492721557617, 'learning_rate': 4.027644806371811e-05, 'epoch': 1.09}
{'loss': 3.0321, 'grad_norm': 1.6614702939987183, 'learning_rate': 4.0252949961940335e-05, 'epoch': 1.09}
{'loss': 3.3975, 'grad_norm': 1.748160719871521, 'learning_rate': 4.022943037465304e-05, 'epoch': 1.09}
{'loss': 3.5379, 'grad_norm': 1.7268911600112915, 'learning_rate': 4.020588933498622e-05, 'epoch': 1.09}
{'loss': 3.5578, 'grad_norm': 2.2292582988739014, 'learning_rate': 4.018232687610011e-05, 'epoch': 1.09}
{'loss': 3.4119, 'grad_norm': 2.546811819076538, 'learning_rate': 4.015874303118509e-05, 'epoch': 1.09}
{'loss': 3.2641, 'grad_norm': 0.552012026309967, 'learning_rate': 4.01351378334617e-05, 'epoch': 1.09}
{'loss': 2.6472, 'grad_norm': 1.6204869747161865, 'learning_rate': 4.011151131618052e-05, 'epoch': 1.09}
{'loss': 2.7309, 'grad_norm': 0.7946160435676575, 'learning_rate': 4.008786351262219e-05, 'epoch': 1.09}
{'loss': 2.8642, 'grad_norm': 0.9374139308929443, 'learning_rate': 4.006419445609732e-05, 'epoch': 1.1}
{'loss': 2.8874, 'grad_norm': 1.1847527027130127, 'learning_rate': 4.0040504179946446e-05, 'epoch': 1.1}
{'loss': 2.9516, 'grad_norm': 1.5097880363464355, 'learning_rate': 4.001679271754004e-05, 'epoch': 1.1}
{'loss': 3.4104, 'grad_norm': 1.931584358215332, 'learning_rate': 3.999306010227835e-05, 'epoch': 1.1}
{'loss': 3.6825, 'grad_norm': 1.9837355613708496, 'learning_rate': 3.9969306367591476e-05, 'epoch': 1.1}
{'loss': 3.7329, 'grad_norm': 2.206101417541504, 'learning_rate': 3.9945531546939254e-05, 'epoch': 1.1}
{'loss': 3.5772, 'grad_norm': 2.106684684753418, 'learning_rate': 3.992173567381118e-05, 'epoch': 1.1}
{'loss': 3.1821, 'grad_norm': 0.5514850616455078, 'learning_rate': 3.9897918781726475e-05, 'epoch': 1.1}
{'loss': 2.7602, 'grad_norm': 0.6339970827102661, 'learning_rate': 3.9874080904233926e-05, 'epoch': 1.1}
{'loss': 2.6742, 'grad_norm': 0.7688882350921631, 'learning_rate': 3.985022207491187e-05, 'epoch': 1.1}
{'loss': 2.7634, 'grad_norm': 0.8715558648109436, 'learning_rate': 3.982634232736818e-05, 'epoch': 1.11}
{'loss': 2.8306, 'grad_norm': 1.291839838027954, 'learning_rate': 3.98024416952402e-05, 'epoch': 1.11}
{'loss': 2.8581, 'grad_norm': 1.4456449747085571, 'learning_rate': 3.977852021219467e-05, 'epoch': 1.11}
{'loss': 3.3801, 'grad_norm': 1.9302244186401367, 'learning_rate': 3.97545779119277e-05, 'epoch': 1.11}
{'loss': 3.5931, 'grad_norm': 1.988654613494873, 'learning_rate': 3.973061482816477e-05, 'epoch': 1.11}
{'loss': 3.6716, 'grad_norm': 2.1369712352752686, 'learning_rate': 3.9706630994660564e-05, 'epoch': 1.11}
{'loss': 3.6177, 'grad_norm': 2.1360092163085938, 'learning_rate': 3.968262644519904e-05, 'epoch': 1.11}
{'loss': 3.1243, 'grad_norm': 0.5635456442832947, 'learning_rate': 3.965860121359335e-05, 'epoch': 1.11}
{'loss': 2.7426, 'grad_norm': 0.7325053215026855, 'learning_rate': 3.963455533368573e-05, 'epoch': 1.11}
{'loss': 2.7045, 'grad_norm': 0.8208097815513611, 'learning_rate': 3.9610488839347536e-05, 'epoch': 1.11}
{'loss': 2.8618, 'grad_norm': 6.603396892547607, 'learning_rate': 3.958640176447915e-05, 'epoch': 1.12}
{'loss': 2.767, 'grad_norm': 1.2131531238555908, 'learning_rate': 3.956229414300997e-05, 'epoch': 1.12}
{'loss': 3.007, 'grad_norm': 1.7198508977890015, 'learning_rate': 3.953816600889829e-05, 'epoch': 1.12}
{'loss': 3.6097, 'grad_norm': 1.933306336402893, 'learning_rate': 3.951401739613134e-05, 'epoch': 1.12}
{'loss': 3.6134, 'grad_norm': 1.9183639287948608, 'learning_rate': 3.9489848338725156e-05, 'epoch': 1.12}
{'loss': 3.4478, 'grad_norm': 2.2139482498168945, 'learning_rate': 3.9465658870724624e-05, 'epoch': 1.12}
{'loss': 3.5043, 'grad_norm': 2.2268805503845215, 'learning_rate': 3.9441449026203336e-05, 'epoch': 1.12}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0825183391571045, 'eval_runtime': 12.4947, 'eval_samples_per_second': 130.775, 'eval_steps_per_second': 16.407, 'epoch': 1.12}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.2563, 'grad_norm': 0.5675723552703857, 'learning_rate': 3.9417218839263604e-05, 'epoch': 1.12}
{'loss': 2.7193, 'grad_norm': 0.6439445614814758, 'learning_rate': 3.93929683440364e-05, 'epoch': 1.12}
{'loss': 2.672, 'grad_norm': 0.7644528150558472, 'learning_rate': 3.936869757468128e-05, 'epoch': 1.13}
{'loss': 2.7091, 'grad_norm': 0.8943292498588562, 'learning_rate': 3.934440656538639e-05, 'epoch': 1.13}
{'loss': 2.8189, 'grad_norm': 1.2825289964675903, 'learning_rate': 3.932009535036838e-05, 'epoch': 1.13}
{'loss': 2.8913, 'grad_norm': 1.4340473413467407, 'learning_rate': 3.9295763963872336e-05, 'epoch': 1.13}
{'loss': 3.4773, 'grad_norm': 1.9141968488693237, 'learning_rate': 3.9271412440171775e-05, 'epoch': 1.13}
{'loss': 3.6404, 'grad_norm': 1.8495985269546509, 'learning_rate': 3.924704081356859e-05, 'epoch': 1.13}
{'loss': 3.6259, 'grad_norm': 2.0809171199798584, 'learning_rate': 3.922264911839296e-05, 'epoch': 1.13}
{'loss': 3.4957, 'grad_norm': 2.401700019836426, 'learning_rate': 3.919823738900338e-05, 'epoch': 1.13}
{'loss': 3.2576, 'grad_norm': 1.3934893608093262, 'learning_rate': 3.9173805659786524e-05, 'epoch': 1.13}
{'loss': 2.8009, 'grad_norm': 0.7080497145652771, 'learning_rate': 3.9149353965157255e-05, 'epoch': 1.13}
{'loss': 2.6927, 'grad_norm': 0.7523189187049866, 'learning_rate': 3.912488233955856e-05, 'epoch': 1.14}
{'loss': 2.7657, 'grad_norm': 0.9506368637084961, 'learning_rate': 3.91003908174615e-05, 'epoch': 1.14}
{'loss': 2.9301, 'grad_norm': 1.2928646802902222, 'learning_rate': 3.907587943336515e-05, 'epoch': 1.14}
{'loss': 3.0032, 'grad_norm': 1.5219779014587402, 'learning_rate': 3.9051348221796575e-05, 'epoch': 1.14}
{'loss': 3.2219, 'grad_norm': 1.7424198389053345, 'learning_rate': 3.9026797217310784e-05, 'epoch': 1.14}
{'loss': 3.5661, 'grad_norm': 1.824894666671753, 'learning_rate': 3.9002226454490634e-05, 'epoch': 1.14}
{'loss': 3.6278, 'grad_norm': 2.0298779010772705, 'learning_rate': 3.8977635967946846e-05, 'epoch': 1.14}
{'loss': 3.5408, 'grad_norm': 2.0583245754241943, 'learning_rate': 3.8953025792317885e-05, 'epoch': 1.14}
{'loss': 3.3448, 'grad_norm': 0.5698572993278503, 'learning_rate': 3.892839596227e-05, 'epoch': 1.14}
{'loss': 2.6796, 'grad_norm': 0.671705961227417, 'learning_rate': 3.890374651249709e-05, 'epoch': 1.14}
{'loss': 2.6812, 'grad_norm': 0.753853976726532, 'learning_rate': 3.8879077477720696e-05, 'epoch': 1.15}
{'loss': 2.8536, 'grad_norm': 0.9771932363510132, 'learning_rate': 3.885438889268995e-05, 'epoch': 1.15}
{'loss': 2.7924, 'grad_norm': 1.1760120391845703, 'learning_rate': 3.8829680792181523e-05, 'epoch': 1.15}
{'loss': 2.9266, 'grad_norm': 1.3984410762786865, 'learning_rate': 3.880495321099958e-05, 'epoch': 1.15}
{'loss': 3.413, 'grad_norm': 1.9353160858154297, 'learning_rate': 3.878020618397573e-05, 'epoch': 1.15}
{'loss': 3.6778, 'grad_norm': 2.0842444896698, 'learning_rate': 3.875543974596895e-05, 'epoch': 1.15}
{'loss': 3.6681, 'grad_norm': 2.1962201595306396, 'learning_rate': 3.873065393186559e-05, 'epoch': 1.15}
{'loss': 3.5756, 'grad_norm': 2.3685498237609863, 'learning_rate': 3.870584877657927e-05, 'epoch': 1.15}
{'loss': 3.0629, 'grad_norm': 0.5463674664497375, 'learning_rate': 3.868102431505087e-05, 'epoch': 1.15}
{'loss': 2.696, 'grad_norm': 0.6729995012283325, 'learning_rate': 3.865618058224846e-05, 'epoch': 1.15}
{'loss': 2.7952, 'grad_norm': 0.8800382018089294, 'learning_rate': 3.863131761316724e-05, 'epoch': 1.16}
{'loss': 2.734, 'grad_norm': 0.9848445057868958, 'learning_rate': 3.860643544282955e-05, 'epoch': 1.16}
{'loss': 2.9405, 'grad_norm': 1.2866417169570923, 'learning_rate': 3.858153410628472e-05, 'epoch': 1.16}
{'loss': 2.9995, 'grad_norm': 1.5774977207183838, 'learning_rate': 3.855661363860912e-05, 'epoch': 1.16}
{'loss': 3.5754, 'grad_norm': 1.693044900894165, 'learning_rate': 3.853167407490605e-05, 'epoch': 1.16}
{'loss': 3.5418, 'grad_norm': 1.9688470363616943, 'learning_rate': 3.8506715450305716e-05, 'epoch': 1.16}
{'loss': 3.6456, 'grad_norm': 1.9179608821868896, 'learning_rate': 3.8481737799965165e-05, 'epoch': 1.16}
{'loss': 3.4989, 'grad_norm': 2.3402090072631836, 'learning_rate': 3.845674115906826e-05, 'epoch': 1.16}
{'loss': 3.0836, 'grad_norm': 0.5750086903572083, 'learning_rate': 3.843172556282559e-05, 'epoch': 1.16}
{'loss': 2.6686, 'grad_norm': 0.6563540101051331, 'learning_rate': 3.840669104647448e-05, 'epoch': 1.16}
{'loss': 2.7051, 'grad_norm': 0.7438116669654846, 'learning_rate': 3.8381637645278854e-05, 'epoch': 1.17}
{'loss': 2.7658, 'grad_norm': 1.9205729961395264, 'learning_rate': 3.835656539452931e-05, 'epoch': 1.17}
{'loss': 2.7737, 'grad_norm': 1.266487956047058, 'learning_rate': 3.833147432954292e-05, 'epoch': 1.17}
{'loss': 2.9633, 'grad_norm': 1.455642580986023, 'learning_rate': 3.83063644856633e-05, 'epoch': 1.17}
{'loss': 3.4432, 'grad_norm': 2.1403515338897705, 'learning_rate': 3.828123589826053e-05, 'epoch': 1.17}
{'loss': 3.5875, 'grad_norm': 1.914348840713501, 'learning_rate': 3.8256088602731056e-05, 'epoch': 1.17}
{'loss': 3.5547, 'grad_norm': 1.9257935285568237, 'learning_rate': 3.8230922634497704e-05, 'epoch': 1.17}
{'loss': 3.6158, 'grad_norm': 2.264707088470459, 'learning_rate': 3.8205738029009594e-05, 'epoch': 1.17}
{'loss': 3.188, 'grad_norm': 0.5164843797683716, 'learning_rate': 3.8180534821742086e-05, 'epoch': 1.17}
{'loss': 2.6942, 'grad_norm': 0.630264937877655, 'learning_rate': 3.815531304819678e-05, 'epoch': 1.18}
{'loss': 2.7337, 'grad_norm': 0.7926192283630371, 'learning_rate': 3.813007274390138e-05, 'epoch': 1.18}
{'loss': 2.6881, 'grad_norm': 0.9189220666885376, 'learning_rate': 3.8104813944409736e-05, 'epoch': 1.18}
{'loss': 2.9803, 'grad_norm': 1.339400291442871, 'learning_rate': 3.807953668530171e-05, 'epoch': 1.18}
{'loss': 3.0388, 'grad_norm': 1.991757869720459, 'learning_rate': 3.805424100218321e-05, 'epoch': 1.18}
{'loss': 3.4597, 'grad_norm': 1.7701562643051147, 'learning_rate': 3.8028926930686045e-05, 'epoch': 1.18}
{'loss': 3.6874, 'grad_norm': 1.9799582958221436, 'learning_rate': 3.8003594506467996e-05, 'epoch': 1.18}
{'loss': 3.5003, 'grad_norm': 2.0425541400909424, 'learning_rate': 3.797824376521262e-05, 'epoch': 1.18}
{'loss': 3.686, 'grad_norm': 2.29207706451416, 'learning_rate': 3.795287474262932e-05, 'epoch': 1.18}
{'loss': 3.1298, 'grad_norm': 0.511439859867096, 'learning_rate': 3.792748747445325e-05, 'epoch': 1.18}
{'loss': 2.7089, 'grad_norm': 0.7061113119125366, 'learning_rate': 3.790208199644524e-05, 'epoch': 1.19}
{'loss': 2.7547, 'grad_norm': 0.8323373794555664, 'learning_rate': 3.787665834439181e-05, 'epoch': 1.19}
{'loss': 2.8458, 'grad_norm': 0.9229663610458374, 'learning_rate': 3.785121655410503e-05, 'epoch': 1.19}
{'loss': 2.8798, 'grad_norm': 1.1441376209259033, 'learning_rate': 3.782575666142256e-05, 'epoch': 1.19}
{'loss': 2.9693, 'grad_norm': 1.7363009452819824, 'learning_rate': 3.7800278702207557e-05, 'epoch': 1.19}
{'loss': 3.2295, 'grad_norm': 1.6765955686569214, 'learning_rate': 3.777478271234859e-05, 'epoch': 1.19}
{'loss': 3.6181, 'grad_norm': 2.005398988723755, 'learning_rate': 3.774926872775966e-05, 'epoch': 1.19}
{'loss': 3.4148, 'grad_norm': 1.9806383848190308, 'learning_rate': 3.772373678438013e-05, 'epoch': 1.19}
{'loss': 3.5081, 'grad_norm': 2.191498041152954, 'learning_rate': 3.76981869181746e-05, 'epoch': 1.19}
{'loss': 3.1678, 'grad_norm': 0.8917364478111267, 'learning_rate': 3.767261916513298e-05, 'epoch': 1.19}
{'loss': 2.853, 'grad_norm': 0.6630505919456482, 'learning_rate': 3.764703356127034e-05, 'epoch': 1.2}
{'loss': 2.6937, 'grad_norm': 0.8307607769966125, 'learning_rate': 3.762143014262691e-05, 'epoch': 1.2}
{'loss': 2.8024, 'grad_norm': 1.0448954105377197, 'learning_rate': 3.7595808945268e-05, 'epoch': 1.2}
{'loss': 2.9617, 'grad_norm': 1.3031102418899536, 'learning_rate': 3.757017000528398e-05, 'epoch': 1.2}
{'loss': 2.8103, 'grad_norm': 1.4960110187530518, 'learning_rate': 3.75445133587902e-05, 'epoch': 1.2}
{'loss': 3.3375, 'grad_norm': 2.1560919284820557, 'learning_rate': 3.751883904192694e-05, 'epoch': 1.2}
{'loss': 3.3526, 'grad_norm': 1.974851369857788, 'learning_rate': 3.749314709085942e-05, 'epoch': 1.2}
{'loss': 3.6475, 'grad_norm': 2.3837273120880127, 'learning_rate': 3.746743754177765e-05, 'epoch': 1.2}
{'loss': 3.3802, 'grad_norm': 2.728043556213379, 'learning_rate': 3.744171043089644e-05, 'epoch': 1.2}
{'loss': 3.0811, 'grad_norm': 0.55409836769104, 'learning_rate': 3.7415965794455335e-05, 'epoch': 1.2}
{'loss': 2.7561, 'grad_norm': 0.6786329746246338, 'learning_rate': 3.73902036687186e-05, 'epoch': 1.21}
{'loss': 2.6441, 'grad_norm': 0.8644083738327026, 'learning_rate': 3.736442408997509e-05, 'epoch': 1.21}
{'loss': 2.7844, 'grad_norm': 0.9803990125656128, 'learning_rate': 3.733862709453827e-05, 'epoch': 1.21}
{'loss': 2.8291, 'grad_norm': 1.2637349367141724, 'learning_rate': 3.731281271874614e-05, 'epoch': 1.21}
{'loss': 2.9995, 'grad_norm': 1.3805056810379028, 'learning_rate': 3.728698099896116e-05, 'epoch': 1.21}
{'loss': 3.3737, 'grad_norm': 1.955291509628296, 'learning_rate': 3.726113197157024e-05, 'epoch': 1.21}
{'loss': 3.5751, 'grad_norm': 1.8041150569915771, 'learning_rate': 3.723526567298469e-05, 'epoch': 1.21}
{'loss': 3.6787, 'grad_norm': 2.0366132259368896, 'learning_rate': 3.72093821396401e-05, 'epoch': 1.21}
{'loss': 3.5289, 'grad_norm': 2.4624149799346924, 'learning_rate': 3.7183481407996366e-05, 'epoch': 1.21}
{'loss': 3.163, 'grad_norm': 0.6357628107070923, 'learning_rate': 3.7157563514537594e-05, 'epoch': 1.21}
{'loss': 2.7959, 'grad_norm': 0.7359985113143921, 'learning_rate': 3.713162849577211e-05, 'epoch': 1.22}
{'loss': 2.6829, 'grad_norm': 0.8011667132377625, 'learning_rate': 3.7105676388232294e-05, 'epoch': 1.22}
{'loss': 2.7951, 'grad_norm': 0.9732500314712524, 'learning_rate': 3.707970722847465e-05, 'epoch': 1.22}
{'loss': 2.8465, 'grad_norm': 1.2690945863723755, 'learning_rate': 3.7053721053079684e-05, 'epoch': 1.22}
{'loss': 2.9946, 'grad_norm': 1.5230928659439087, 'learning_rate': 3.702771789865186e-05, 'epoch': 1.22}
{'loss': 3.2981, 'grad_norm': 1.6951431035995483, 'learning_rate': 3.700169780181958e-05, 'epoch': 1.22}
{'loss': 3.5163, 'grad_norm': 1.9314444065093994, 'learning_rate': 3.69756607992351e-05, 'epoch': 1.22}
{'loss': 3.5904, 'grad_norm': 2.5844919681549072, 'learning_rate': 3.694960692757448e-05, 'epoch': 1.22}
{'loss': 3.5989, 'grad_norm': 2.228722095489502, 'learning_rate': 3.692353622353755e-05, 'epoch': 1.22}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0731167793273926, 'eval_runtime': 12.5858, 'eval_samples_per_second': 129.828, 'eval_steps_per_second': 16.288, 'epoch': 1.22}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.0637, 'grad_norm': 0.5127715468406677, 'learning_rate': 3.689744872384788e-05, 'epoch': 1.23}
{'loss': 2.7988, 'grad_norm': 0.7372184991836548, 'learning_rate': 3.687134446525265e-05, 'epoch': 1.23}
{'loss': 2.7095, 'grad_norm': 0.7962082028388977, 'learning_rate': 3.684522348452266e-05, 'epoch': 1.23}
{'loss': 2.7354, 'grad_norm': 1.0083792209625244, 'learning_rate': 3.6819085818452294e-05, 'epoch': 1.23}
{'loss': 2.8193, 'grad_norm': 1.1703356504440308, 'learning_rate': 3.679293150385941e-05, 'epoch': 1.23}
{'loss': 2.7973, 'grad_norm': 1.4530104398727417, 'learning_rate': 3.676676057758532e-05, 'epoch': 1.23}
{'loss': 3.3392, 'grad_norm': 1.711848497390747, 'learning_rate': 3.6740573076494744e-05, 'epoch': 1.23}
{'loss': 3.6085, 'grad_norm': 1.8226951360702515, 'learning_rate': 3.671436903747576e-05, 'epoch': 1.23}
{'loss': 3.6315, 'grad_norm': 2.117581844329834, 'learning_rate': 3.6688148497439705e-05, 'epoch': 1.23}
{'loss': 3.5397, 'grad_norm': 2.295559883117676, 'learning_rate': 3.6661911493321196e-05, 'epoch': 1.23}
{'loss': 3.1374, 'grad_norm': 0.5063169598579407, 'learning_rate': 3.6635658062078026e-05, 'epoch': 1.24}
{'loss': 2.7579, 'grad_norm': 0.9648577570915222, 'learning_rate': 3.660938824069114e-05, 'epoch': 1.24}
{'loss': 2.7001, 'grad_norm': 0.763508677482605, 'learning_rate': 3.658310206616454e-05, 'epoch': 1.24}
{'loss': 2.8116, 'grad_norm': 0.9835614562034607, 'learning_rate': 3.6556799575525304e-05, 'epoch': 1.24}
{'loss': 2.7988, 'grad_norm': 1.3517321348190308, 'learning_rate': 3.653048080582345e-05, 'epoch': 1.24}
{'loss': 2.8714, 'grad_norm': 1.4442437887191772, 'learning_rate': 3.6504145794131963e-05, 'epoch': 1.24}
{'loss': 3.4509, 'grad_norm': 2.034207582473755, 'learning_rate': 3.6477794577546706e-05, 'epoch': 1.24}
{'loss': 3.5715, 'grad_norm': 1.9528847932815552, 'learning_rate': 3.645142719318633e-05, 'epoch': 1.24}
{'loss': 3.4604, 'grad_norm': 2.1168110370635986, 'learning_rate': 3.642504367819229e-05, 'epoch': 1.24}
{'loss': 3.4291, 'grad_norm': 2.202208995819092, 'learning_rate': 3.639864406972878e-05, 'epoch': 1.24}
{'loss': 3.0806, 'grad_norm': 0.51707524061203, 'learning_rate': 3.637222840498263e-05, 'epoch': 1.25}
{'loss': 2.7728, 'grad_norm': 0.7043973803520203, 'learning_rate': 3.6345796721163285e-05, 'epoch': 1.25}
{'loss': 2.7504, 'grad_norm': 0.7680988311767578, 'learning_rate': 3.631934905550278e-05, 'epoch': 1.25}
{'loss': 2.8578, 'grad_norm': 0.9842223525047302, 'learning_rate': 3.629288544525566e-05, 'epoch': 1.25}
{'loss': 2.9856, 'grad_norm': 1.2508046627044678, 'learning_rate': 3.62664059276989e-05, 'epoch': 1.25}
{'loss': 2.9234, 'grad_norm': 1.4589627981185913, 'learning_rate': 3.6239910540131923e-05, 'epoch': 1.25}
{'loss': 3.2618, 'grad_norm': 1.7411072254180908, 'learning_rate': 3.6213399319876475e-05, 'epoch': 1.25}
{'loss': 3.5675, 'grad_norm': 2.3655574321746826, 'learning_rate': 3.618687230427662e-05, 'epoch': 1.25}
{'loss': 3.6768, 'grad_norm': 1.8889979124069214, 'learning_rate': 3.616032953069866e-05, 'epoch': 1.25}
{'loss': 3.4756, 'grad_norm': 2.1272737979888916, 'learning_rate': 3.613377103653112e-05, 'epoch': 1.25}
{'loss': 3.2276, 'grad_norm': 0.5274503827095032, 'learning_rate': 3.6107196859184625e-05, 'epoch': 1.26}
{'loss': 2.7423, 'grad_norm': 0.6673554182052612, 'learning_rate': 3.608060703609194e-05, 'epoch': 1.26}
{'loss': 2.7245, 'grad_norm': 0.731083869934082, 'learning_rate': 3.6054001604707826e-05, 'epoch': 1.26}
{'loss': 2.6209, 'grad_norm': 0.9589629769325256, 'learning_rate': 3.602738060250907e-05, 'epoch': 1.26}
{'loss': 3.0034, 'grad_norm': 1.2456955909729004, 'learning_rate': 3.600074406699436e-05, 'epoch': 1.26}
{'loss': 3.0381, 'grad_norm': 1.3610870838165283, 'learning_rate': 3.597409203568427e-05, 'epoch': 1.26}
{'loss': 3.2401, 'grad_norm': 1.8022929430007935, 'learning_rate': 3.594742454612122e-05, 'epoch': 1.26}
{'loss': 3.6385, 'grad_norm': 1.8243244886398315, 'learning_rate': 3.592074163586939e-05, 'epoch': 1.26}
{'loss': 3.6515, 'grad_norm': 2.01043963432312, 'learning_rate': 3.589404334251469e-05, 'epoch': 1.26}
{'loss': 3.5564, 'grad_norm': 2.2329089641571045, 'learning_rate': 3.586732970366468e-05, 'epoch': 1.26}
{'loss': 3.2428, 'grad_norm': 0.5143808126449585, 'learning_rate': 3.5840600756948564e-05, 'epoch': 1.27}
{'loss': 2.6891, 'grad_norm': 0.669468343257904, 'learning_rate': 3.581385654001708e-05, 'epoch': 1.27}
{'loss': 2.7786, 'grad_norm': 0.8415769338607788, 'learning_rate': 3.5787097090542496e-05, 'epoch': 1.27}
{'loss': 2.7318, 'grad_norm': 1.0301562547683716, 'learning_rate': 3.576032244621853e-05, 'epoch': 1.27}
{'loss': 2.8756, 'grad_norm': 1.197702407836914, 'learning_rate': 3.573353264476031e-05, 'epoch': 1.27}
{'loss': 3.0787, 'grad_norm': 1.6455610990524292, 'learning_rate': 3.570672772390431e-05, 'epoch': 1.27}
{'loss': 3.4433, 'grad_norm': 1.9498201608657837, 'learning_rate': 3.567990772140829e-05, 'epoch': 1.27}
{'loss': 3.5766, 'grad_norm': 1.8354718685150146, 'learning_rate': 3.5653072675051265e-05, 'epoch': 1.27}
{'loss': 3.6216, 'grad_norm': 2.076462984085083, 'learning_rate': 3.562622262263345e-05, 'epoch': 1.27}
{'loss': 3.5297, 'grad_norm': 2.240628957748413, 'learning_rate': 3.5599357601976184e-05, 'epoch': 1.28}
{'loss': 3.1936, 'grad_norm': 0.6421566605567932, 'learning_rate': 3.55724776509219e-05, 'epoch': 1.28}
{'loss': 2.6607, 'grad_norm': 0.6712847352027893, 'learning_rate': 3.5545582807334036e-05, 'epoch': 1.28}
{'loss': 2.7029, 'grad_norm': 0.854979395866394, 'learning_rate': 3.551867310909706e-05, 'epoch': 1.28}
{'loss': 2.8549, 'grad_norm': 1.07573401927948, 'learning_rate': 3.549174859411631e-05, 'epoch': 1.28}
{'loss': 2.8499, 'grad_norm': 1.3083775043487549, 'learning_rate': 3.546480930031803e-05, 'epoch': 1.28}
{'loss': 3.0027, 'grad_norm': 1.2122001647949219, 'learning_rate': 3.543785526564927e-05, 'epoch': 1.28}
{'loss': 3.2861, 'grad_norm': 1.802718162536621, 'learning_rate': 3.541088652807783e-05, 'epoch': 1.28}
{'loss': 3.4905, 'grad_norm': 1.82351815700531, 'learning_rate': 3.5383903125592245e-05, 'epoch': 1.28}
{'loss': 3.5784, 'grad_norm': 1.8973405361175537, 'learning_rate': 3.5356905096201707e-05, 'epoch': 1.28}
{'loss': 3.5317, 'grad_norm': 2.3685660362243652, 'learning_rate': 3.5329892477935976e-05, 'epoch': 1.29}
{'loss': 3.2821, 'grad_norm': 0.5816546678543091, 'learning_rate': 3.530286530884541e-05, 'epoch': 1.29}
{'loss': 2.656, 'grad_norm': 0.7159698605537415, 'learning_rate': 3.527582362700084e-05, 'epoch': 1.29}
{'loss': 2.7317, 'grad_norm': 0.84260493516922, 'learning_rate': 3.5248767470493534e-05, 'epoch': 1.29}
{'loss': 2.7053, 'grad_norm': 1.0140527486801147, 'learning_rate': 3.522169687743515e-05, 'epoch': 1.29}
{'loss': 2.8544, 'grad_norm': 1.1648417711257935, 'learning_rate': 3.5194611885957696e-05, 'epoch': 1.29}
{'loss': 2.9408, 'grad_norm': 1.544055461883545, 'learning_rate': 3.5167512534213466e-05, 'epoch': 1.29}
{'loss': 3.4991, 'grad_norm': 1.8747005462646484, 'learning_rate': 3.514039886037494e-05, 'epoch': 1.29}
{'loss': 3.6228, 'grad_norm': 2.4361732006073, 'learning_rate': 3.5113270902634825e-05, 'epoch': 1.29}
{'loss': 3.5101, 'grad_norm': 2.178260087966919, 'learning_rate': 3.5086128699205924e-05, 'epoch': 1.29}
{'loss': 3.4366, 'grad_norm': 2.66070818901062, 'learning_rate': 3.505897228832109e-05, 'epoch': 1.3}
{'loss': 3.2196, 'grad_norm': 0.5348567962646484, 'learning_rate': 3.503180170823324e-05, 'epoch': 1.3}
{'loss': 2.7258, 'grad_norm': 0.6611484885215759, 'learning_rate': 3.500461699721518e-05, 'epoch': 1.3}
{'loss': 2.7898, 'grad_norm': 0.7915128469467163, 'learning_rate': 3.497741819355968e-05, 'epoch': 1.3}
{'loss': 2.7305, 'grad_norm': 0.9417725205421448, 'learning_rate': 3.495020533557933e-05, 'epoch': 1.3}
{'loss': 2.8937, 'grad_norm': 1.1875030994415283, 'learning_rate': 3.492297846160654e-05, 'epoch': 1.3}
{'loss': 2.7603, 'grad_norm': 1.8798160552978516, 'learning_rate': 3.489573760999344e-05, 'epoch': 1.3}
{'loss': 3.3302, 'grad_norm': 1.819300889968872, 'learning_rate': 3.4868482819111845e-05, 'epoch': 1.3}
{'loss': 3.6897, 'grad_norm': 2.1317687034606934, 'learning_rate': 3.484121412735324e-05, 'epoch': 1.3}
{'loss': 3.6168, 'grad_norm': 2.1427409648895264, 'learning_rate': 3.481393157312866e-05, 'epoch': 1.3}
{'loss': 3.6212, 'grad_norm': 2.018090009689331, 'learning_rate': 3.478663519486867e-05, 'epoch': 1.31}
{'loss': 3.1724, 'grad_norm': 0.5420277118682861, 'learning_rate': 3.4759325031023327e-05, 'epoch': 1.31}
{'loss': 2.7356, 'grad_norm': 0.6492525339126587, 'learning_rate': 3.4732001120062084e-05, 'epoch': 1.31}
{'loss': 2.7596, 'grad_norm': 0.89414381980896, 'learning_rate': 3.470466350047377e-05, 'epoch': 1.31}
{'loss': 2.8769, 'grad_norm': 0.9880419373512268, 'learning_rate': 3.4677312210766524e-05, 'epoch': 1.31}
{'loss': 2.8638, 'grad_norm': 1.3553037643432617, 'learning_rate': 3.464994728946774e-05, 'epoch': 1.31}
{'loss': 2.9489, 'grad_norm': 1.4014602899551392, 'learning_rate': 3.4622568775124e-05, 'epoch': 1.31}
{'loss': 3.2284, 'grad_norm': 1.737615942955017, 'learning_rate': 3.459517670630106e-05, 'epoch': 1.31}
{'loss': 3.5385, 'grad_norm': 1.859532117843628, 'learning_rate': 3.456777112158375e-05, 'epoch': 1.31}
{'loss': 3.5257, 'grad_norm': 2.0988683700561523, 'learning_rate': 3.454035205957595e-05, 'epoch': 1.31}
{'loss': 3.5414, 'grad_norm': 2.186720371246338, 'learning_rate': 3.4512919558900506e-05, 'epoch': 1.32}
{'loss': 3.1301, 'grad_norm': 0.47995731234550476, 'learning_rate': 3.4485473658199217e-05, 'epoch': 1.32}
{'loss': 2.7293, 'grad_norm': 0.6465887427330017, 'learning_rate': 3.445801439613273e-05, 'epoch': 1.32}
{'loss': 2.7294, 'grad_norm': 1.540317416191101, 'learning_rate': 3.4430541811380545e-05, 'epoch': 1.32}
{'loss': 2.8195, 'grad_norm': 1.0055373907089233, 'learning_rate': 3.44030559426409e-05, 'epoch': 1.32}
{'loss': 2.8159, 'grad_norm': 1.1049022674560547, 'learning_rate': 3.437555682863079e-05, 'epoch': 1.32}
{'loss': 2.9205, 'grad_norm': 1.3683433532714844, 'learning_rate': 3.4348044508085786e-05, 'epoch': 1.32}
{'loss': 3.1544, 'grad_norm': 1.5096585750579834, 'learning_rate': 3.432051901976014e-05, 'epoch': 1.32}
{'loss': 3.4569, 'grad_norm': 1.9763703346252441, 'learning_rate': 3.4292980402426624e-05, 'epoch': 1.32}
{'loss': 3.6536, 'grad_norm': 1.975019931793213, 'learning_rate': 3.42654286948765e-05, 'epoch': 1.33}
{'loss': 3.6621, 'grad_norm': 2.237074375152588, 'learning_rate': 3.423786393591946e-05, 'epoch': 1.33}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.060487747192383, 'eval_runtime': 12.5464, 'eval_samples_per_second': 130.236, 'eval_steps_per_second': 16.339, 'epoch': 1.33}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.3576, 'grad_norm': 0.5166218876838684, 'learning_rate': 3.421028616438361e-05, 'epoch': 1.33}
{'loss': 2.8426, 'grad_norm': 0.7052028775215149, 'learning_rate': 3.418269541911536e-05, 'epoch': 1.33}
{'loss': 2.8015, 'grad_norm': 0.7678659558296204, 'learning_rate': 3.41550917389794e-05, 'epoch': 1.33}
{'loss': 2.9363, 'grad_norm': 1.071094036102295, 'learning_rate': 3.412747516285865e-05, 'epoch': 1.33}
{'loss': 2.8526, 'grad_norm': 1.1600641012191772, 'learning_rate': 3.409984572965419e-05, 'epoch': 1.33}
{'loss': 2.8963, 'grad_norm': 1.4217828512191772, 'learning_rate': 3.4072203478285194e-05, 'epoch': 1.33}
{'loss': 3.1533, 'grad_norm': 1.4491182565689087, 'learning_rate': 3.404454844768893e-05, 'epoch': 1.33}
{'loss': 3.4863, 'grad_norm': 2.0226099491119385, 'learning_rate': 3.401688067682063e-05, 'epoch': 1.33}
{'loss': 3.5543, 'grad_norm': 2.210212230682373, 'learning_rate': 3.398920020465349e-05, 'epoch': 1.34}
{'loss': 3.7226, 'grad_norm': 2.2499053478240967, 'learning_rate': 3.3961507070178586e-05, 'epoch': 1.34}
{'loss': 3.142, 'grad_norm': 0.5419166684150696, 'learning_rate': 3.393380131240485e-05, 'epoch': 1.34}
{'loss': 2.7712, 'grad_norm': 0.6865108013153076, 'learning_rate': 3.390608297035899e-05, 'epoch': 1.34}
{'loss': 2.7598, 'grad_norm': 0.8337035775184631, 'learning_rate': 3.3878352083085414e-05, 'epoch': 1.34}
{'loss': 2.7857, 'grad_norm': 1.076291561126709, 'learning_rate': 3.385060868964623e-05, 'epoch': 1.34}
{'loss': 2.776, 'grad_norm': 1.310387372970581, 'learning_rate': 3.382285282912115e-05, 'epoch': 1.34}
{'loss': 3.1078, 'grad_norm': 1.5760606527328491, 'learning_rate': 3.379508454060745e-05, 'epoch': 1.34}
{'loss': 3.7714, 'grad_norm': 1.8934835195541382, 'learning_rate': 3.376730386321993e-05, 'epoch': 1.34}
{'loss': 3.7213, 'grad_norm': 2.0162906646728516, 'learning_rate': 3.3739510836090805e-05, 'epoch': 1.34}
{'loss': 3.4841, 'grad_norm': 2.031939744949341, 'learning_rate': 3.371170549836971e-05, 'epoch': 1.35}
{'loss': 3.4119, 'grad_norm': 2.2233781814575195, 'learning_rate': 3.368388788922361e-05, 'epoch': 1.35}
{'loss': 3.1333, 'grad_norm': 0.5537102818489075, 'learning_rate': 3.365605804783677e-05, 'epoch': 1.35}
{'loss': 2.8121, 'grad_norm': 0.9568559527397156, 'learning_rate': 3.362821601341069e-05, 'epoch': 1.35}
{'loss': 2.7549, 'grad_norm': 0.8044044971466064, 'learning_rate': 3.3600361825164e-05, 'epoch': 1.35}
{'loss': 2.7339, 'grad_norm': 0.9522239565849304, 'learning_rate': 3.357249552233251e-05, 'epoch': 1.35}
{'loss': 2.9026, 'grad_norm': 1.23569655418396, 'learning_rate': 3.3544617144169064e-05, 'epoch': 1.35}
{'loss': 2.9956, 'grad_norm': 1.4958769083023071, 'learning_rate': 3.35167267299435e-05, 'epoch': 1.35}
{'loss': 3.3597, 'grad_norm': 1.9638179540634155, 'learning_rate': 3.348882431894265e-05, 'epoch': 1.35}
{'loss': 3.6821, 'grad_norm': 2.002493381500244, 'learning_rate': 3.3460909950470207e-05, 'epoch': 1.35}
{'loss': 3.6301, 'grad_norm': 1.872491717338562, 'learning_rate': 3.343298366384674e-05, 'epoch': 1.36}
{'loss': 3.5969, 'grad_norm': 2.4311366081237793, 'learning_rate': 3.3405045498409574e-05, 'epoch': 1.36}
{'loss': 3.1741, 'grad_norm': 0.5257099866867065, 'learning_rate': 3.33770954935128e-05, 'epoch': 1.36}
{'loss': 2.6944, 'grad_norm': 0.6805402040481567, 'learning_rate': 3.3349133688527156e-05, 'epoch': 1.36}
{'loss': 2.7338, 'grad_norm': 0.8714851140975952, 'learning_rate': 3.3321160122840024e-05, 'epoch': 1.36}
{'loss': 2.7416, 'grad_norm': 1.1400266885757446, 'learning_rate': 3.3293174835855335e-05, 'epoch': 1.36}
{'loss': 2.8535, 'grad_norm': 1.2442994117736816, 'learning_rate': 3.3265177866993534e-05, 'epoch': 1.36}
{'loss': 2.9091, 'grad_norm': 1.3337335586547852, 'learning_rate': 3.3237169255691555e-05, 'epoch': 1.36}
{'loss': 3.099, 'grad_norm': 2.205422878265381, 'learning_rate': 3.3209149041402666e-05, 'epoch': 1.36}
{'loss': 3.5606, 'grad_norm': 1.968019723892212, 'learning_rate': 3.318111726359653e-05, 'epoch': 1.36}
{'loss': 3.6075, 'grad_norm': 1.9433876276016235, 'learning_rate': 3.315307396175909e-05, 'epoch': 1.37}
{'loss': 3.6194, 'grad_norm': 2.314009428024292, 'learning_rate': 3.312501917539253e-05, 'epoch': 1.37}
{'loss': 3.0391, 'grad_norm': 0.9478753209114075, 'learning_rate': 3.309695294401517e-05, 'epoch': 1.37}
{'loss': 2.7806, 'grad_norm': 0.649623453617096, 'learning_rate': 3.3068875307161485e-05, 'epoch': 1.37}
{'loss': 2.7506, 'grad_norm': 0.7513647675514221, 'learning_rate': 3.304078630438202e-05, 'epoch': 1.37}
{'loss': 2.8264, 'grad_norm': 0.9654523134231567, 'learning_rate': 3.301268597524331e-05, 'epoch': 1.37}
{'loss': 2.881, 'grad_norm': 1.2120863199234009, 'learning_rate': 3.298457435932787e-05, 'epoch': 1.37}
{'loss': 2.7832, 'grad_norm': 1.4259812831878662, 'learning_rate': 3.2956451496234084e-05, 'epoch': 1.37}
{'loss': 3.4609, 'grad_norm': 1.917672872543335, 'learning_rate': 3.29283174255762e-05, 'epoch': 1.37}
{'loss': 3.5424, 'grad_norm': 1.9474000930786133, 'learning_rate': 3.290017218698425e-05, 'epoch': 1.37}
{'loss': 3.5156, 'grad_norm': 1.906558871269226, 'learning_rate': 3.287201582010399e-05, 'epoch': 1.38}
{'loss': 3.6446, 'grad_norm': 2.1925201416015625, 'learning_rate': 3.284384836459684e-05, 'epoch': 1.38}
{'loss': 3.1093, 'grad_norm': 0.5593951940536499, 'learning_rate': 3.2815669860139874e-05, 'epoch': 1.38}
{'loss': 2.7317, 'grad_norm': 0.6325173377990723, 'learning_rate': 3.278748034642572e-05, 'epoch': 1.38}
{'loss': 2.7177, 'grad_norm': 0.7088340520858765, 'learning_rate': 3.275927986316247e-05, 'epoch': 1.38}
{'loss': 2.7273, 'grad_norm': 1.0602465867996216, 'learning_rate': 3.273106845007373e-05, 'epoch': 1.38}
{'loss': 2.7206, 'grad_norm': 1.125282645225525, 'learning_rate': 3.270284614689847e-05, 'epoch': 1.38}
{'loss': 3.0244, 'grad_norm': 1.595460057258606, 'learning_rate': 3.2674612993391005e-05, 'epoch': 1.38}
{'loss': 3.4776, 'grad_norm': 1.6893080472946167, 'learning_rate': 3.264636902932093e-05, 'epoch': 1.38}
{'loss': 3.5154, 'grad_norm': 1.9620498418807983, 'learning_rate': 3.261811429447306e-05, 'epoch': 1.39}
{'loss': 3.5867, 'grad_norm': 1.997384786605835, 'learning_rate': 3.2589848828647415e-05, 'epoch': 1.39}
{'loss': 3.5172, 'grad_norm': 2.2681145668029785, 'learning_rate': 3.2561572671659085e-05, 'epoch': 1.39}
{'loss': 3.1066, 'grad_norm': 0.5567203760147095, 'learning_rate': 3.253328586333826e-05, 'epoch': 1.39}
{'loss': 2.623, 'grad_norm': 0.6474907398223877, 'learning_rate': 3.2504988443530116e-05, 'epoch': 1.39}
{'loss': 2.7903, 'grad_norm': 0.7832468152046204, 'learning_rate': 3.247668045209477e-05, 'epoch': 1.39}
{'loss': 2.8397, 'grad_norm': 0.9453911781311035, 'learning_rate': 3.2448361928907226e-05, 'epoch': 1.39}
{'loss': 2.9425, 'grad_norm': 1.2022560834884644, 'learning_rate': 3.242003291385737e-05, 'epoch': 1.39}
{'loss': 2.9014, 'grad_norm': 1.3887560367584229, 'learning_rate': 3.23916934468498e-05, 'epoch': 1.39}
{'loss': 3.0019, 'grad_norm': 1.8644044399261475, 'learning_rate': 3.236334356780388e-05, 'epoch': 1.39}
{'loss': 3.5724, 'grad_norm': 1.8623456954956055, 'learning_rate': 3.233498331665363e-05, 'epoch': 1.4}
{'loss': 3.5429, 'grad_norm': 1.9646879434585571, 'learning_rate': 3.230661273334769e-05, 'epoch': 1.4}
{'loss': 3.662, 'grad_norm': 2.130927085876465, 'learning_rate': 3.2278231857849216e-05, 'epoch': 1.4}
{'loss': 3.2033, 'grad_norm': 0.6144213676452637, 'learning_rate': 3.2249840730135916e-05, 'epoch': 1.4}
{'loss': 2.7302, 'grad_norm': 0.6928750872612, 'learning_rate': 3.2221439390199906e-05, 'epoch': 1.4}
{'loss': 2.6505, 'grad_norm': 0.7865703105926514, 'learning_rate': 3.219302787804768e-05, 'epoch': 1.4}
{'loss': 2.8869, 'grad_norm': 1.0455979108810425, 'learning_rate': 3.216460623370008e-05, 'epoch': 1.4}
{'loss': 2.8739, 'grad_norm': 1.1512253284454346, 'learning_rate': 3.213617449719223e-05, 'epoch': 1.4}
{'loss': 2.996, 'grad_norm': 1.4392091035842896, 'learning_rate': 3.210773270857344e-05, 'epoch': 1.4}
{'loss': 3.38, 'grad_norm': 1.7150275707244873, 'learning_rate': 3.207928090790719e-05, 'epoch': 1.4}
{'loss': 3.5817, 'grad_norm': 1.818129062652588, 'learning_rate': 3.205081913527108e-05, 'epoch': 1.41}
{'loss': 3.524, 'grad_norm': 1.7988556623458862, 'learning_rate': 3.2022347430756744e-05, 'epoch': 1.41}
{'loss': 3.603, 'grad_norm': 2.2216293811798096, 'learning_rate': 3.199386583446979e-05, 'epoch': 1.41}
{'loss': 3.1469, 'grad_norm': 0.6441780924797058, 'learning_rate': 3.1965374386529804e-05, 'epoch': 1.41}
{'loss': 2.7223, 'grad_norm': 0.6354624032974243, 'learning_rate': 3.193687312707019e-05, 'epoch': 1.41}
{'loss': 2.7511, 'grad_norm': 0.7734382748603821, 'learning_rate': 3.190836209623823e-05, 'epoch': 1.41}
{'loss': 2.7136, 'grad_norm': 1.0009194612503052, 'learning_rate': 3.1879841334194916e-05, 'epoch': 1.41}
{'loss': 2.7824, 'grad_norm': 1.0777946710586548, 'learning_rate': 3.1851310881115e-05, 'epoch': 1.41}
{'loss': 2.8276, 'grad_norm': 1.4143422842025757, 'learning_rate': 3.182277077718684e-05, 'epoch': 1.41}
{'loss': 3.3456, 'grad_norm': 1.8742492198944092, 'learning_rate': 3.179422106261244e-05, 'epoch': 1.41}
{'loss': 3.5527, 'grad_norm': 1.8114923238754272, 'learning_rate': 3.176566177760727e-05, 'epoch': 1.42}
{'loss': 3.3727, 'grad_norm': 1.9672847986221313, 'learning_rate': 3.173709296240035e-05, 'epoch': 1.42}
{'loss': 3.3588, 'grad_norm': 2.030336380004883, 'learning_rate': 3.170851465723408e-05, 'epoch': 1.42}
{'loss': 3.1875, 'grad_norm': 0.4918006956577301, 'learning_rate': 3.167992690236426e-05, 'epoch': 1.42}
{'loss': 2.8158, 'grad_norm': 0.6642143726348877, 'learning_rate': 3.165132973805997e-05, 'epoch': 1.42}
{'loss': 2.7244, 'grad_norm': 0.7808080911636353, 'learning_rate': 3.162272320460356e-05, 'epoch': 1.42}
{'loss': 2.8054, 'grad_norm': 0.8583992123603821, 'learning_rate': 3.1594107342290577e-05, 'epoch': 1.42}
{'loss': 2.7732, 'grad_norm': 1.1575210094451904, 'learning_rate': 3.1565482191429716e-05, 'epoch': 1.42}
{'loss': 2.9408, 'grad_norm': 1.3658264875411987, 'learning_rate': 3.1536847792342735e-05, 'epoch': 1.42}
{'loss': 3.1472, 'grad_norm': 2.0578675270080566, 'learning_rate': 3.150820418536444e-05, 'epoch': 1.42}
{'loss': 3.5086, 'grad_norm': 1.7572425603866577, 'learning_rate': 3.147955141084259e-05, 'epoch': 1.43}
{'loss': 3.6735, 'grad_norm': 2.0284154415130615, 'learning_rate': 3.145088950913788e-05, 'epoch': 1.43}
{'loss': 3.4131, 'grad_norm': 2.128904342651367, 'learning_rate': 3.142221852062384e-05, 'epoch': 1.43}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0562455654144287, 'eval_runtime': 12.6414, 'eval_samples_per_second': 129.258, 'eval_steps_per_second': 16.217, 'epoch': 1.43}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.2555, 'grad_norm': 0.5416976809501648, 'learning_rate': 3.1393538485686816e-05, 'epoch': 1.43}
{'loss': 2.7924, 'grad_norm': 0.8062795996665955, 'learning_rate': 3.136484944472588e-05, 'epoch': 1.43}
{'loss': 2.7391, 'grad_norm': 0.8002685904502869, 'learning_rate': 3.13361514381528e-05, 'epoch': 1.43}
{'loss': 2.7153, 'grad_norm': 0.969500720500946, 'learning_rate': 3.1307444506391985e-05, 'epoch': 1.43}
{'loss': 2.8658, 'grad_norm': 1.2681692838668823, 'learning_rate': 3.127872868988039e-05, 'epoch': 1.43}
{'loss': 2.955, 'grad_norm': 1.5013296604156494, 'learning_rate': 3.125000402906749e-05, 'epoch': 1.43}
{'loss': 3.4986, 'grad_norm': 2.364074468612671, 'learning_rate': 3.122127056441526e-05, 'epoch': 1.44}
{'loss': 3.5663, 'grad_norm': 2.159339666366577, 'learning_rate': 3.119252833639801e-05, 'epoch': 1.44}
{'loss': 3.5, 'grad_norm': 1.9573966264724731, 'learning_rate': 3.116377738550243e-05, 'epoch': 1.44}
{'loss': 3.436, 'grad_norm': 2.1908440589904785, 'learning_rate': 3.113501775222752e-05, 'epoch': 1.44}
{'loss': 3.1768, 'grad_norm': 0.537073016166687, 'learning_rate': 3.110624947708446e-05, 'epoch': 1.44}
{'loss': 2.8071, 'grad_norm': 0.6454381942749023, 'learning_rate': 3.107747260059664e-05, 'epoch': 1.44}
{'loss': 2.7765, 'grad_norm': 0.7837048768997192, 'learning_rate': 3.104868716329955e-05, 'epoch': 1.44}
{'loss': 2.7612, 'grad_norm': 0.9210700988769531, 'learning_rate': 3.101989320574073e-05, 'epoch': 1.44}
{'loss': 2.8773, 'grad_norm': 1.2115285396575928, 'learning_rate': 3.099109076847975e-05, 'epoch': 1.44}
{'loss': 3.0388, 'grad_norm': 1.4590572118759155, 'learning_rate': 3.096227989208811e-05, 'epoch': 1.44}
{'loss': 3.2054, 'grad_norm': 1.8080195188522339, 'learning_rate': 3.093346061714919e-05, 'epoch': 1.45}
{'loss': 3.3391, 'grad_norm': 2.337679386138916, 'learning_rate': 3.0904632984258204e-05, 'epoch': 1.45}
{'loss': 3.5659, 'grad_norm': 2.030881404876709, 'learning_rate': 3.087579703402214e-05, 'epoch': 1.45}
{'loss': 3.5467, 'grad_norm': 2.0928516387939453, 'learning_rate': 3.0846952807059714e-05, 'epoch': 1.45}
{'loss': 3.1952, 'grad_norm': 0.5329753756523132, 'learning_rate': 3.0818100344001285e-05, 'epoch': 1.45}
{'loss': 2.7905, 'grad_norm': 0.7385249733924866, 'learning_rate': 3.0789239685488816e-05, 'epoch': 1.45}
{'loss': 2.7314, 'grad_norm': 0.857563853263855, 'learning_rate': 3.076037087217582e-05, 'epoch': 1.45}
{'loss': 2.7705, 'grad_norm': 1.0636210441589355, 'learning_rate': 3.07314939447273e-05, 'epoch': 1.45}
{'loss': 2.8557, 'grad_norm': 1.3315151929855347, 'learning_rate': 3.070260894381966e-05, 'epoch': 1.45}
{'loss': 3.0634, 'grad_norm': 1.6664884090423584, 'learning_rate': 3.067371591014073e-05, 'epoch': 1.45}
{'loss': 3.3529, 'grad_norm': 1.8517322540283203, 'learning_rate': 3.0644814884389594e-05, 'epoch': 1.46}
{'loss': 3.5375, 'grad_norm': 2.034792900085449, 'learning_rate': 3.061590590727665e-05, 'epoch': 1.46}
{'loss': 3.4828, 'grad_norm': 2.0534093379974365, 'learning_rate': 3.0586989019523446e-05, 'epoch': 1.46}
{'loss': 3.528, 'grad_norm': 2.2645316123962402, 'learning_rate': 3.0558064261862725e-05, 'epoch': 1.46}
{'loss': 3.1915, 'grad_norm': 0.5225701928138733, 'learning_rate': 3.052913167503827e-05, 'epoch': 1.46}
{'loss': 2.8058, 'grad_norm': 0.6888083815574646, 'learning_rate': 3.0500191299804905e-05, 'epoch': 1.46}
{'loss': 2.654, 'grad_norm': 0.7758792042732239, 'learning_rate': 3.047124317692845e-05, 'epoch': 1.46}
{'loss': 2.6758, 'grad_norm': 1.0240283012390137, 'learning_rate': 3.0442287347185594e-05, 'epoch': 1.46}
{'loss': 2.8438, 'grad_norm': 1.2530231475830078, 'learning_rate': 3.0413323851363924e-05, 'epoch': 1.46}
{'loss': 3.0084, 'grad_norm': 1.3988735675811768, 'learning_rate': 3.03843527302618e-05, 'epoch': 1.46}
{'loss': 3.3221, 'grad_norm': 1.8108501434326172, 'learning_rate': 3.0355374024688344e-05, 'epoch': 1.47}
{'loss': 3.494, 'grad_norm': 2.0007519721984863, 'learning_rate': 3.0326387775463328e-05, 'epoch': 1.47}
{'loss': 3.441, 'grad_norm': 2.102468252182007, 'learning_rate': 3.029739402341718e-05, 'epoch': 1.47}
{'loss': 3.5645, 'grad_norm': 2.1559839248657227, 'learning_rate': 3.0268392809390896e-05, 'epoch': 1.47}
{'loss': 3.1274, 'grad_norm': 0.5200291275978088, 'learning_rate': 3.0239384174235968e-05, 'epoch': 1.47}
{'loss': 2.7798, 'grad_norm': 0.6880714297294617, 'learning_rate': 3.0210368158814338e-05, 'epoch': 1.47}
{'loss': 2.7598, 'grad_norm': 0.8644417524337769, 'learning_rate': 3.0181344803998367e-05, 'epoch': 1.47}
{'loss': 2.7841, 'grad_norm': 0.9794428944587708, 'learning_rate': 3.0152314150670737e-05, 'epoch': 1.47}
{'loss': 2.8352, 'grad_norm': 1.1723202466964722, 'learning_rate': 3.0123276239724413e-05, 'epoch': 1.47}
{'loss': 2.8121, 'grad_norm': 1.4826596975326538, 'learning_rate': 3.0094231112062593e-05, 'epoch': 1.47}
{'loss': 3.4811, 'grad_norm': 1.7966800928115845, 'learning_rate': 3.0065178808598622e-05, 'epoch': 1.48}
{'loss': 3.55, 'grad_norm': 1.7868454456329346, 'learning_rate': 3.0036119370255967e-05, 'epoch': 1.48}
{'loss': 3.5137, 'grad_norm': 2.2303807735443115, 'learning_rate': 3.0007052837968148e-05, 'epoch': 1.48}
{'loss': 3.3999, 'grad_norm': 2.3248507976531982, 'learning_rate': 2.9977979252678662e-05, 'epoch': 1.48}
{'loss': 3.0997, 'grad_norm': 0.5552372932434082, 'learning_rate': 2.994889865534095e-05, 'epoch': 1.48}
{'loss': 2.7725, 'grad_norm': 0.727726399898529, 'learning_rate': 2.9919811086918348e-05, 'epoch': 1.48}
{'loss': 2.7532, 'grad_norm': 0.908856213092804, 'learning_rate': 2.9890716588383978e-05, 'epoch': 1.48}
{'loss': 2.9111, 'grad_norm': 1.017950415611267, 'learning_rate': 2.9861615200720737e-05, 'epoch': 1.48}
{'loss': 2.8629, 'grad_norm': 1.3342725038528442, 'learning_rate': 2.9832506964921247e-05, 'epoch': 1.48}
{'loss': 2.9679, 'grad_norm': 1.4272828102111816, 'learning_rate': 2.980339192198774e-05, 'epoch': 1.49}
{'loss': 3.0821, 'grad_norm': 1.5432102680206299, 'learning_rate': 2.977427011293207e-05, 'epoch': 1.49}
{'loss': 3.6392, 'grad_norm': 2.0071704387664795, 'learning_rate': 2.9745141578775597e-05, 'epoch': 1.49}
{'loss': 3.6496, 'grad_norm': 2.1489927768707275, 'learning_rate': 2.9716006360549164e-05, 'epoch': 1.49}
{'loss': 3.584, 'grad_norm': 2.0064785480499268, 'learning_rate': 2.9686864499293037e-05, 'epoch': 1.49}
{'loss': 3.1634, 'grad_norm': 0.580366313457489, 'learning_rate': 2.9657716036056827e-05, 'epoch': 1.49}
{'loss': 2.6845, 'grad_norm': 0.6562438607215881, 'learning_rate': 2.9628561011899454e-05, 'epoch': 1.49}
{'loss': 2.7218, 'grad_norm': 0.809378981590271, 'learning_rate': 2.9599399467889068e-05, 'epoch': 1.49}
{'loss': 2.7049, 'grad_norm': 0.9026931524276733, 'learning_rate': 2.9570231445103015e-05, 'epoch': 1.49}
{'loss': 2.8611, 'grad_norm': 1.2561031579971313, 'learning_rate': 2.954105698462776e-05, 'epoch': 1.49}
{'loss': 2.8528, 'grad_norm': 1.5725724697113037, 'learning_rate': 2.9511876127558848e-05, 'epoch': 1.5}
{'loss': 3.3901, 'grad_norm': 1.8798832893371582, 'learning_rate': 2.948268891500081e-05, 'epoch': 1.5}
{'loss': 3.5479, 'grad_norm': 2.050447463989258, 'learning_rate': 2.9453495388067154e-05, 'epoch': 1.5}
{'loss': 3.5069, 'grad_norm': 1.9369359016418457, 'learning_rate': 2.9424295587880268e-05, 'epoch': 1.5}
{'loss': 3.368, 'grad_norm': 2.201761245727539, 'learning_rate': 2.9395089555571386e-05, 'epoch': 1.5}
{'loss': 3.159, 'grad_norm': 0.5191066861152649, 'learning_rate': 2.9365877332280522e-05, 'epoch': 1.5}
{'loss': 2.6557, 'grad_norm': 0.6811538934707642, 'learning_rate': 2.9336658959156387e-05, 'epoch': 1.5}
{'loss': 2.7419, 'grad_norm': 5.1478047370910645, 'learning_rate': 2.930743447735638e-05, 'epoch': 1.5}
{'loss': 2.7332, 'grad_norm': 0.8835010528564453, 'learning_rate': 2.9278203928046504e-05, 'epoch': 1.5}
{'loss': 2.7934, 'grad_norm': 1.1151829957962036, 'learning_rate': 2.9248967352401302e-05, 'epoch': 1.5}
{'loss': 2.9087, 'grad_norm': 1.549974799156189, 'learning_rate': 2.92197247916038e-05, 'epoch': 1.51}
{'loss': 3.3564, 'grad_norm': 1.9134867191314697, 'learning_rate': 2.919047628684546e-05, 'epoch': 1.51}
{'loss': 3.6602, 'grad_norm': 1.9655543565750122, 'learning_rate': 2.916122187932612e-05, 'epoch': 1.51}
{'loss': 3.4889, 'grad_norm': 1.903745412826538, 'learning_rate': 2.913196161025393e-05, 'epoch': 1.51}
{'loss': 3.5346, 'grad_norm': 2.3692710399627686, 'learning_rate': 2.91026955208453e-05, 'epoch': 1.51}
{'loss': 3.0765, 'grad_norm': 0.4892212450504303, 'learning_rate': 2.907342365232483e-05, 'epoch': 1.51}
{'loss': 2.7917, 'grad_norm': 0.7349326610565186, 'learning_rate': 2.904414604592527e-05, 'epoch': 1.51}
{'loss': 2.6789, 'grad_norm': 0.865019679069519, 'learning_rate': 2.901486274288744e-05, 'epoch': 1.51}
{'loss': 2.6845, 'grad_norm': 0.9819385409355164, 'learning_rate': 2.8985573784460206e-05, 'epoch': 1.51}
{'loss': 2.752, 'grad_norm': 1.2182940244674683, 'learning_rate': 2.8956279211900378e-05, 'epoch': 1.51}
{'loss': 3.0203, 'grad_norm': 1.394052505493164, 'learning_rate': 2.892697906647268e-05, 'epoch': 1.52}
{'loss': 3.3511, 'grad_norm': 1.6943002939224243, 'learning_rate': 2.8897673389449702e-05, 'epoch': 1.52}
{'loss': 3.4694, 'grad_norm': 1.8229401111602783, 'learning_rate': 2.88683622221118e-05, 'epoch': 1.52}
{'loss': 3.404, 'grad_norm': 1.9927074909210205, 'learning_rate': 2.8839045605747084e-05, 'epoch': 1.52}
{'loss': 3.5314, 'grad_norm': 2.2385387420654297, 'learning_rate': 2.8809723581651327e-05, 'epoch': 1.52}
{'loss': 3.0688, 'grad_norm': 0.7113590836524963, 'learning_rate': 2.8780396191127935e-05, 'epoch': 1.52}
{'loss': 2.8194, 'grad_norm': 0.6760299205780029, 'learning_rate': 2.875106347548785e-05, 'epoch': 1.52}
{'loss': 2.7305, 'grad_norm': 0.787589430809021, 'learning_rate': 2.8721725476049533e-05, 'epoch': 1.52}
{'loss': 2.783, 'grad_norm': 1.0715280771255493, 'learning_rate': 2.869238223413888e-05, 'epoch': 1.52}
{'loss': 2.8055, 'grad_norm': 1.2571663856506348, 'learning_rate': 2.866303379108919e-05, 'epoch': 1.52}
{'loss': 2.8987, 'grad_norm': 1.6567559242248535, 'learning_rate': 2.8633680188241053e-05, 'epoch': 1.53}
{'loss': 3.2784, 'grad_norm': 1.7866129875183105, 'learning_rate': 2.8604321466942356e-05, 'epoch': 1.53}
{'loss': 3.5153, 'grad_norm': 1.9486087560653687, 'learning_rate': 2.8574957668548186e-05, 'epoch': 1.53}
{'loss': 3.6423, 'grad_norm': 1.906858205795288, 'learning_rate': 2.8545588834420776e-05, 'epoch': 1.53}
{'loss': 3.6439, 'grad_norm': 2.4695475101470947, 'learning_rate': 2.8516215005929482e-05, 'epoch': 1.53}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.052600383758545, 'eval_runtime': 12.5357, 'eval_samples_per_second': 130.348, 'eval_steps_per_second': 16.353, 'epoch': 1.53}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.1219, 'grad_norm': 0.6029891967773438, 'learning_rate': 2.848683622445064e-05, 'epoch': 1.53}
{'loss': 2.8023, 'grad_norm': 0.7167566418647766, 'learning_rate': 2.8457452531367612e-05, 'epoch': 1.53}
{'loss': 2.7084, 'grad_norm': 0.794292688369751, 'learning_rate': 2.8428063968070667e-05, 'epoch': 1.53}
{'loss': 2.8462, 'grad_norm': 1.0543495416641235, 'learning_rate': 2.8398670575956916e-05, 'epoch': 1.53}
{'loss': 2.7831, 'grad_norm': 1.5173163414001465, 'learning_rate': 2.836927239643029e-05, 'epoch': 1.54}
{'loss': 3.0591, 'grad_norm': 1.5718131065368652, 'learning_rate': 2.833986947090146e-05, 'epoch': 1.54}
{'loss': 3.388, 'grad_norm': 1.946194052696228, 'learning_rate': 2.8310461840787776e-05, 'epoch': 1.54}
{'loss': 3.5431, 'grad_norm': 1.9820386171340942, 'learning_rate': 2.8281049547513223e-05, 'epoch': 1.54}
{'loss': 3.5829, 'grad_norm': 2.4157774448394775, 'learning_rate': 2.825163263250835e-05, 'epoch': 1.54}
{'loss': 3.5305, 'grad_norm': 2.4757938385009766, 'learning_rate': 2.8222211137210226e-05, 'epoch': 1.54}
{'loss': 3.0914, 'grad_norm': 0.5654719471931458, 'learning_rate': 2.8192785103062345e-05, 'epoch': 1.54}
{'loss': 2.7835, 'grad_norm': 0.6725344657897949, 'learning_rate': 2.8163354571514623e-05, 'epoch': 1.54}
{'loss': 2.6877, 'grad_norm': 0.7467455267906189, 'learning_rate': 2.8133919584023305e-05, 'epoch': 1.54}
{'loss': 2.7804, 'grad_norm': 1.0011022090911865, 'learning_rate': 2.81044801820509e-05, 'epoch': 1.54}
{'loss': 2.8806, 'grad_norm': 1.171644926071167, 'learning_rate': 2.8075036407066145e-05, 'epoch': 1.55}
{'loss': 2.9058, 'grad_norm': 1.2387112379074097, 'learning_rate': 2.804558830054395e-05, 'epoch': 1.55}
{'loss': 3.3164, 'grad_norm': 1.8138010501861572, 'learning_rate': 2.801613590396529e-05, 'epoch': 1.55}
{'loss': 3.5172, 'grad_norm': 1.8397767543792725, 'learning_rate': 2.7986679258817216e-05, 'epoch': 1.55}
{'loss': 3.4095, 'grad_norm': 2.0815346240997314, 'learning_rate': 2.7957218406592762e-05, 'epoch': 1.55}
{'loss': 3.5929, 'grad_norm': 2.0956761837005615, 'learning_rate': 2.7927753388790856e-05, 'epoch': 1.55}
{'loss': 3.0982, 'grad_norm': 0.5465092658996582, 'learning_rate': 2.7898284246916335e-05, 'epoch': 1.55}
{'loss': 2.7879, 'grad_norm': 0.677395761013031, 'learning_rate': 2.7868811022479817e-05, 'epoch': 1.55}
{'loss': 2.6227, 'grad_norm': 0.773307740688324, 'learning_rate': 2.7839333756997692e-05, 'epoch': 1.55}
{'loss': 2.7974, 'grad_norm': 1.9233318567276, 'learning_rate': 2.7809852491992018e-05, 'epoch': 1.55}
{'loss': 2.93, 'grad_norm': 1.1850621700286865, 'learning_rate': 2.7780367268990504e-05, 'epoch': 1.56}
{'loss': 3.0786, 'grad_norm': 1.4756945371627808, 'learning_rate': 2.7750878129526437e-05, 'epoch': 1.56}
{'loss': 3.2482, 'grad_norm': 1.8001893758773804, 'learning_rate': 2.7721385115138602e-05, 'epoch': 1.56}
{'loss': 3.6306, 'grad_norm': 2.377176523208618, 'learning_rate': 2.7691888267371265e-05, 'epoch': 1.56}
{'loss': 3.451, 'grad_norm': 2.1640312671661377, 'learning_rate': 2.7662387627774077e-05, 'epoch': 1.56}
{'loss': 3.3858, 'grad_norm': 2.394646644592285, 'learning_rate': 2.7632883237902025e-05, 'epoch': 1.56}
{'loss': 3.177, 'grad_norm': 0.5349488258361816, 'learning_rate': 2.7603375139315397e-05, 'epoch': 1.56}
{'loss': 2.6632, 'grad_norm': 0.6608669757843018, 'learning_rate': 2.75738633735797e-05, 'epoch': 1.56}
{'loss': 2.7964, 'grad_norm': 9.467303276062012, 'learning_rate': 2.7544347982265588e-05, 'epoch': 1.56}
{'loss': 2.8195, 'grad_norm': 1.0846941471099854, 'learning_rate': 2.7514829006948844e-05, 'epoch': 1.56}
{'loss': 2.8515, 'grad_norm': 1.265085220336914, 'learning_rate': 2.7485306489210295e-05, 'epoch': 1.57}
{'loss': 2.9795, 'grad_norm': 1.4815987348556519, 'learning_rate': 2.7455780470635755e-05, 'epoch': 1.57}
{'loss': 3.0174, 'grad_norm': 1.78231680393219, 'learning_rate': 2.742625099281596e-05, 'epoch': 1.57}
{'loss': 3.5447, 'grad_norm': 2.052719831466675, 'learning_rate': 2.7396718097346536e-05, 'epoch': 1.57}
{'loss': 3.5505, 'grad_norm': 1.9279417991638184, 'learning_rate': 2.7367181825827914e-05, 'epoch': 1.57}
{'loss': 3.6186, 'grad_norm': 2.036234140396118, 'learning_rate': 2.733764221986528e-05, 'epoch': 1.57}
{'loss': 3.1899, 'grad_norm': 0.5073983669281006, 'learning_rate': 2.7308099321068514e-05, 'epoch': 1.57}
{'loss': 2.7546, 'grad_norm': 0.6424434781074524, 'learning_rate': 2.7278553171052146e-05, 'epoch': 1.57}
{'loss': 2.7118, 'grad_norm': 0.7640424966812134, 'learning_rate': 2.7249003811435274e-05, 'epoch': 1.57}
{'loss': 2.7561, 'grad_norm': 0.9895849227905273, 'learning_rate': 2.7219451283841525e-05, 'epoch': 1.57}
{'loss': 2.8583, 'grad_norm': 1.083802342414856, 'learning_rate': 2.7189895629898987e-05, 'epoch': 1.58}
{'loss': 2.9109, 'grad_norm': 1.609381914138794, 'learning_rate': 2.7160336891240146e-05, 'epoch': 1.58}
{'loss': 3.0933, 'grad_norm': 1.8978841304779053, 'learning_rate': 2.7130775109501827e-05, 'epoch': 1.58}
{'loss': 3.447, 'grad_norm': 2.013793468475342, 'learning_rate': 2.710121032632517e-05, 'epoch': 1.58}
{'loss': 3.5855, 'grad_norm': 1.9157764911651611, 'learning_rate': 2.7071642583355516e-05, 'epoch': 1.58}
{'loss': 3.6138, 'grad_norm': 2.1984922885894775, 'learning_rate': 2.7042071922242386e-05, 'epoch': 1.58}
{'loss': 3.1819, 'grad_norm': 0.9841598868370056, 'learning_rate': 2.70124983846394e-05, 'epoch': 1.58}
{'loss': 2.7634, 'grad_norm': 0.6729269623756409, 'learning_rate': 2.698292201220425e-05, 'epoch': 1.58}
{'loss': 2.69, 'grad_norm': 0.8503113389015198, 'learning_rate': 2.69533428465986e-05, 'epoch': 1.58}
{'loss': 2.6848, 'grad_norm': 0.9584378600120544, 'learning_rate': 2.6923760929488073e-05, 'epoch': 1.59}
{'loss': 2.8822, 'grad_norm': 1.0921998023986816, 'learning_rate': 2.6894176302542146e-05, 'epoch': 1.59}
{'loss': 2.8929, 'grad_norm': 1.6231719255447388, 'learning_rate': 2.6864589007434117e-05, 'epoch': 1.59}
{'loss': 3.2166, 'grad_norm': 1.8118067979812622, 'learning_rate': 2.683499908584104e-05, 'epoch': 1.59}
{'loss': 3.418, 'grad_norm': 1.9686745405197144, 'learning_rate': 2.6805406579443698e-05, 'epoch': 1.59}
{'loss': 3.6468, 'grad_norm': 2.024308681488037, 'learning_rate': 2.677581152992647e-05, 'epoch': 1.59}
{'loss': 3.563, 'grad_norm': 2.2059221267700195, 'learning_rate': 2.6746213978977347e-05, 'epoch': 1.59}
{'loss': 3.0705, 'grad_norm': 0.533202052116394, 'learning_rate': 2.6716613968287834e-05, 'epoch': 1.59}
{'loss': 2.7142, 'grad_norm': 0.6390272378921509, 'learning_rate': 2.6687011539552903e-05, 'epoch': 1.59}
{'loss': 2.7247, 'grad_norm': 0.6924059987068176, 'learning_rate': 2.6657406734470935e-05, 'epoch': 1.59}
{'loss': 2.7109, 'grad_norm': 0.9365698099136353, 'learning_rate': 2.6627799594743652e-05, 'epoch': 1.6}
{'loss': 2.8789, 'grad_norm': 1.268235683441162, 'learning_rate': 2.659819016207606e-05, 'epoch': 1.6}
{'loss': 2.7325, 'grad_norm': 1.2571460008621216, 'learning_rate': 2.656857847817641e-05, 'epoch': 1.6}
{'loss': 3.0267, 'grad_norm': 1.8763355016708374, 'learning_rate': 2.6538964584756116e-05, 'epoch': 1.6}
{'loss': 3.5927, 'grad_norm': 1.967584490776062, 'learning_rate': 2.6509348523529708e-05, 'epoch': 1.6}
{'loss': 3.5901, 'grad_norm': 2.0796501636505127, 'learning_rate': 2.6479730336214765e-05, 'epoch': 1.6}
{'loss': 3.6056, 'grad_norm': 2.0911457538604736, 'learning_rate': 2.6450110064531852e-05, 'epoch': 1.6}
{'loss': 3.1892, 'grad_norm': 0.5699959993362427, 'learning_rate': 2.642048775020449e-05, 'epoch': 1.6}
{'loss': 2.7865, 'grad_norm': 0.6803262233734131, 'learning_rate': 2.639086343495907e-05, 'epoch': 1.6}
{'loss': 2.693, 'grad_norm': 0.757620096206665, 'learning_rate': 2.6361237160524787e-05, 'epoch': 1.6}
{'loss': 2.7022, 'grad_norm': 1.0208765268325806, 'learning_rate': 2.6331608968633626e-05, 'epoch': 1.61}
{'loss': 2.7518, 'grad_norm': 1.1804784536361694, 'learning_rate': 2.6301978901020235e-05, 'epoch': 1.61}
{'loss': 2.8346, 'grad_norm': 1.4527029991149902, 'learning_rate': 2.627234699942193e-05, 'epoch': 1.61}
{'loss': 3.289, 'grad_norm': 1.6724222898483276, 'learning_rate': 2.624271330557862e-05, 'epoch': 1.61}
{'loss': 3.4077, 'grad_norm': 1.8235782384872437, 'learning_rate': 2.6213077861232698e-05, 'epoch': 1.61}
{'loss': 3.3746, 'grad_norm': 2.127382278442383, 'learning_rate': 2.6183440708129065e-05, 'epoch': 1.61}
{'loss': 3.5423, 'grad_norm': 2.2087409496307373, 'learning_rate': 2.6153801888015006e-05, 'epoch': 1.61}
{'loss': 3.0764, 'grad_norm': 0.5018025040626526, 'learning_rate': 2.612416144264016e-05, 'epoch': 1.61}
{'loss': 2.7417, 'grad_norm': 0.6959406137466431, 'learning_rate': 2.6094519413756456e-05, 'epoch': 1.61}
{'loss': 2.7263, 'grad_norm': 0.8987245559692383, 'learning_rate': 2.6064875843118052e-05, 'epoch': 1.61}
{'loss': 2.7205, 'grad_norm': 1.0959837436676025, 'learning_rate': 2.6035230772481285e-05, 'epoch': 1.62}
{'loss': 2.9504, 'grad_norm': 1.2311091423034668, 'learning_rate': 2.6005584243604582e-05, 'epoch': 1.62}
{'loss': 2.9979, 'grad_norm': 1.4308834075927734, 'learning_rate': 2.5975936298248454e-05, 'epoch': 1.62}
{'loss': 3.43, 'grad_norm': 1.8025749921798706, 'learning_rate': 2.5946286978175393e-05, 'epoch': 1.62}
{'loss': 3.6383, 'grad_norm': 3.103442907333374, 'learning_rate': 2.5916636325149823e-05, 'epoch': 1.62}
{'loss': 3.5614, 'grad_norm': 2.368588447570801, 'learning_rate': 2.588698438093805e-05, 'epoch': 1.62}
{'loss': 3.6082, 'grad_norm': 2.3201687335968018, 'learning_rate': 2.5857331187308216e-05, 'epoch': 1.62}
{'loss': 2.9793, 'grad_norm': 0.5140131115913391, 'learning_rate': 2.582767678603018e-05, 'epoch': 1.62}
{'loss': 2.7331, 'grad_norm': 0.721007227897644, 'learning_rate': 2.5798021218875546e-05, 'epoch': 1.62}
{'loss': 2.796, 'grad_norm': 0.8433743715286255, 'learning_rate': 2.5768364527617545e-05, 'epoch': 1.62}
{'loss': 2.8412, 'grad_norm': 1.098050594329834, 'learning_rate': 2.5738706754030974e-05, 'epoch': 1.63}
{'loss': 2.8945, 'grad_norm': 1.2139921188354492, 'learning_rate': 2.5709047939892183e-05, 'epoch': 1.63}
{'loss': 2.8625, 'grad_norm': 1.3259457349777222, 'learning_rate': 2.567938812697897e-05, 'epoch': 1.63}
{'loss': 3.1148, 'grad_norm': 1.6251444816589355, 'learning_rate': 2.564972735707055e-05, 'epoch': 1.63}
{'loss': 3.4075, 'grad_norm': 1.7471652030944824, 'learning_rate': 2.5620065671947473e-05, 'epoch': 1.63}
{'loss': 3.4679, 'grad_norm': 2.022703170776367, 'learning_rate': 2.5590403113391585e-05, 'epoch': 1.63}
{'loss': 3.3945, 'grad_norm': 2.258178949356079, 'learning_rate': 2.5560739723185977e-05, 'epoch': 1.63}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0472371578216553, 'eval_runtime': 12.4954, 'eval_samples_per_second': 130.768, 'eval_steps_per_second': 16.406, 'epoch': 1.63}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.044, 'grad_norm': 0.5556140542030334, 'learning_rate': 2.5531075543114878e-05, 'epoch': 1.63}
{'loss': 2.8555, 'grad_norm': 0.7421054840087891, 'learning_rate': 2.5501410614963667e-05, 'epoch': 1.63}
{'loss': 2.7308, 'grad_norm': 0.843033492565155, 'learning_rate': 2.5471744980518752e-05, 'epoch': 1.64}
{'loss': 2.8029, 'grad_norm': 1.1628199815750122, 'learning_rate': 2.5442078681567532e-05, 'epoch': 1.64}
{'loss': 2.8083, 'grad_norm': 1.5108057260513306, 'learning_rate': 2.541241175989837e-05, 'epoch': 1.64}
{'loss': 3.0483, 'grad_norm': 1.4291132688522339, 'learning_rate': 2.538274425730047e-05, 'epoch': 1.64}
{'loss': 3.5681, 'grad_norm': 1.9093677997589111, 'learning_rate': 2.53530762155639e-05, 'epoch': 1.64}
{'loss': 3.6217, 'grad_norm': 2.243250608444214, 'learning_rate': 2.532340767647944e-05, 'epoch': 1.64}
{'loss': 3.5796, 'grad_norm': 1.9847935438156128, 'learning_rate': 2.5293738681838597e-05, 'epoch': 1.64}
{'loss': 3.4855, 'grad_norm': 2.3052313327789307, 'learning_rate': 2.526406927343351e-05, 'epoch': 1.64}
{'loss': 3.1847, 'grad_norm': 0.5631179213523865, 'learning_rate': 2.523439949305691e-05, 'epoch': 1.64}
{'loss': 2.7015, 'grad_norm': 0.7709197998046875, 'learning_rate': 2.5204729382502053e-05, 'epoch': 1.64}
{'loss': 2.6985, 'grad_norm': 0.8289231061935425, 'learning_rate': 2.5175058983562645e-05, 'epoch': 1.65}
{'loss': 2.7182, 'grad_norm': 0.9670151472091675, 'learning_rate': 2.51453883380328e-05, 'epoch': 1.65}
{'loss': 2.9446, 'grad_norm': 1.2890825271606445, 'learning_rate': 2.5115717487707003e-05, 'epoch': 1.65}
{'loss': 3.1185, 'grad_norm': 1.5224875211715698, 'learning_rate': 2.5086046474380005e-05, 'epoch': 1.65}
{'loss': 3.3308, 'grad_norm': 2.132155418395996, 'learning_rate': 2.5056375339846787e-05, 'epoch': 1.65}
{'loss': 3.7165, 'grad_norm': 1.921739935874939, 'learning_rate': 2.5026704125902513e-05, 'epoch': 1.65}
{'loss': 3.3889, 'grad_norm': 2.0848865509033203, 'learning_rate': 2.499703287434245e-05, 'epoch': 1.65}
{'loss': 3.5244, 'grad_norm': 2.136375665664673, 'learning_rate': 2.4967361626961923e-05, 'epoch': 1.65}
{'loss': 3.175, 'grad_norm': 0.5014855265617371, 'learning_rate': 2.4937690425556237e-05, 'epoch': 1.65}
{'loss': 2.7459, 'grad_norm': 0.6999830007553101, 'learning_rate': 2.4908019311920657e-05, 'epoch': 1.65}
{'loss': 2.6699, 'grad_norm': 0.7804405093193054, 'learning_rate': 2.4878348327850302e-05, 'epoch': 1.66}
{'loss': 2.676, 'grad_norm': 1.04673433303833, 'learning_rate': 2.484867751514013e-05, 'epoch': 1.66}
{'loss': 2.8873, 'grad_norm': 12.736565589904785, 'learning_rate': 2.4819006915584832e-05, 'epoch': 1.66}
{'loss': 3.0006, 'grad_norm': 1.529941201210022, 'learning_rate': 2.478933657097881e-05, 'epoch': 1.66}
{'loss': 3.298, 'grad_norm': 1.8305301666259766, 'learning_rate': 2.4759666523116116e-05, 'epoch': 1.66}
{'loss': 3.547, 'grad_norm': 1.9381568431854248, 'learning_rate': 2.4729996813790378e-05, 'epoch': 1.66}
{'loss': 3.4539, 'grad_norm': 1.940899133682251, 'learning_rate': 2.4700327484794742e-05, 'epoch': 1.66}
{'loss': 3.5732, 'grad_norm': 2.1812808513641357, 'learning_rate': 2.467065857792183e-05, 'epoch': 1.66}
{'loss': 3.0742, 'grad_norm': 0.5574859380722046, 'learning_rate': 2.4640990134963647e-05, 'epoch': 1.66}
{'loss': 2.7934, 'grad_norm': 0.6793093085289001, 'learning_rate': 2.4611322197711567e-05, 'epoch': 1.66}
{'loss': 2.6989, 'grad_norm': 0.8071233630180359, 'learning_rate': 2.458165480795624e-05, 'epoch': 1.67}
{'loss': 2.7197, 'grad_norm': 1.007773756980896, 'learning_rate': 2.4551988007487553e-05, 'epoch': 1.67}
{'loss': 2.8018, 'grad_norm': 1.2513166666030884, 'learning_rate': 2.452232183809456e-05, 'epoch': 1.67}
{'loss': 2.9177, 'grad_norm': 1.5486516952514648, 'learning_rate': 2.4492656341565416e-05, 'epoch': 1.67}
{'loss': 3.4505, 'grad_norm': 1.8628840446472168, 'learning_rate': 2.4462991559687355e-05, 'epoch': 1.67}
{'loss': 3.4311, 'grad_norm': 1.8996914625167847, 'learning_rate': 2.4433327534246562e-05, 'epoch': 1.67}
{'loss': 3.5463, 'grad_norm': 1.99460768699646, 'learning_rate': 2.4403664307028186e-05, 'epoch': 1.67}
{'loss': 3.4406, 'grad_norm': 1.9823153018951416, 'learning_rate': 2.4374001919816256e-05, 'epoch': 1.67}
{'loss': 3.2296, 'grad_norm': 0.5579043626785278, 'learning_rate': 2.4344340414393603e-05, 'epoch': 1.67}
{'loss': 2.7016, 'grad_norm': 0.6541338562965393, 'learning_rate': 2.4314679832541818e-05, 'epoch': 1.67}
{'loss': 2.6195, 'grad_norm': 0.7340344190597534, 'learning_rate': 2.4285020216041208e-05, 'epoch': 1.68}
{'loss': 2.7273, 'grad_norm': 1.6251827478408813, 'learning_rate': 2.425536160667068e-05, 'epoch': 1.68}
{'loss': 2.8006, 'grad_norm': 1.1262308359146118, 'learning_rate': 2.4225704046207767e-05, 'epoch': 1.68}
{'loss': 2.8265, 'grad_norm': 1.407185673713684, 'learning_rate': 2.4196047576428498e-05, 'epoch': 1.68}
{'loss': 3.1784, 'grad_norm': 1.7444754838943481, 'learning_rate': 2.4166392239107377e-05, 'epoch': 1.68}
{'loss': 3.4403, 'grad_norm': 1.848062515258789, 'learning_rate': 2.4136738076017307e-05, 'epoch': 1.68}
{'loss': 3.576, 'grad_norm': 1.9537723064422607, 'learning_rate': 2.4107085128929545e-05, 'epoch': 1.68}
{'loss': 3.3163, 'grad_norm': 2.123236656188965, 'learning_rate': 2.407743343961363e-05, 'epoch': 1.68}
{'loss': 3.0441, 'grad_norm': 0.5545888543128967, 'learning_rate': 2.4047783049837315e-05, 'epoch': 1.68}
{'loss': 2.6698, 'grad_norm': 0.7890447378158569, 'learning_rate': 2.4018134001366545e-05, 'epoch': 1.69}
{'loss': 2.679, 'grad_norm': 0.9309960603713989, 'learning_rate': 2.3988486335965367e-05, 'epoch': 1.69}
{'loss': 2.6362, 'grad_norm': 1.2068090438842773, 'learning_rate': 2.3958840095395874e-05, 'epoch': 1.69}
{'loss': 2.9641, 'grad_norm': 1.344446063041687, 'learning_rate': 2.3929195321418166e-05, 'epoch': 1.69}
{'loss': 3.0062, 'grad_norm': 1.4440644979476929, 'learning_rate': 2.3899552055790268e-05, 'epoch': 1.69}
{'loss': 3.3356, 'grad_norm': 1.8871304988861084, 'learning_rate': 2.3869910340268077e-05, 'epoch': 1.69}
{'loss': 3.5648, 'grad_norm': 2.015897750854492, 'learning_rate': 2.3840270216605306e-05, 'epoch': 1.69}
{'loss': 3.4554, 'grad_norm': 2.103243112564087, 'learning_rate': 2.3810631726553434e-05, 'epoch': 1.69}
{'loss': 3.4888, 'grad_norm': 1.9185905456542969, 'learning_rate': 2.3780994911861635e-05, 'epoch': 1.69}
{'loss': 3.1893, 'grad_norm': 0.559794545173645, 'learning_rate': 2.3751359814276722e-05, 'epoch': 1.69}
{'loss': 2.7901, 'grad_norm': 0.6528561115264893, 'learning_rate': 2.37217264755431e-05, 'epoch': 1.7}
{'loss': 2.7137, 'grad_norm': 0.819667398929596, 'learning_rate': 2.369209493740267e-05, 'epoch': 1.7}
{'loss': 2.7762, 'grad_norm': 1.0511879920959473, 'learning_rate': 2.3662465241594823e-05, 'epoch': 1.7}
{'loss': 2.887, 'grad_norm': 1.4142483472824097, 'learning_rate': 2.3632837429856345e-05, 'epoch': 1.7}
{'loss': 2.884, 'grad_norm': 1.3296207189559937, 'learning_rate': 2.3603211543921368e-05, 'epoch': 1.7}
{'loss': 3.256, 'grad_norm': 1.548967719078064, 'learning_rate': 2.357358762552131e-05, 'epoch': 1.7}
{'loss': 3.3976, 'grad_norm': 1.8819184303283691, 'learning_rate': 2.3543965716384818e-05, 'epoch': 1.7}
{'loss': 3.7553, 'grad_norm': 1.9473003149032593, 'learning_rate': 2.3514345858237715e-05, 'epoch': 1.7}
{'loss': 3.5292, 'grad_norm': 2.2254672050476074, 'learning_rate': 2.3484728092802924e-05, 'epoch': 1.7}
{'loss': 3.1353, 'grad_norm': 0.48717451095581055, 'learning_rate': 2.3455112461800426e-05, 'epoch': 1.7}
{'loss': 2.7347, 'grad_norm': 0.7289718985557556, 'learning_rate': 2.34254990069472e-05, 'epoch': 1.71}
{'loss': 2.6768, 'grad_norm': 0.7944648265838623, 'learning_rate': 2.339588776995715e-05, 'epoch': 1.71}
{'loss': 2.7269, 'grad_norm': 2.775763511657715, 'learning_rate': 2.336627879254106e-05, 'epoch': 1.71}
{'loss': 2.9152, 'grad_norm': 1.4490042924880981, 'learning_rate': 2.3336672116406552e-05, 'epoch': 1.71}
{'loss': 2.9358, 'grad_norm': 1.459484338760376, 'learning_rate': 2.3307067783257955e-05, 'epoch': 1.71}
{'loss': 3.2826, 'grad_norm': 1.69206702709198, 'learning_rate': 2.3277465834796343e-05, 'epoch': 1.71}
{'loss': 3.6871, 'grad_norm': 2.03266978263855, 'learning_rate': 2.3247866312719423e-05, 'epoch': 1.71}
{'loss': 3.498, 'grad_norm': 2.4194605350494385, 'learning_rate': 2.3218269258721466e-05, 'epoch': 1.71}
{'loss': 3.5618, 'grad_norm': 2.2466752529144287, 'learning_rate': 2.318867471449329e-05, 'epoch': 1.71}
{'loss': 3.1851, 'grad_norm': 0.4748230576515198, 'learning_rate': 2.315908272172216e-05, 'epoch': 1.71}
{'loss': 2.7483, 'grad_norm': 0.5989791750907898, 'learning_rate': 2.3129493322091763e-05, 'epoch': 1.72}
{'loss': 2.5559, 'grad_norm': 0.7666867971420288, 'learning_rate': 2.3099906557282107e-05, 'epoch': 1.72}
{'loss': 2.8174, 'grad_norm': 1.0201890468597412, 'learning_rate': 2.3070322468969514e-05, 'epoch': 1.72}
{'loss': 2.7454, 'grad_norm': 1.2752633094787598, 'learning_rate': 2.3040741098826522e-05, 'epoch': 1.72}
{'loss': 2.8882, 'grad_norm': 1.4865282773971558, 'learning_rate': 2.3011162488521855e-05, 'epoch': 1.72}
{'loss': 3.1051, 'grad_norm': 1.6847355365753174, 'learning_rate': 2.2981586679720327e-05, 'epoch': 1.72}
{'loss': 3.5291, 'grad_norm': 2.0968616008758545, 'learning_rate': 2.2952013714082837e-05, 'epoch': 1.72}
{'loss': 3.5762, 'grad_norm': 1.88865065574646, 'learning_rate': 2.292244363326624e-05, 'epoch': 1.72}
{'loss': 3.5002, 'grad_norm': 2.1944525241851807, 'learning_rate': 2.2892876478923352e-05, 'epoch': 1.72}
{'loss': 3.204, 'grad_norm': 0.5823827385902405, 'learning_rate': 2.2863312292702868e-05, 'epoch': 1.72}
{'loss': 2.7218, 'grad_norm': 0.6464142203330994, 'learning_rate': 2.2833751116249296e-05, 'epoch': 1.73}
{'loss': 2.7351, 'grad_norm': 0.8029916286468506, 'learning_rate': 2.2804192991202904e-05, 'epoch': 1.73}
{'loss': 2.7143, 'grad_norm': 1.0020065307617188, 'learning_rate': 2.2774637959199657e-05, 'epoch': 1.73}
{'loss': 2.905, 'grad_norm': 1.207985281944275, 'learning_rate': 2.274508606187119e-05, 'epoch': 1.73}
{'loss': 3.0775, 'grad_norm': 1.6232746839523315, 'learning_rate': 2.2715537340844673e-05, 'epoch': 1.73}
{'loss': 3.3161, 'grad_norm': 1.8287352323532104, 'learning_rate': 2.2685991837742844e-05, 'epoch': 1.73}
{'loss': 3.6419, 'grad_norm': 2.081291437149048, 'learning_rate': 2.265644959418389e-05, 'epoch': 1.73}
{'loss': 3.4336, 'grad_norm': 1.89582359790802, 'learning_rate': 2.2626910651781413e-05, 'epoch': 1.73}
{'loss': 3.4266, 'grad_norm': 2.3714370727539062, 'learning_rate': 2.2597375052144364e-05, 'epoch': 1.73}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.042259454727173, 'eval_runtime': 12.5187, 'eval_samples_per_second': 130.524, 'eval_steps_per_second': 16.375, 'epoch': 1.73}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.272, 'grad_norm': 0.5183840394020081, 'learning_rate': 2.2567842836876994e-05, 'epoch': 1.74}
{'loss': 2.7587, 'grad_norm': 0.6593768000602722, 'learning_rate': 2.2538314047578754e-05, 'epoch': 1.74}
{'loss': 2.78, 'grad_norm': 1.590506672859192, 'learning_rate': 2.2508788725844306e-05, 'epoch': 1.74}
{'loss': 2.8418, 'grad_norm': 1.062843680381775, 'learning_rate': 2.247926691326341e-05, 'epoch': 1.74}
{'loss': 2.7255, 'grad_norm': 1.3257750272750854, 'learning_rate': 2.2449748651420887e-05, 'epoch': 1.74}
{'loss': 2.9838, 'grad_norm': 1.32272207736969, 'learning_rate': 2.242023398189656e-05, 'epoch': 1.74}
{'loss': 3.2664, 'grad_norm': 1.649627685546875, 'learning_rate': 2.239072294626519e-05, 'epoch': 1.74}
{'loss': 3.7514, 'grad_norm': 1.9674559831619263, 'learning_rate': 2.2361215586096407e-05, 'epoch': 1.74}
{'loss': 3.6588, 'grad_norm': 2.1562108993530273, 'learning_rate': 2.2331711942954673e-05, 'epoch': 1.74}
{'loss': 3.5686, 'grad_norm': 2.407057285308838, 'learning_rate': 2.230221205839922e-05, 'epoch': 1.74}
{'loss': 3.2148, 'grad_norm': 0.545019268989563, 'learning_rate': 2.2272715973983977e-05, 'epoch': 1.75}
{'loss': 2.8357, 'grad_norm': 0.6451585292816162, 'learning_rate': 2.224322373125753e-05, 'epoch': 1.75}
{'loss': 2.6507, 'grad_norm': 0.7453626394271851, 'learning_rate': 2.2213735371763045e-05, 'epoch': 1.75}
{'loss': 2.731, 'grad_norm': 0.9470551013946533, 'learning_rate': 2.218425093703822e-05, 'epoch': 1.75}
{'loss': 2.7996, 'grad_norm': 1.2522153854370117, 'learning_rate': 2.215477046861522e-05, 'epoch': 1.75}
{'loss': 2.9599, 'grad_norm': 1.3216018676757812, 'learning_rate': 2.212529400802063e-05, 'epoch': 1.75}
{'loss': 3.2197, 'grad_norm': 1.632776141166687, 'learning_rate': 2.2095821596775386e-05, 'epoch': 1.75}
{'loss': 3.4943, 'grad_norm': 2.170408010482788, 'learning_rate': 2.2066353276394722e-05, 'epoch': 1.75}
{'loss': 3.5235, 'grad_norm': 2.025092840194702, 'learning_rate': 2.2036889088388107e-05, 'epoch': 1.75}
{'loss': 3.5338, 'grad_norm': 2.150120496749878, 'learning_rate': 2.20074290742592e-05, 'epoch': 1.75}
{'loss': 3.1463, 'grad_norm': 0.5091276168823242, 'learning_rate': 2.197797327550576e-05, 'epoch': 1.76}
{'loss': 2.7209, 'grad_norm': 0.6854087710380554, 'learning_rate': 2.194852173361962e-05, 'epoch': 1.76}
{'loss': 2.6675, 'grad_norm': 0.7864442467689514, 'learning_rate': 2.1919074490086623e-05, 'epoch': 1.76}
{'loss': 2.7166, 'grad_norm': 0.9177365303039551, 'learning_rate': 2.1889631586386546e-05, 'epoch': 1.76}
{'loss': 2.8542, 'grad_norm': 1.2401479482650757, 'learning_rate': 2.1860193063993057e-05, 'epoch': 1.76}
{'loss': 2.9057, 'grad_norm': 1.5114738941192627, 'learning_rate': 2.1830758964373654e-05, 'epoch': 1.76}
{'loss': 3.3068, 'grad_norm': 1.7999192476272583, 'learning_rate': 2.1801329328989613e-05, 'epoch': 1.76}
{'loss': 3.4149, 'grad_norm': 1.9852694272994995, 'learning_rate': 2.1771904199295894e-05, 'epoch': 1.76}
{'loss': 3.5191, 'grad_norm': 2.0015954971313477, 'learning_rate': 2.1742483616741136e-05, 'epoch': 1.76}
{'loss': 3.2554, 'grad_norm': 2.444155693054199, 'learning_rate': 2.1713067622767568e-05, 'epoch': 1.76}
{'loss': 3.071, 'grad_norm': 0.578065812587738, 'learning_rate': 2.1683656258810954e-05, 'epoch': 1.77}
{'loss': 2.5997, 'grad_norm': 0.6887938976287842, 'learning_rate': 2.1654249566300534e-05, 'epoch': 1.77}
{'loss': 2.7959, 'grad_norm': 0.8088720440864563, 'learning_rate': 2.162484758665898e-05, 'epoch': 1.77}
{'loss': 2.8715, 'grad_norm': 0.9860926866531372, 'learning_rate': 2.159545036130229e-05, 'epoch': 1.77}
{'loss': 2.7398, 'grad_norm': 1.152767300605774, 'learning_rate': 2.1566057931639798e-05, 'epoch': 1.77}
{'loss': 2.9139, 'grad_norm': 1.4352515935897827, 'learning_rate': 2.153667033907408e-05, 'epoch': 1.77}
{'loss': 3.2988, 'grad_norm': 1.8271446228027344, 'learning_rate': 2.150728762500089e-05, 'epoch': 1.77}
{'loss': 3.523, 'grad_norm': 1.7662564516067505, 'learning_rate': 2.1477909830809113e-05, 'epoch': 1.77}
{'loss': 3.4243, 'grad_norm': 2.123213529586792, 'learning_rate': 2.1448536997880704e-05, 'epoch': 1.77}
{'loss': 3.4054, 'grad_norm': 2.1454875469207764, 'learning_rate': 2.1419169167590637e-05, 'epoch': 1.77}
{'loss': 3.1963, 'grad_norm': 0.5138231515884399, 'learning_rate': 2.1389806381306805e-05, 'epoch': 1.78}
{'loss': 2.8187, 'grad_norm': 0.6390589475631714, 'learning_rate': 2.1360448680390044e-05, 'epoch': 1.78}
{'loss': 2.6748, 'grad_norm': 0.866750955581665, 'learning_rate': 2.1331096106193997e-05, 'epoch': 1.78}
{'loss': 2.74, 'grad_norm': 0.9682913422584534, 'learning_rate': 2.1301748700065095e-05, 'epoch': 1.78}
{'loss': 2.9297, 'grad_norm': 1.2747985124588013, 'learning_rate': 2.127240650334248e-05, 'epoch': 1.78}
{'loss': 2.9464, 'grad_norm': 1.4160910844802856, 'learning_rate': 2.1243069557357988e-05, 'epoch': 1.78}
{'loss': 3.3364, 'grad_norm': 2.0754776000976562, 'learning_rate': 2.1213737903436005e-05, 'epoch': 1.78}
{'loss': 3.3824, 'grad_norm': 2.3318099975585938, 'learning_rate': 2.11844115828935e-05, 'epoch': 1.78}
{'loss': 3.4323, 'grad_norm': 1.9673197269439697, 'learning_rate': 2.115509063703992e-05, 'epoch': 1.78}
{'loss': 3.676, 'grad_norm': 2.2986207008361816, 'learning_rate': 2.1125775107177147e-05, 'epoch': 1.79}
{'loss': 3.244, 'grad_norm': 0.54438316822052, 'learning_rate': 2.1096465034599427e-05, 'epoch': 1.79}
{'loss': 2.6413, 'grad_norm': 0.6899540424346924, 'learning_rate': 2.1067160460593333e-05, 'epoch': 1.79}
{'loss': 2.6873, 'grad_norm': 0.8237379789352417, 'learning_rate': 2.1037861426437654e-05, 'epoch': 1.79}
{'loss': 2.7701, 'grad_norm': 0.9795558452606201, 'learning_rate': 2.1008567973403416e-05, 'epoch': 1.79}
{'loss': 2.8073, 'grad_norm': 1.2679048776626587, 'learning_rate': 2.097928014275377e-05, 'epoch': 1.79}
{'loss': 2.9511, 'grad_norm': 1.576733112335205, 'learning_rate': 2.094999797574394e-05, 'epoch': 1.79}
{'loss': 3.0631, 'grad_norm': 2.306110382080078, 'learning_rate': 2.0920721513621188e-05, 'epoch': 1.79}
{'loss': 3.4326, 'grad_norm': 2.0361011028289795, 'learning_rate': 2.0891450797624724e-05, 'epoch': 1.79}
{'loss': 3.5266, 'grad_norm': 2.1566660404205322, 'learning_rate': 2.086218586898568e-05, 'epoch': 1.79}
{'loss': 3.5074, 'grad_norm': 2.418429374694824, 'learning_rate': 2.0832926768927012e-05, 'epoch': 1.8}
{'loss': 3.1206, 'grad_norm': 0.5939661860466003, 'learning_rate': 2.0803673538663486e-05, 'epoch': 1.8}
{'loss': 2.6324, 'grad_norm': 0.7051149606704712, 'learning_rate': 2.077442621940159e-05, 'epoch': 1.8}
{'loss': 2.766, 'grad_norm': 0.9222372174263, 'learning_rate': 2.0745184852339496e-05, 'epoch': 1.8}
{'loss': 2.7416, 'grad_norm': 0.9818741083145142, 'learning_rate': 2.0715949478666985e-05, 'epoch': 1.8}
{'loss': 2.8542, 'grad_norm': 1.372430682182312, 'learning_rate': 2.0686720139565403e-05, 'epoch': 1.8}
{'loss': 3.0502, 'grad_norm': 1.5351907014846802, 'learning_rate': 2.065749687620757e-05, 'epoch': 1.8}
{'loss': 3.4509, 'grad_norm': 1.9553714990615845, 'learning_rate': 2.0628279729757773e-05, 'epoch': 1.8}
{'loss': 3.5998, 'grad_norm': 1.9373760223388672, 'learning_rate': 2.059906874137168e-05, 'epoch': 1.8}
{'loss': 3.4716, 'grad_norm': 2.1556954383850098, 'learning_rate': 2.0569863952196272e-05, 'epoch': 1.8}
{'loss': 3.5455, 'grad_norm': 2.264084815979004, 'learning_rate': 2.0540665403369814e-05, 'epoch': 1.81}
{'loss': 3.1296, 'grad_norm': 0.5084730982780457, 'learning_rate': 2.0511473136021776e-05, 'epoch': 1.81}
{'loss': 2.6358, 'grad_norm': 0.6657949090003967, 'learning_rate': 2.0482287191272772e-05, 'epoch': 1.81}
{'loss': 2.7992, 'grad_norm': 0.8229666352272034, 'learning_rate': 2.045310761023451e-05, 'epoch': 1.81}
{'loss': 2.7703, 'grad_norm': 1.0008116960525513, 'learning_rate': 2.0423934434009747e-05, 'epoch': 1.81}
{'loss': 2.8027, 'grad_norm': 1.153743863105774, 'learning_rate': 2.0394767703692204e-05, 'epoch': 1.81}
{'loss': 2.9465, 'grad_norm': 1.5468062162399292, 'learning_rate': 2.0365607460366523e-05, 'epoch': 1.81}
{'loss': 3.3137, 'grad_norm': 1.6832859516143799, 'learning_rate': 2.033645374510822e-05, 'epoch': 1.81}
{'loss': 3.5336, 'grad_norm': 1.9251912832260132, 'learning_rate': 2.0307306598983615e-05, 'epoch': 1.81}
{'loss': 3.4328, 'grad_norm': 1.9406570196151733, 'learning_rate': 2.0278166063049757e-05, 'epoch': 1.81}
{'loss': 3.3701, 'grad_norm': 2.165419816970825, 'learning_rate': 2.0249032178354398e-05, 'epoch': 1.82}
{'loss': 3.1704, 'grad_norm': 0.5645371675491333, 'learning_rate': 2.0219904985935915e-05, 'epoch': 1.82}
{'loss': 2.7151, 'grad_norm': 0.7145967483520508, 'learning_rate': 2.019078452682327e-05, 'epoch': 1.82}
{'loss': 2.7157, 'grad_norm': 0.7994406819343567, 'learning_rate': 2.016167084203593e-05, 'epoch': 1.82}
{'loss': 2.7931, 'grad_norm': 0.930457592010498, 'learning_rate': 2.0132563972583813e-05, 'epoch': 1.82}
{'loss': 2.9505, 'grad_norm': 1.307996153831482, 'learning_rate': 2.0103463959467254e-05, 'epoch': 1.82}
{'loss': 2.9765, 'grad_norm': 1.4265724420547485, 'learning_rate': 2.007437084367691e-05, 'epoch': 1.82}
{'loss': 3.4044, 'grad_norm': 1.7653752565383911, 'learning_rate': 2.0045284666193746e-05, 'epoch': 1.82}
{'loss': 3.5181, 'grad_norm': 1.9205560684204102, 'learning_rate': 2.0016205467988932e-05, 'epoch': 1.82}
{'loss': 3.5128, 'grad_norm': 2.1553680896759033, 'learning_rate': 1.9987133290023818e-05, 'epoch': 1.82}
{'loss': 3.4901, 'grad_norm': 2.245415210723877, 'learning_rate': 1.9958068173249866e-05, 'epoch': 1.83}
{'loss': 3.2252, 'grad_norm': 0.4735002815723419, 'learning_rate': 1.99290101586086e-05, 'epoch': 1.83}
{'loss': 2.7528, 'grad_norm': 0.8052153587341309, 'learning_rate': 1.9899959287031504e-05, 'epoch': 1.83}
{'loss': 2.8101, 'grad_norm': 0.8248575329780579, 'learning_rate': 1.987091559944003e-05, 'epoch': 1.83}
{'loss': 2.7766, 'grad_norm': 0.9637104272842407, 'learning_rate': 1.984187913674552e-05, 'epoch': 1.83}
{'loss': 2.714, 'grad_norm': 1.164773941040039, 'learning_rate': 1.981284993984911e-05, 'epoch': 1.83}
{'loss': 2.9445, 'grad_norm': 1.424933671951294, 'learning_rate': 1.9783828049641723e-05, 'epoch': 1.83}
{'loss': 3.0213, 'grad_norm': 1.6369657516479492, 'learning_rate': 1.975481350700399e-05, 'epoch': 1.83}
{'loss': 3.5777, 'grad_norm': 2.0144872665405273, 'learning_rate': 1.9725806352806163e-05, 'epoch': 1.83}
{'loss': 3.5264, 'grad_norm': 2.28755521774292, 'learning_rate': 1.969680662790812e-05, 'epoch': 1.84}
{'loss': 3.6271, 'grad_norm': 2.3714513778686523, 'learning_rate': 1.966781437315926e-05, 'epoch': 1.84}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.041106700897217, 'eval_runtime': 12.6211, 'eval_samples_per_second': 129.466, 'eval_steps_per_second': 16.243, 'epoch': 1.84}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.1918, 'grad_norm': 0.7228975892066956, 'learning_rate': 1.9638829629398462e-05, 'epoch': 1.84}
{'loss': 2.7505, 'grad_norm': 0.6363102197647095, 'learning_rate': 1.9609852437454023e-05, 'epoch': 1.84}
{'loss': 2.8032, 'grad_norm': 1.2727047204971313, 'learning_rate': 1.9580882838143606e-05, 'epoch': 1.84}
{'loss': 2.6949, 'grad_norm': 0.9617840647697449, 'learning_rate': 1.9551920872274183e-05, 'epoch': 1.84}
{'loss': 2.7861, 'grad_norm': 1.2086421251296997, 'learning_rate': 1.952296658064195e-05, 'epoch': 1.84}
{'loss': 2.7957, 'grad_norm': 1.4122772216796875, 'learning_rate': 1.9494020004032315e-05, 'epoch': 1.84}
{'loss': 3.2667, 'grad_norm': 1.6527501344680786, 'learning_rate': 1.9465081183219822e-05, 'epoch': 1.84}
{'loss': 3.432, 'grad_norm': 2.034792184829712, 'learning_rate': 1.9436150158968076e-05, 'epoch': 1.84}
{'loss': 3.6184, 'grad_norm': 2.0794100761413574, 'learning_rate': 1.94072269720297e-05, 'epoch': 1.85}
{'loss': 3.4777, 'grad_norm': 2.6250200271606445, 'learning_rate': 1.9378311663146303e-05, 'epoch': 1.85}
{'loss': 3.0474, 'grad_norm': 0.4923813045024872, 'learning_rate': 1.934940427304835e-05, 'epoch': 1.85}
{'loss': 2.734, 'grad_norm': 0.6218037605285645, 'learning_rate': 1.9320504842455188e-05, 'epoch': 1.85}
{'loss': 2.6787, 'grad_norm': 0.8383253216743469, 'learning_rate': 1.9291613412074945e-05, 'epoch': 1.85}
{'loss': 2.7252, 'grad_norm': 0.9385369420051575, 'learning_rate': 1.926273002260447e-05, 'epoch': 1.85}
{'loss': 2.6993, 'grad_norm': 1.1972429752349854, 'learning_rate': 1.9233854714729298e-05, 'epoch': 1.85}
{'loss': 3.0076, 'grad_norm': 1.6247589588165283, 'learning_rate': 1.9204987529123574e-05, 'epoch': 1.85}
{'loss': 3.3275, 'grad_norm': 1.8163667917251587, 'learning_rate': 1.917612850645001e-05, 'epoch': 1.85}
{'loss': 3.5309, 'grad_norm': 1.9208383560180664, 'learning_rate': 1.914727768735979e-05, 'epoch': 1.85}
{'loss': 3.4697, 'grad_norm': 1.938171625137329, 'learning_rate': 1.9118435112492578e-05, 'epoch': 1.86}
{'loss': 3.3867, 'grad_norm': 2.400160312652588, 'learning_rate': 1.908960082247641e-05, 'epoch': 1.86}
{'loss': 3.1045, 'grad_norm': 0.5065489411354065, 'learning_rate': 1.9060774857927652e-05, 'epoch': 1.86}
{'loss': 2.6965, 'grad_norm': 0.6393934488296509, 'learning_rate': 1.9031957259450944e-05, 'epoch': 1.86}
{'loss': 2.6593, 'grad_norm': 1.1836825609207153, 'learning_rate': 1.9003148067639144e-05, 'epoch': 1.86}
{'loss': 2.7896, 'grad_norm': 1.0156656503677368, 'learning_rate': 1.8974347323073265e-05, 'epoch': 1.86}
{'loss': 2.8963, 'grad_norm': 1.262603998184204, 'learning_rate': 1.8945555066322416e-05, 'epoch': 1.86}
{'loss': 2.8405, 'grad_norm': 1.4873247146606445, 'learning_rate': 1.8916771337943753e-05, 'epoch': 1.86}
{'loss': 3.2659, 'grad_norm': 1.8239035606384277, 'learning_rate': 1.8887996178482427e-05, 'epoch': 1.86}
{'loss': 3.5172, 'grad_norm': 1.7095344066619873, 'learning_rate': 1.8859229628471515e-05, 'epoch': 1.86}
{'loss': 3.5839, 'grad_norm': 2.1972763538360596, 'learning_rate': 1.8830471728431966e-05, 'epoch': 1.87}
{'loss': 3.4027, 'grad_norm': 2.1816232204437256, 'learning_rate': 1.8801722518872533e-05, 'epoch': 1.87}
{'loss': 3.143, 'grad_norm': 0.5369543433189392, 'learning_rate': 1.8772982040289744e-05, 'epoch': 1.87}
{'loss': 2.7816, 'grad_norm': 0.6977900862693787, 'learning_rate': 1.8744250333167822e-05, 'epoch': 1.87}
{'loss': 2.7867, 'grad_norm': 2.156712293624878, 'learning_rate': 1.871552743797863e-05, 'epoch': 1.87}
{'loss': 2.8445, 'grad_norm': 0.9606789946556091, 'learning_rate': 1.868681339518163e-05, 'epoch': 1.87}
{'loss': 2.8411, 'grad_norm': 1.1267948150634766, 'learning_rate': 1.86581082452238e-05, 'epoch': 1.87}
{'loss': 2.858, 'grad_norm': 1.463198184967041, 'learning_rate': 1.8629412028539607e-05, 'epoch': 1.87}
{'loss': 3.2295, 'grad_norm': 1.6304348707199097, 'learning_rate': 1.8600724785550917e-05, 'epoch': 1.87}
{'loss': 3.5986, 'grad_norm': 2.000746726989746, 'learning_rate': 1.8572046556666963e-05, 'epoch': 1.87}
{'loss': 3.6212, 'grad_norm': 1.940373182296753, 'learning_rate': 1.8543377382284292e-05, 'epoch': 1.88}
{'loss': 3.4621, 'grad_norm': 2.010831356048584, 'learning_rate': 1.851471730278668e-05, 'epoch': 1.88}
{'loss': 3.1888, 'grad_norm': 0.5420159101486206, 'learning_rate': 1.8486066358545106e-05, 'epoch': 1.88}
{'loss': 2.8237, 'grad_norm': 0.6662630438804626, 'learning_rate': 1.845742458991767e-05, 'epoch': 1.88}
{'loss': 2.7758, 'grad_norm': 0.8504093885421753, 'learning_rate': 1.8428792037249545e-05, 'epoch': 1.88}
{'loss': 2.6015, 'grad_norm': 0.9420191645622253, 'learning_rate': 1.8400168740872932e-05, 'epoch': 1.88}
{'loss': 2.7144, 'grad_norm': 1.1847814321517944, 'learning_rate': 1.8371554741106988e-05, 'epoch': 1.88}
{'loss': 2.8251, 'grad_norm': 1.5281081199645996, 'learning_rate': 1.8342950078257777e-05, 'epoch': 1.88}
{'loss': 3.3979, 'grad_norm': 2.0359749794006348, 'learning_rate': 1.8314354792618215e-05, 'epoch': 1.88}
{'loss': 3.5491, 'grad_norm': 2.204660654067993, 'learning_rate': 1.8285768924467997e-05, 'epoch': 1.89}
{'loss': 3.5421, 'grad_norm': 1.951209545135498, 'learning_rate': 1.8257192514073572e-05, 'epoch': 1.89}
{'loss': 3.5231, 'grad_norm': 2.215683937072754, 'learning_rate': 1.8228625601688037e-05, 'epoch': 1.89}
{'loss': 3.2905, 'grad_norm': 0.5091022849082947, 'learning_rate': 1.8200068227551137e-05, 'epoch': 1.89}
{'loss': 2.7557, 'grad_norm': 0.6353739500045776, 'learning_rate': 1.8171520431889166e-05, 'epoch': 1.89}
{'loss': 2.6956, 'grad_norm': 0.8451250195503235, 'learning_rate': 1.8142982254914938e-05, 'epoch': 1.89}
{'loss': 2.7261, 'grad_norm': 1.0126452445983887, 'learning_rate': 1.811445373682771e-05, 'epoch': 1.89}
{'loss': 2.8364, 'grad_norm': 1.973041296005249, 'learning_rate': 1.8085934917813137e-05, 'epoch': 1.89}
{'loss': 2.9055, 'grad_norm': 1.3507410287857056, 'learning_rate': 1.8057425838043196e-05, 'epoch': 1.89}
{'loss': 3.1694, 'grad_norm': 1.5404938459396362, 'learning_rate': 1.8028926537676173e-05, 'epoch': 1.89}
{'loss': 3.2798, 'grad_norm': 2.0885066986083984, 'learning_rate': 1.800043705685655e-05, 'epoch': 1.9}
{'loss': 3.3, 'grad_norm': 1.9393295049667358, 'learning_rate': 1.7971957435715004e-05, 'epoch': 1.9}
{'loss': 3.5939, 'grad_norm': 2.1679086685180664, 'learning_rate': 1.7943487714368306e-05, 'epoch': 1.9}
{'loss': 3.1172, 'grad_norm': 0.5553529858589172, 'learning_rate': 1.7915027932919287e-05, 'epoch': 1.9}
{'loss': 2.8488, 'grad_norm': 0.672213077545166, 'learning_rate': 1.788657813145679e-05, 'epoch': 1.9}
{'loss': 2.6761, 'grad_norm': 0.7751123905181885, 'learning_rate': 1.7858138350055554e-05, 'epoch': 1.9}
{'loss': 2.7722, 'grad_norm': 0.948853075504303, 'learning_rate': 1.782970862877626e-05, 'epoch': 1.9}
{'loss': 2.773, 'grad_norm': 1.2290174961090088, 'learning_rate': 1.7801289007665382e-05, 'epoch': 1.9}
{'loss': 2.8961, 'grad_norm': 1.4099980592727661, 'learning_rate': 1.7772879526755194e-05, 'epoch': 1.9}
{'loss': 3.3633, 'grad_norm': 1.868381142616272, 'learning_rate': 1.7744480226063652e-05, 'epoch': 1.9}
{'loss': 3.5562, 'grad_norm': 1.910203218460083, 'learning_rate': 1.771609114559442e-05, 'epoch': 1.91}
{'loss': 3.5424, 'grad_norm': 1.9712294340133667, 'learning_rate': 1.76877123253367e-05, 'epoch': 1.91}
{'loss': 3.6416, 'grad_norm': 2.1489925384521484, 'learning_rate': 1.7659343805265302e-05, 'epoch': 1.91}
{'loss': 3.1543, 'grad_norm': 0.5256568789482117, 'learning_rate': 1.7630985625340495e-05, 'epoch': 1.91}
{'loss': 2.6808, 'grad_norm': 0.6608360409736633, 'learning_rate': 1.7602637825507995e-05, 'epoch': 1.91}
{'loss': 2.7414, 'grad_norm': 0.8742773532867432, 'learning_rate': 1.757430044569889e-05, 'epoch': 1.91}
{'loss': 2.7667, 'grad_norm': 0.9499378800392151, 'learning_rate': 1.75459735258296e-05, 'epoch': 1.91}
{'loss': 2.8238, 'grad_norm': 1.1723546981811523, 'learning_rate': 1.751765710580179e-05, 'epoch': 1.91}
{'loss': 2.8516, 'grad_norm': 1.503348708152771, 'learning_rate': 1.7489351225502354e-05, 'epoch': 1.91}
{'loss': 3.0372, 'grad_norm': 1.5585267543792725, 'learning_rate': 1.7461055924803336e-05, 'epoch': 1.91}
{'loss': 3.3999, 'grad_norm': 1.8749204874038696, 'learning_rate': 1.7432771243561875e-05, 'epoch': 1.92}
{'loss': 3.5892, 'grad_norm': 2.172093152999878, 'learning_rate': 1.7404497221620148e-05, 'epoch': 1.92}
{'loss': 3.3706, 'grad_norm': 2.307814598083496, 'learning_rate': 1.7376233898805327e-05, 'epoch': 1.92}
{'loss': 3.2857, 'grad_norm': 0.5235538482666016, 'learning_rate': 1.7347981314929508e-05, 'epoch': 1.92}
{'loss': 2.7203, 'grad_norm': 0.614237368106842, 'learning_rate': 1.7319739509789652e-05, 'epoch': 1.92}
{'loss': 2.6939, 'grad_norm': 0.7302781939506531, 'learning_rate': 1.7291508523167542e-05, 'epoch': 1.92}
{'loss': 2.7386, 'grad_norm': 0.8806714415550232, 'learning_rate': 1.7263288394829736e-05, 'epoch': 1.92}
{'loss': 2.8288, 'grad_norm': 1.0686649084091187, 'learning_rate': 1.723507916452747e-05, 'epoch': 1.92}
{'loss': 2.9863, 'grad_norm': 1.506745457649231, 'learning_rate': 1.720688087199665e-05, 'epoch': 1.92}
{'loss': 3.1203, 'grad_norm': 1.604163646697998, 'learning_rate': 1.7178693556957776e-05, 'epoch': 1.92}
{'loss': 3.4443, 'grad_norm': 2.2522785663604736, 'learning_rate': 1.715051725911586e-05, 'epoch': 1.93}
{'loss': 3.5831, 'grad_norm': 2.2089550495147705, 'learning_rate': 1.712235201816043e-05, 'epoch': 1.93}
{'loss': 3.6277, 'grad_norm': 2.386345148086548, 'learning_rate': 1.709419787376541e-05, 'epoch': 1.93}
{'loss': 3.1426, 'grad_norm': 0.5696247816085815, 'learning_rate': 1.7066054865589115e-05, 'epoch': 1.93}
{'loss': 2.7087, 'grad_norm': 0.6907275915145874, 'learning_rate': 1.703792303327415e-05, 'epoch': 1.93}
{'loss': 2.6614, 'grad_norm': 0.935860276222229, 'learning_rate': 1.7009802416447397e-05, 'epoch': 1.93}
{'loss': 2.7547, 'grad_norm': 1.109930157661438, 'learning_rate': 1.6981693054719942e-05, 'epoch': 1.93}
{'loss': 2.8973, 'grad_norm': 1.3166308403015137, 'learning_rate': 1.6953594987686993e-05, 'epoch': 1.93}
{'loss': 2.9581, 'grad_norm': 1.4271304607391357, 'learning_rate': 1.692550825492787e-05, 'epoch': 1.93}
{'loss': 3.3534, 'grad_norm': 1.8230454921722412, 'learning_rate': 1.6897432896005922e-05, 'epoch': 1.94}
{'loss': 3.4762, 'grad_norm': 2.0064759254455566, 'learning_rate': 1.686936895046847e-05, 'epoch': 1.94}
{'loss': 3.5161, 'grad_norm': 2.218730926513672, 'learning_rate': 1.6841316457846774e-05, 'epoch': 1.94}
{'loss': 3.5047, 'grad_norm': 2.217722177505493, 'learning_rate': 1.6813275457655956e-05, 'epoch': 1.94}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0384111404418945, 'eval_runtime': 12.6746, 'eval_samples_per_second': 128.919, 'eval_steps_per_second': 16.174, 'epoch': 1.94}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.0586, 'grad_norm': 0.5396898984909058, 'learning_rate': 1.678524598939492e-05, 'epoch': 1.94}
{'loss': 2.7523, 'grad_norm': 0.6698244214057922, 'learning_rate': 1.6757228092546362e-05, 'epoch': 1.94}
{'loss': 2.6875, 'grad_norm': 0.7426151633262634, 'learning_rate': 1.672922180657667e-05, 'epoch': 1.94}
{'loss': 2.7499, 'grad_norm': 0.8937812447547913, 'learning_rate': 1.670122717093587e-05, 'epoch': 1.94}
{'loss': 2.8356, 'grad_norm': 1.3046901226043701, 'learning_rate': 1.667324422505758e-05, 'epoch': 1.94}
{'loss': 2.8626, 'grad_norm': 1.3837006092071533, 'learning_rate': 1.664527300835896e-05, 'epoch': 1.94}
{'loss': 3.2023, 'grad_norm': 1.8075753450393677, 'learning_rate': 1.661731356024064e-05, 'epoch': 1.95}
{'loss': 3.5061, 'grad_norm': 1.8716351985931396, 'learning_rate': 1.6589365920086657e-05, 'epoch': 1.95}
{'loss': 3.5474, 'grad_norm': 2.3171536922454834, 'learning_rate': 1.6561430127264443e-05, 'epoch': 1.95}
{'loss': 3.4803, 'grad_norm': 2.206940174102783, 'learning_rate': 1.6533506221124727e-05, 'epoch': 1.95}
{'loss': 3.1257, 'grad_norm': 0.5531811118125916, 'learning_rate': 1.6505594241001497e-05, 'epoch': 1.95}
{'loss': 2.6957, 'grad_norm': 0.6697933673858643, 'learning_rate': 1.6477694226211943e-05, 'epoch': 1.95}
{'loss': 2.6154, 'grad_norm': 0.9361044764518738, 'learning_rate': 1.6449806216056413e-05, 'epoch': 1.95}
{'loss': 2.7491, 'grad_norm': 0.9969202876091003, 'learning_rate': 1.642193024981831e-05, 'epoch': 1.95}
{'loss': 2.9088, 'grad_norm': 1.271620273590088, 'learning_rate': 1.6394066366764094e-05, 'epoch': 1.95}
{'loss': 2.9103, 'grad_norm': 1.585980772972107, 'learning_rate': 1.636621460614321e-05, 'epoch': 1.95}
{'loss': 3.3283, 'grad_norm': 1.9324214458465576, 'learning_rate': 1.6338375007188028e-05, 'epoch': 1.96}
{'loss': 3.7083, 'grad_norm': 1.9723013639450073, 'learning_rate': 1.631054760911377e-05, 'epoch': 1.96}
{'loss': 3.4247, 'grad_norm': 2.049011468887329, 'learning_rate': 1.6282732451118496e-05, 'epoch': 1.96}
{'loss': 3.4301, 'grad_norm': 2.430609703063965, 'learning_rate': 1.6254929572382987e-05, 'epoch': 1.96}
{'loss': 3.0716, 'grad_norm': 0.5142370462417603, 'learning_rate': 1.6227139012070764e-05, 'epoch': 1.96}
{'loss': 2.7127, 'grad_norm': 0.7378650903701782, 'learning_rate': 1.619936080932798e-05, 'epoch': 1.96}
{'loss': 2.7026, 'grad_norm': 0.8855330944061279, 'learning_rate': 1.6171595003283387e-05, 'epoch': 1.96}
{'loss': 2.6321, 'grad_norm': 1.0229671001434326, 'learning_rate': 1.6143841633048262e-05, 'epoch': 1.96}
{'loss': 2.7892, 'grad_norm': 1.142948865890503, 'learning_rate': 1.6116100737716383e-05, 'epoch': 1.96}
{'loss': 2.7758, 'grad_norm': 1.5410090684890747, 'learning_rate': 1.6088372356363956e-05, 'epoch': 1.96}
{'loss': 3.4291, 'grad_norm': 1.81485915184021, 'learning_rate': 1.606065652804952e-05, 'epoch': 1.97}
{'loss': 3.6874, 'grad_norm': 1.9469250440597534, 'learning_rate': 1.603295329181399e-05, 'epoch': 1.97}
{'loss': 3.5406, 'grad_norm': 2.1396918296813965, 'learning_rate': 1.6005262686680496e-05, 'epoch': 1.97}
{'loss': 3.4402, 'grad_norm': 2.2649669647216797, 'learning_rate': 1.5977584751654405e-05, 'epoch': 1.97}
{'loss': 2.9905, 'grad_norm': 0.5560621619224548, 'learning_rate': 1.594991952572323e-05, 'epoch': 1.97}
{'loss': 2.7555, 'grad_norm': 0.6611077189445496, 'learning_rate': 1.592226704785658e-05, 'epoch': 1.97}
{'loss': 2.7866, 'grad_norm': 0.8454409241676331, 'learning_rate': 1.5894627357006097e-05, 'epoch': 1.97}
{'loss': 2.7461, 'grad_norm': 1.1211345195770264, 'learning_rate': 1.5867000492105416e-05, 'epoch': 1.97}
{'loss': 2.8081, 'grad_norm': 1.203474521636963, 'learning_rate': 1.5839386492070117e-05, 'epoch': 1.97}
{'loss': 2.9964, 'grad_norm': 1.427260160446167, 'learning_rate': 1.5811785395797648e-05, 'epoch': 1.97}
{'loss': 3.2967, 'grad_norm': 1.6340484619140625, 'learning_rate': 1.5784197242167284e-05, 'epoch': 1.98}
{'loss': 3.5103, 'grad_norm': 2.2934346199035645, 'learning_rate': 1.5756622070040068e-05, 'epoch': 1.98}
{'loss': 3.4271, 'grad_norm': 2.3200254440307617, 'learning_rate': 1.5729059918258758e-05, 'epoch': 1.98}
{'loss': 3.4751, 'grad_norm': 2.0552191734313965, 'learning_rate': 1.5701510825647767e-05, 'epoch': 1.98}
{'loss': 3.2264, 'grad_norm': 0.548370897769928, 'learning_rate': 1.567397483101311e-05, 'epoch': 1.98}
{'loss': 2.64, 'grad_norm': 0.6538093686103821, 'learning_rate': 1.5646451973142366e-05, 'epoch': 1.98}
{'loss': 2.6548, 'grad_norm': 0.7383027076721191, 'learning_rate': 1.56189422908046e-05, 'epoch': 1.98}
{'loss': 2.7586, 'grad_norm': 1.0303428173065186, 'learning_rate': 1.5591445822750316e-05, 'epoch': 1.98}
{'loss': 2.7538, 'grad_norm': 1.1674830913543701, 'learning_rate': 1.5563962607711413e-05, 'epoch': 1.98}
{'loss': 2.7845, 'grad_norm': 1.704406499862671, 'learning_rate': 1.553649268440111e-05, 'epoch': 1.99}
{'loss': 3.1363, 'grad_norm': 1.8306611776351929, 'learning_rate': 1.5509036091513906e-05, 'epoch': 1.99}
{'loss': 3.3492, 'grad_norm': 2.140718460083008, 'learning_rate': 1.548159286772553e-05, 'epoch': 1.99}
{'loss': 3.6016, 'grad_norm': 2.1824090480804443, 'learning_rate': 1.5454163051692883e-05, 'epoch': 1.99}
{'loss': 3.3912, 'grad_norm': 2.134312152862549, 'learning_rate': 1.5426746682053953e-05, 'epoch': 1.99}
{'loss': 3.0008, 'grad_norm': 0.5591299533843994, 'learning_rate': 1.5399343797427816e-05, 'epoch': 1.99}
{'loss': 2.6345, 'grad_norm': 0.6027522683143616, 'learning_rate': 1.5371954436414545e-05, 'epoch': 1.99}
{'loss': 2.681, 'grad_norm': 0.7501081824302673, 'learning_rate': 1.5344578637595146e-05, 'epoch': 1.99}
{'loss': 2.6509, 'grad_norm': 1.0833423137664795, 'learning_rate': 1.531721643953155e-05, 'epoch': 1.99}
{'loss': 2.7652, 'grad_norm': 1.2954201698303223, 'learning_rate': 1.5289867880766516e-05, 'epoch': 1.99}
{'loss': 2.9036, 'grad_norm': 1.4519140720367432, 'learning_rate': 1.5262532999823582e-05, 'epoch': 2.0}
{'loss': 3.2534, 'grad_norm': 1.6556403636932373, 'learning_rate': 1.5235211835207025e-05, 'epoch': 2.0}
{'loss': 3.6533, 'grad_norm': 2.041682004928589, 'learning_rate': 1.5207904425401825e-05, 'epoch': 2.0}
{'loss': 3.5542, 'grad_norm': 6.366336345672607, 'learning_rate': 1.5180610808873538e-05, 'epoch': 2.0}
{'loss': 3.4655, 'grad_norm': 2.194896936416626, 'learning_rate': 1.5153331024068329e-05, 'epoch': 2.0}
{'loss': 3.2005, 'grad_norm': 0.4171835482120514, 'learning_rate': 1.5126065109412865e-05, 'epoch': 2.0}
{'loss': 2.8405, 'grad_norm': 0.574271559715271, 'learning_rate': 1.5098813103314283e-05, 'epoch': 2.0}
{'loss': 2.7852, 'grad_norm': 0.7514213919639587, 'learning_rate': 1.5071575044160118e-05, 'epoch': 2.0}
{'loss': 2.6953, 'grad_norm': 0.9160273671150208, 'learning_rate': 1.5044350970318282e-05, 'epoch': 2.0}
{'loss': 2.9391, 'grad_norm': 1.187896490097046, 'learning_rate': 1.5017140920136949e-05, 'epoch': 2.0}
{'loss': 2.8756, 'grad_norm': 1.4440330266952515, 'learning_rate': 1.4989944931944564e-05, 'epoch': 2.01}
{'loss': 2.9491, 'grad_norm': 1.7597358226776123, 'learning_rate': 1.4962763044049766e-05, 'epoch': 2.01}
{'loss': 3.4455, 'grad_norm': 1.9389348030090332, 'learning_rate': 1.4935595294741319e-05, 'epoch': 2.01}
{'loss': 3.5216, 'grad_norm': 1.963065505027771, 'learning_rate': 1.4908441722288086e-05, 'epoch': 2.01}
{'loss': 3.525, 'grad_norm': 2.026646137237549, 'learning_rate': 1.4881302364938948e-05, 'epoch': 2.01}
{'loss': 3.4598, 'grad_norm': 0.412135511636734, 'learning_rate': 1.485417726092278e-05, 'epoch': 2.01}
{'loss': 2.717, 'grad_norm': 0.6270104050636292, 'learning_rate': 1.4827066448448346e-05, 'epoch': 2.01}
{'loss': 2.7815, 'grad_norm': 0.7751636505126953, 'learning_rate': 1.479996996570431e-05, 'epoch': 2.01}
{'loss': 2.7461, 'grad_norm': 0.966657817363739, 'learning_rate': 1.4772887850859137e-05, 'epoch': 2.01}
{'loss': 2.7738, 'grad_norm': 1.0979183912277222, 'learning_rate': 1.4745820142061063e-05, 'epoch': 2.01}
{'loss': 2.9393, 'grad_norm': 1.3870892524719238, 'learning_rate': 1.4718766877438017e-05, 'epoch': 2.02}
{'loss': 3.027, 'grad_norm': 1.4844177961349487, 'learning_rate': 1.46917280950976e-05, 'epoch': 2.02}
{'loss': 3.4167, 'grad_norm': 1.9807673692703247, 'learning_rate': 1.466470383312699e-05, 'epoch': 2.02}
{'loss': 3.3173, 'grad_norm': 1.9805688858032227, 'learning_rate': 1.4637694129592933e-05, 'epoch': 2.02}
{'loss': 3.3846, 'grad_norm': 2.1565499305725098, 'learning_rate': 1.4610699022541657e-05, 'epoch': 2.02}
{'loss': 3.3895, 'grad_norm': 0.5222022533416748, 'learning_rate': 1.4583718549998821e-05, 'epoch': 2.02}
{'loss': 2.8587, 'grad_norm': 0.6249536871910095, 'learning_rate': 1.4556752749969505e-05, 'epoch': 2.02}
{'loss': 2.7613, 'grad_norm': 0.7353547811508179, 'learning_rate': 1.4529801660438074e-05, 'epoch': 2.02}
{'loss': 2.8316, 'grad_norm': 0.9290401935577393, 'learning_rate': 1.4502865319368205e-05, 'epoch': 2.02}
{'loss': 2.7287, 'grad_norm': 1.272818684577942, 'learning_rate': 1.4475943764702774e-05, 'epoch': 2.02}
{'loss': 2.8778, 'grad_norm': 1.619620680809021, 'learning_rate': 1.444903703436386e-05, 'epoch': 2.03}
{'loss': 2.9683, 'grad_norm': 1.8010382652282715, 'learning_rate': 1.4422145166252629e-05, 'epoch': 2.03}
{'loss': 3.4292, 'grad_norm': 1.928080439567566, 'learning_rate': 1.4395268198249345e-05, 'epoch': 2.03}
{'loss': 3.4121, 'grad_norm': 1.8981572389602661, 'learning_rate': 1.4368406168213241e-05, 'epoch': 2.03}
{'loss': 3.4846, 'grad_norm': 2.05853009223938, 'learning_rate': 1.4341559113982567e-05, 'epoch': 2.03}
{'loss': 3.4656, 'grad_norm': 0.46581658720970154, 'learning_rate': 1.4314727073374401e-05, 'epoch': 2.03}
{'loss': 2.7295, 'grad_norm': 0.6059584617614746, 'learning_rate': 1.4287910084184746e-05, 'epoch': 2.03}
{'loss': 2.7232, 'grad_norm': 0.8117398619651794, 'learning_rate': 1.426110818418835e-05, 'epoch': 2.03}
{'loss': 2.8262, 'grad_norm': 0.9360604286193848, 'learning_rate': 1.4234321411138752e-05, 'epoch': 2.03}
{'loss': 2.8107, 'grad_norm': 1.1583538055419922, 'learning_rate': 1.420754980276813e-05, 'epoch': 2.03}
{'loss': 2.7861, 'grad_norm': 1.6010327339172363, 'learning_rate': 1.4180793396787362e-05, 'epoch': 2.04}
{'loss': 2.9969, 'grad_norm': 1.814388632774353, 'learning_rate': 1.4154052230885861e-05, 'epoch': 2.04}
{'loss': 3.5112, 'grad_norm': 2.1017935276031494, 'learning_rate': 1.4127326342731595e-05, 'epoch': 2.04}
{'loss': 3.5284, 'grad_norm': 2.0659303665161133, 'learning_rate': 1.4100615769970992e-05, 'epoch': 2.04}
{'loss': 3.3901, 'grad_norm': 2.297116279602051, 'learning_rate': 1.4073920550228952e-05, 'epoch': 2.04}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0346004962921143, 'eval_runtime': 12.6778, 'eval_samples_per_second': 128.887, 'eval_steps_per_second': 16.17, 'epoch': 2.04}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.5882, 'grad_norm': 0.4811747372150421, 'learning_rate': 1.4047240721108695e-05, 'epoch': 2.04}
{'loss': 2.7693, 'grad_norm': 0.663712203502655, 'learning_rate': 1.4020576320191805e-05, 'epoch': 2.04}
{'loss': 2.7171, 'grad_norm': 0.7360135912895203, 'learning_rate': 1.3993927385038102e-05, 'epoch': 2.04}
{'loss': 2.787, 'grad_norm': 0.9576502442359924, 'learning_rate': 1.3967293953185639e-05, 'epoch': 2.04}
{'loss': 2.7618, 'grad_norm': 1.168570876121521, 'learning_rate': 1.3940676062150607e-05, 'epoch': 2.05}
{'loss': 2.8666, 'grad_norm': 1.4077470302581787, 'learning_rate': 1.3914073749427347e-05, 'epoch': 2.05}
{'loss': 2.9985, 'grad_norm': 1.7681576013565063, 'learning_rate': 1.3887487052488212e-05, 'epoch': 2.05}
{'loss': 3.5103, 'grad_norm': 1.856168270111084, 'learning_rate': 1.3860916008783603e-05, 'epoch': 2.05}
{'loss': 3.4913, 'grad_norm': 1.8755011558532715, 'learning_rate': 1.3834360655741835e-05, 'epoch': 2.05}
{'loss': 3.5384, 'grad_norm': 2.025028705596924, 'learning_rate': 1.380782103076913e-05, 'epoch': 2.05}
{'loss': 3.3332, 'grad_norm': 0.44422876834869385, 'learning_rate': 1.3781297171249557e-05, 'epoch': 2.05}
{'loss': 2.7536, 'grad_norm': 0.6211758852005005, 'learning_rate': 1.3754789114544995e-05, 'epoch': 2.05}
{'loss': 2.6818, 'grad_norm': 0.7197152376174927, 'learning_rate': 1.3728296897995028e-05, 'epoch': 2.05}
{'loss': 2.7206, 'grad_norm': 0.9506706595420837, 'learning_rate': 1.3701820558916967e-05, 'epoch': 2.05}
{'loss': 2.7687, 'grad_norm': 1.1427501440048218, 'learning_rate': 1.3675360134605716e-05, 'epoch': 2.06}
{'loss': 2.9305, 'grad_norm': 1.3908809423446655, 'learning_rate': 1.3648915662333817e-05, 'epoch': 2.06}
{'loss': 3.089, 'grad_norm': 1.6061333417892456, 'learning_rate': 1.3622487179351264e-05, 'epoch': 2.06}
{'loss': 3.5914, 'grad_norm': 1.8609591722488403, 'learning_rate': 1.3596074722885604e-05, 'epoch': 2.06}
{'loss': 3.5678, 'grad_norm': 2.1070632934570312, 'learning_rate': 1.3569678330141758e-05, 'epoch': 2.06}
{'loss': 3.4327, 'grad_norm': 2.03393292427063, 'learning_rate': 1.3543298038302055e-05, 'epoch': 2.06}
{'loss': 3.4643, 'grad_norm': 0.644790530204773, 'learning_rate': 1.3516933884526106e-05, 'epoch': 2.06}
{'loss': 2.8266, 'grad_norm': 0.7236415147781372, 'learning_rate': 1.3490585905950836e-05, 'epoch': 2.06}
{'loss': 2.6652, 'grad_norm': 1.4651497602462769, 'learning_rate': 1.3464254139690353e-05, 'epoch': 2.06}
{'loss': 2.7763, 'grad_norm': 0.9009877443313599, 'learning_rate': 1.3437938622835921e-05, 'epoch': 2.06}
{'loss': 2.8005, 'grad_norm': 1.1925463676452637, 'learning_rate': 1.3411639392455935e-05, 'epoch': 2.07}
{'loss': 2.7483, 'grad_norm': 1.3536510467529297, 'learning_rate': 1.3385356485595857e-05, 'epoch': 2.07}
{'loss': 3.0203, 'grad_norm': 1.832400918006897, 'learning_rate': 1.3359089939278119e-05, 'epoch': 2.07}
{'loss': 3.5736, 'grad_norm': 1.847171664237976, 'learning_rate': 1.3332839790502152e-05, 'epoch': 2.07}
{'loss': 3.5295, 'grad_norm': 2.11199951171875, 'learning_rate': 1.330660607624426e-05, 'epoch': 2.07}
{'loss': 3.5299, 'grad_norm': 2.2278010845184326, 'learning_rate': 1.3280388833457589e-05, 'epoch': 2.07}
{'loss': 3.4351, 'grad_norm': 0.392696350812912, 'learning_rate': 1.3254188099072117e-05, 'epoch': 2.07}
{'loss': 2.8885, 'grad_norm': 0.6310684084892273, 'learning_rate': 1.3228003909994549e-05, 'epoch': 2.07}
{'loss': 2.7025, 'grad_norm': 0.745269238948822, 'learning_rate': 1.3201836303108267e-05, 'epoch': 2.07}
{'loss': 2.7552, 'grad_norm': 0.8271728754043579, 'learning_rate': 1.317568531527333e-05, 'epoch': 2.07}
{'loss': 2.8185, 'grad_norm': 1.0185755491256714, 'learning_rate': 1.3149550983326364e-05, 'epoch': 2.08}
{'loss': 2.8803, 'grad_norm': 1.1541862487792969, 'learning_rate': 1.312343334408053e-05, 'epoch': 2.08}
{'loss': 2.9456, 'grad_norm': 1.4845069646835327, 'learning_rate': 1.3097332434325498e-05, 'epoch': 2.08}
{'loss': 3.4611, 'grad_norm': 1.8569954633712769, 'learning_rate': 1.3071248290827349e-05, 'epoch': 2.08}
{'loss': 3.5516, 'grad_norm': 1.789420485496521, 'learning_rate': 1.3045180950328566e-05, 'epoch': 2.08}
{'loss': 3.513, 'grad_norm': 2.21960186958313, 'learning_rate': 1.301913044954794e-05, 'epoch': 2.08}
{'loss': 3.3024, 'grad_norm': 0.5377641916275024, 'learning_rate': 1.2993096825180576e-05, 'epoch': 2.08}
{'loss': 2.8768, 'grad_norm': 0.649047315120697, 'learning_rate': 1.2967080113897773e-05, 'epoch': 2.08}
{'loss': 2.7312, 'grad_norm': 0.7186422348022461, 'learning_rate': 1.2941080352347015e-05, 'epoch': 2.08}
{'loss': 2.6911, 'grad_norm': 1.031388759613037, 'learning_rate': 1.2915097577151914e-05, 'epoch': 2.08}
{'loss': 2.777, 'grad_norm': 1.1378850936889648, 'learning_rate': 1.2889131824912165e-05, 'epoch': 2.09}
{'loss': 2.9844, 'grad_norm': 1.5259331464767456, 'learning_rate': 1.2863183132203463e-05, 'epoch': 2.09}
{'loss': 3.0148, 'grad_norm': 1.8929941654205322, 'learning_rate': 1.2837251535577499e-05, 'epoch': 2.09}
{'loss': 3.4794, 'grad_norm': 1.9316881895065308, 'learning_rate': 1.2811337071561858e-05, 'epoch': 2.09}
{'loss': 3.5819, 'grad_norm': 1.9110418558120728, 'learning_rate': 1.2785439776659996e-05, 'epoch': 2.09}
{'loss': 3.4588, 'grad_norm': 2.004102945327759, 'learning_rate': 1.275955968735119e-05, 'epoch': 2.09}
{'loss': 3.2593, 'grad_norm': 0.7500231862068176, 'learning_rate': 1.2733696840090493e-05, 'epoch': 2.09}
{'loss': 2.8568, 'grad_norm': 0.6142613291740417, 'learning_rate': 1.2707851271308641e-05, 'epoch': 2.09}
{'loss': 2.6079, 'grad_norm': 0.7610851526260376, 'learning_rate': 1.2682023017412064e-05, 'epoch': 2.09}
{'loss': 2.829, 'grad_norm': 1.4353259801864624, 'learning_rate': 1.2656212114782782e-05, 'epoch': 2.1}
{'loss': 2.7953, 'grad_norm': 1.1907049417495728, 'learning_rate': 1.2630418599778371e-05, 'epoch': 2.1}
{'loss': 2.8156, 'grad_norm': 1.2851334810256958, 'learning_rate': 1.2604642508731918e-05, 'epoch': 2.1}
{'loss': 3.361, 'grad_norm': 1.8631144762039185, 'learning_rate': 1.2578883877951985e-05, 'epoch': 2.1}
{'loss': 3.5135, 'grad_norm': 1.8143205642700195, 'learning_rate': 1.2553142743722512e-05, 'epoch': 2.1}
{'loss': 3.4563, 'grad_norm': 1.9836596250534058, 'learning_rate': 1.2527419142302816e-05, 'epoch': 2.1}
{'loss': 3.4977, 'grad_norm': 2.1595046520233154, 'learning_rate': 1.250171310992749e-05, 'epoch': 2.1}
{'loss': 3.4236, 'grad_norm': 0.3991229832172394, 'learning_rate': 1.2476024682806425e-05, 'epoch': 2.1}
{'loss': 2.7981, 'grad_norm': 0.6011158227920532, 'learning_rate': 1.2450353897124648e-05, 'epoch': 2.1}
{'loss': 2.748, 'grad_norm': 0.7067134976387024, 'learning_rate': 1.2424700789042393e-05, 'epoch': 2.1}
{'loss': 2.709, 'grad_norm': 0.8226972222328186, 'learning_rate': 1.2399065394694958e-05, 'epoch': 2.11}
{'loss': 2.6481, 'grad_norm': 0.9394583702087402, 'learning_rate': 1.2373447750192722e-05, 'epoch': 2.11}
{'loss': 2.819, 'grad_norm': 1.4244575500488281, 'learning_rate': 1.2347847891621017e-05, 'epoch': 2.11}
{'loss': 2.9769, 'grad_norm': 1.7478089332580566, 'learning_rate': 1.2322265855040183e-05, 'epoch': 2.11}
{'loss': 3.6626, 'grad_norm': 1.9314650297164917, 'learning_rate': 1.2296701676485375e-05, 'epoch': 2.11}
{'loss': 3.4195, 'grad_norm': 2.043550968170166, 'learning_rate': 1.2271155391966669e-05, 'epoch': 2.11}
{'loss': 3.4892, 'grad_norm': 1.908724069595337, 'learning_rate': 1.2245627037468883e-05, 'epoch': 2.11}
{'loss': 3.3924, 'grad_norm': 0.39898213744163513, 'learning_rate': 1.2220116648951615e-05, 'epoch': 2.11}
{'loss': 2.7702, 'grad_norm': 0.662002682685852, 'learning_rate': 1.2194624262349127e-05, 'epoch': 2.11}
{'loss': 2.6779, 'grad_norm': 4.669288158416748, 'learning_rate': 1.2169149913570354e-05, 'epoch': 2.11}
{'loss': 2.7623, 'grad_norm': 0.9534234404563904, 'learning_rate': 1.214369363849879e-05, 'epoch': 2.12}
{'loss': 2.8554, 'grad_norm': 1.206983208656311, 'learning_rate': 1.211825547299249e-05, 'epoch': 2.12}
{'loss': 2.8794, 'grad_norm': 1.4982662200927734, 'learning_rate': 1.2092835452883985e-05, 'epoch': 2.12}
{'loss': 3.0197, 'grad_norm': 1.6599147319793701, 'learning_rate': 1.206743361398027e-05, 'epoch': 2.12}
{'loss': 3.3226, 'grad_norm': 1.8319216966629028, 'learning_rate': 1.2042049992062701e-05, 'epoch': 2.12}
{'loss': 3.6123, 'grad_norm': 2.051316022872925, 'learning_rate': 1.2016684622887004e-05, 'epoch': 2.12}
{'loss': 3.4632, 'grad_norm': 1.9888088703155518, 'learning_rate': 1.199133754218317e-05, 'epoch': 2.12}
{'loss': 3.312, 'grad_norm': 0.38373878598213196, 'learning_rate': 1.1966008785655424e-05, 'epoch': 2.12}
{'loss': 2.7736, 'grad_norm': 0.5888490676879883, 'learning_rate': 1.1940698388982216e-05, 'epoch': 2.12}
{'loss': 2.6944, 'grad_norm': 0.7162883281707764, 'learning_rate': 1.1915406387816094e-05, 'epoch': 2.12}
{'loss': 2.6896, 'grad_norm': 0.8360890746116638, 'learning_rate': 1.1890132817783703e-05, 'epoch': 2.13}
{'loss': 2.8194, 'grad_norm': 1.1897690296173096, 'learning_rate': 1.1864877714485748e-05, 'epoch': 2.13}
{'loss': 2.8531, 'grad_norm': 1.2433489561080933, 'learning_rate': 1.1839641113496902e-05, 'epoch': 2.13}
{'loss': 3.1913, 'grad_norm': 1.9543395042419434, 'learning_rate': 1.1814423050365764e-05, 'epoch': 2.13}
{'loss': 3.555, 'grad_norm': 1.9291259050369263, 'learning_rate': 1.1789223560614857e-05, 'epoch': 2.13}
{'loss': 3.3154, 'grad_norm': 1.8975144624710083, 'learning_rate': 1.17640426797405e-05, 'epoch': 2.13}
{'loss': 3.4906, 'grad_norm': 2.0331990718841553, 'learning_rate': 1.1738880443212841e-05, 'epoch': 2.13}
{'loss': 3.4477, 'grad_norm': 0.4359431564807892, 'learning_rate': 1.1713736886475727e-05, 'epoch': 2.13}
{'loss': 2.8152, 'grad_norm': 0.58685302734375, 'learning_rate': 1.1688612044946704e-05, 'epoch': 2.13}
{'loss': 2.6387, 'grad_norm': 0.7305731177330017, 'learning_rate': 1.166350595401698e-05, 'epoch': 2.13}
{'loss': 2.7272, 'grad_norm': 0.9761875867843628, 'learning_rate': 1.1638418649051316e-05, 'epoch': 2.14}
{'loss': 2.7727, 'grad_norm': 1.1263713836669922, 'learning_rate': 1.1613350165388024e-05, 'epoch': 2.14}
{'loss': 2.9665, 'grad_norm': 1.44114089012146, 'learning_rate': 1.1588300538338917e-05, 'epoch': 2.14}
{'loss': 3.0473, 'grad_norm': 1.795029640197754, 'learning_rate': 1.156326980318922e-05, 'epoch': 2.14}
{'loss': 3.4215, 'grad_norm': 1.9192659854888916, 'learning_rate': 1.153825799519758e-05, 'epoch': 2.14}
{'loss': 3.5464, 'grad_norm': 2.0941762924194336, 'learning_rate': 1.1513265149595954e-05, 'epoch': 2.14}
{'loss': 3.5416, 'grad_norm': 2.187062978744507, 'learning_rate': 1.1488291301589601e-05, 'epoch': 2.14}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0314347743988037, 'eval_runtime': 12.4936, 'eval_samples_per_second': 130.787, 'eval_steps_per_second': 16.408, 'epoch': 2.14}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.4161, 'grad_norm': 0.3974453806877136, 'learning_rate': 1.1463336486357007e-05, 'epoch': 2.14}
{'loss': 2.804, 'grad_norm': 0.6399965286254883, 'learning_rate': 1.1438400739049879e-05, 'epoch': 2.14}
{'loss': 2.6646, 'grad_norm': 0.7768882513046265, 'learning_rate': 1.1413484094793022e-05, 'epoch': 2.15}
{'loss': 2.6713, 'grad_norm': 0.9618049263954163, 'learning_rate': 1.138858658868438e-05, 'epoch': 2.15}
{'loss': 2.8092, 'grad_norm': 1.1041737794876099, 'learning_rate': 1.1363708255794888e-05, 'epoch': 2.15}
{'loss': 2.9064, 'grad_norm': 1.3747719526290894, 'learning_rate': 1.133884913116853e-05, 'epoch': 2.15}
{'loss': 3.059, 'grad_norm': 1.6712267398834229, 'learning_rate': 1.1314009249822163e-05, 'epoch': 2.15}
{'loss': 3.4007, 'grad_norm': 2.07594895362854, 'learning_rate': 1.1289188646745607e-05, 'epoch': 2.15}
{'loss': 3.6483, 'grad_norm': 2.193789482116699, 'learning_rate': 1.1264387356901474e-05, 'epoch': 2.15}
{'loss': 3.3435, 'grad_norm': 3.812483549118042, 'learning_rate': 1.1239605415225216e-05, 'epoch': 2.15}
{'loss': 3.2553, 'grad_norm': 0.4532906115055084, 'learning_rate': 1.1214842856624982e-05, 'epoch': 2.15}
{'loss': 2.77, 'grad_norm': 0.6152127385139465, 'learning_rate': 1.1190099715981677e-05, 'epoch': 2.15}
{'loss': 2.7765, 'grad_norm': 0.7990989089012146, 'learning_rate': 1.1165376028148786e-05, 'epoch': 2.16}
{'loss': 2.6791, 'grad_norm': 0.9882579445838928, 'learning_rate': 1.1140671827952445e-05, 'epoch': 2.16}
{'loss': 2.8791, 'grad_norm': 1.2116702795028687, 'learning_rate': 1.1115987150191313e-05, 'epoch': 2.16}
{'loss': 2.8939, 'grad_norm': 1.554414987564087, 'learning_rate': 1.1091322029636567e-05, 'epoch': 2.16}
{'loss': 3.0107, 'grad_norm': 1.759602665901184, 'learning_rate': 1.106667650103181e-05, 'epoch': 2.16}
{'loss': 3.6009, 'grad_norm': 1.949249505996704, 'learning_rate': 1.1042050599093087e-05, 'epoch': 2.16}
{'loss': 3.4203, 'grad_norm': 2.0587873458862305, 'learning_rate': 1.1017444358508761e-05, 'epoch': 2.16}
{'loss': 3.4642, 'grad_norm': 1.9642341136932373, 'learning_rate': 1.0992857813939509e-05, 'epoch': 2.16}
{'loss': 3.2784, 'grad_norm': 0.39284956455230713, 'learning_rate': 1.096829100001826e-05, 'epoch': 2.16}
{'loss': 2.8266, 'grad_norm': 0.6797278523445129, 'learning_rate': 1.0943743951350177e-05, 'epoch': 2.16}
{'loss': 2.7592, 'grad_norm': 0.8590055704116821, 'learning_rate': 1.091921670251254e-05, 'epoch': 2.17}
{'loss': 2.7931, 'grad_norm': 0.9240582585334778, 'learning_rate': 1.0894709288054781e-05, 'epoch': 2.17}
{'loss': 2.8361, 'grad_norm': 1.191422462463379, 'learning_rate': 1.0870221742498362e-05, 'epoch': 2.17}
{'loss': 2.6974, 'grad_norm': 1.4298348426818848, 'learning_rate': 1.0845754100336766e-05, 'epoch': 2.17}
{'loss': 3.1674, 'grad_norm': 1.7539687156677246, 'learning_rate': 1.0821306396035436e-05, 'epoch': 2.17}
{'loss': 3.4919, 'grad_norm': 1.8680593967437744, 'learning_rate': 1.0796878664031748e-05, 'epoch': 2.17}
{'loss': 3.5386, 'grad_norm': 2.0412135124206543, 'learning_rate': 1.0772470938734918e-05, 'epoch': 2.17}
{'loss': 3.516, 'grad_norm': 2.0938711166381836, 'learning_rate': 1.0748083254526014e-05, 'epoch': 2.17}
{'loss': 3.2831, 'grad_norm': 0.4302312433719635, 'learning_rate': 1.0723715645757846e-05, 'epoch': 2.17}
{'loss': 2.6917, 'grad_norm': 0.5693318247795105, 'learning_rate': 1.0699368146754948e-05, 'epoch': 2.17}
{'loss': 2.6989, 'grad_norm': 0.7145169377326965, 'learning_rate': 1.0675040791813529e-05, 'epoch': 2.18}
{'loss': 2.7275, 'grad_norm': 1.3966540098190308, 'learning_rate': 1.0650733615201442e-05, 'epoch': 2.18}
{'loss': 2.8583, 'grad_norm': 1.0691585540771484, 'learning_rate': 1.0626446651158087e-05, 'epoch': 2.18}
{'loss': 2.9851, 'grad_norm': 1.2891747951507568, 'learning_rate': 1.0602179933894422e-05, 'epoch': 2.18}
{'loss': 2.913, 'grad_norm': 1.6186844110488892, 'learning_rate': 1.0577933497592852e-05, 'epoch': 2.18}
{'loss': 3.41, 'grad_norm': 1.7963701486587524, 'learning_rate': 1.0553707376407248e-05, 'epoch': 2.18}
{'loss': 3.6001, 'grad_norm': 2.123414993286133, 'learning_rate': 1.0529501604462846e-05, 'epoch': 2.18}
{'loss': 3.4892, 'grad_norm': 2.15714168548584, 'learning_rate': 1.0505316215856214e-05, 'epoch': 2.18}
{'loss': 3.2523, 'grad_norm': 0.3834531903266907, 'learning_rate': 1.048115124465521e-05, 'epoch': 2.18}
{'loss': 2.8109, 'grad_norm': 0.6217206120491028, 'learning_rate': 1.045700672489895e-05, 'epoch': 2.18}
{'loss': 2.7304, 'grad_norm': 0.745976984500885, 'learning_rate': 1.0432882690597712e-05, 'epoch': 2.19}
{'loss': 2.8256, 'grad_norm': 0.9258366227149963, 'learning_rate': 1.040877917573295e-05, 'epoch': 2.19}
{'loss': 2.8882, 'grad_norm': 1.174712896347046, 'learning_rate': 1.0384696214257189e-05, 'epoch': 2.19}
{'loss': 2.9503, 'grad_norm': 1.5036948919296265, 'learning_rate': 1.0360633840094003e-05, 'epoch': 2.19}
{'loss': 3.2868, 'grad_norm': 1.7911419868469238, 'learning_rate': 1.0336592087137994e-05, 'epoch': 2.19}
{'loss': 3.3753, 'grad_norm': 2.2035505771636963, 'learning_rate': 1.0312570989254675e-05, 'epoch': 2.19}
{'loss': 3.4106, 'grad_norm': 1.8760243654251099, 'learning_rate': 1.0288570580280506e-05, 'epoch': 2.19}
{'loss': 3.5041, 'grad_norm': 2.1343576908111572, 'learning_rate': 1.026459089402278e-05, 'epoch': 2.19}
{'loss': 3.3614, 'grad_norm': 0.42084649205207825, 'learning_rate': 1.0240631964259595e-05, 'epoch': 2.19}
{'loss': 2.8183, 'grad_norm': 0.7818780541419983, 'learning_rate': 1.0216693824739817e-05, 'epoch': 2.2}
{'loss': 2.7121, 'grad_norm': 0.7701388597488403, 'learning_rate': 1.0192776509183047e-05, 'epoch': 2.2}
{'loss': 2.6144, 'grad_norm': 0.9187848567962646, 'learning_rate': 1.0168880051279519e-05, 'epoch': 2.2}
{'loss': 2.8014, 'grad_norm': 1.2166659832000732, 'learning_rate': 1.0145004484690118e-05, 'epoch': 2.2}
{'loss': 2.647, 'grad_norm': 1.491460919380188, 'learning_rate': 1.0121149843046274e-05, 'epoch': 2.2}
{'loss': 3.1573, 'grad_norm': 1.9405449628829956, 'learning_rate': 1.0097316159949976e-05, 'epoch': 2.2}
{'loss': 3.4216, 'grad_norm': 1.9906009435653687, 'learning_rate': 1.0073503468973636e-05, 'epoch': 2.2}
{'loss': 3.4308, 'grad_norm': 2.187612295150757, 'learning_rate': 1.0049711803660156e-05, 'epoch': 2.2}
{'loss': 3.4556, 'grad_norm': 2.341036319732666, 'learning_rate': 1.0025941197522778e-05, 'epoch': 2.2}
{'loss': 3.3417, 'grad_norm': 0.3851166367530823, 'learning_rate': 1.0002191684045108e-05, 'epoch': 2.2}
{'loss': 2.877, 'grad_norm': 0.5689519643783569, 'learning_rate': 9.97846329668102e-06, 'epoch': 2.21}
{'loss': 2.6827, 'grad_norm': 0.7236530780792236, 'learning_rate': 9.954756068854649e-06, 'epoch': 2.21}
{'loss': 2.7417, 'grad_norm': 1.8394404649734497, 'learning_rate': 9.931070033960307e-06, 'epoch': 2.21}
{'loss': 2.7699, 'grad_norm': 1.9470771551132202, 'learning_rate': 9.90740522536246e-06, 'epoch': 2.21}
{'loss': 2.9129, 'grad_norm': 1.3026670217514038, 'learning_rate': 9.883761676395661e-06, 'epoch': 2.21}
{'loss': 3.0333, 'grad_norm': 1.653196930885315, 'learning_rate': 9.860139420364556e-06, 'epoch': 2.21}
{'loss': 3.2193, 'grad_norm': 1.9275785684585571, 'learning_rate': 9.836538490543745e-06, 'epoch': 2.21}
{'loss': 3.4866, 'grad_norm': 1.9699063301086426, 'learning_rate': 9.812958920177836e-06, 'epoch': 2.21}
{'loss': 3.4253, 'grad_norm': 2.3605432510375977, 'learning_rate': 9.789400742481319e-06, 'epoch': 2.21}
{'loss': 3.3804, 'grad_norm': 0.4241389334201813, 'learning_rate': 9.765863990638555e-06, 'epoch': 2.21}
{'loss': 2.7016, 'grad_norm': 0.6195861101150513, 'learning_rate': 9.742348697803722e-06, 'epoch': 2.22}
{'loss': 2.7038, 'grad_norm': 0.7172082662582397, 'learning_rate': 9.718854897100794e-06, 'epoch': 2.22}
{'loss': 2.6523, 'grad_norm': 0.9207741022109985, 'learning_rate': 9.695382621623434e-06, 'epoch': 2.22}
{'loss': 2.6927, 'grad_norm': 1.157362699508667, 'learning_rate': 9.67193190443502e-06, 'epoch': 2.22}
{'loss': 2.6823, 'grad_norm': 1.4083504676818848, 'learning_rate': 9.648502778568541e-06, 'epoch': 2.22}
{'loss': 3.0264, 'grad_norm': 1.7078615427017212, 'learning_rate': 9.625095277026574e-06, 'epoch': 2.22}
{'loss': 3.494, 'grad_norm': 2.0346603393554688, 'learning_rate': 9.601709432781231e-06, 'epoch': 2.22}
{'loss': 3.5695, 'grad_norm': 1.9364572763442993, 'learning_rate': 9.578345278774144e-06, 'epoch': 2.22}
{'loss': 3.4447, 'grad_norm': 2.2844507694244385, 'learning_rate': 9.555002847916358e-06, 'epoch': 2.22}
{'loss': 3.2343, 'grad_norm': 0.4342106878757477, 'learning_rate': 9.531682173088347e-06, 'epoch': 2.22}
{'loss': 2.8311, 'grad_norm': 0.6430709362030029, 'learning_rate': 9.508383287139907e-06, 'epoch': 2.23}
{'loss': 2.7385, 'grad_norm': 0.7586358189582825, 'learning_rate': 9.485106222890191e-06, 'epoch': 2.23}
{'loss': 2.6184, 'grad_norm': 0.8463988900184631, 'learning_rate': 9.461851013127548e-06, 'epoch': 2.23}
{'loss': 2.7186, 'grad_norm': 1.0242891311645508, 'learning_rate': 9.438617690609602e-06, 'epoch': 2.23}
{'loss': 2.9936, 'grad_norm': 1.2765854597091675, 'learning_rate': 9.415406288063106e-06, 'epoch': 2.23}
{'loss': 3.0463, 'grad_norm': 1.732599139213562, 'learning_rate': 9.39221683818397e-06, 'epoch': 2.23}
{'loss': 3.4492, 'grad_norm': 2.2194480895996094, 'learning_rate': 9.369049373637143e-06, 'epoch': 2.23}
{'loss': 3.474, 'grad_norm': 2.021601676940918, 'learning_rate': 9.345903927056649e-06, 'epoch': 2.23}
{'loss': 3.6308, 'grad_norm': 2.7290849685668945, 'learning_rate': 9.322780531045458e-06, 'epoch': 2.23}
{'loss': 3.341, 'grad_norm': 0.43694910407066345, 'learning_rate': 9.299679218175503e-06, 'epoch': 2.23}
{'loss': 2.7673, 'grad_norm': 0.6801949739456177, 'learning_rate': 9.276600020987588e-06, 'epoch': 2.24}
{'loss': 2.7056, 'grad_norm': 0.7539981007575989, 'learning_rate': 9.253542971991397e-06, 'epoch': 2.24}
{'loss': 2.6047, 'grad_norm': 0.8428573608398438, 'learning_rate': 9.230508103665384e-06, 'epoch': 2.24}
{'loss': 2.8263, 'grad_norm': 1.1695692539215088, 'learning_rate': 9.207495448456787e-06, 'epoch': 2.24}
{'loss': 2.7233, 'grad_norm': 1.301809310913086, 'learning_rate': 9.184505038781536e-06, 'epoch': 2.24}
{'loss': 2.9171, 'grad_norm': 1.5744858980178833, 'learning_rate': 9.161536907024215e-06, 'epoch': 2.24}
{'loss': 3.5346, 'grad_norm': 1.9222383499145508, 'learning_rate': 9.138591085538064e-06, 'epoch': 2.24}
{'loss': 3.6046, 'grad_norm': 2.0142107009887695, 'learning_rate': 9.115667606644863e-06, 'epoch': 2.24}
{'loss': 3.4567, 'grad_norm': 2.116741418838501, 'learning_rate': 9.092766502634922e-06, 'epoch': 2.24}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0284230709075928, 'eval_runtime': 12.5383, 'eval_samples_per_second': 130.32, 'eval_steps_per_second': 16.35, 'epoch': 2.24}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.3409, 'grad_norm': 0.45314735174179077, 'learning_rate': 9.069887805767066e-06, 'epoch': 2.25}
{'loss': 2.8356, 'grad_norm': 0.6081417798995972, 'learning_rate': 9.047031548268518e-06, 'epoch': 2.25}
{'loss': 2.7036, 'grad_norm': 0.7587860226631165, 'learning_rate': 9.024197762334902e-06, 'epoch': 2.25}
{'loss': 2.7198, 'grad_norm': 0.9172958135604858, 'learning_rate': 9.001386480130211e-06, 'epoch': 2.25}
{'loss': 2.6924, 'grad_norm': 1.280604600906372, 'learning_rate': 8.978597733786708e-06, 'epoch': 2.25}
{'loss': 2.8057, 'grad_norm': 1.339927315711975, 'learning_rate': 8.955831555404943e-06, 'epoch': 2.25}
{'loss': 3.094, 'grad_norm': 1.4900598526000977, 'learning_rate': 8.933087977053647e-06, 'epoch': 2.25}
{'loss': 3.3895, 'grad_norm': 1.9972316026687622, 'learning_rate': 8.910367030769725e-06, 'epoch': 2.25}
{'loss': 3.6056, 'grad_norm': 2.10229754447937, 'learning_rate': 8.887668748558222e-06, 'epoch': 2.25}
{'loss': 3.6084, 'grad_norm': 2.1972174644470215, 'learning_rate': 8.864993162392235e-06, 'epoch': 2.25}
{'loss': 3.3507, 'grad_norm': 0.3852752149105072, 'learning_rate': 8.842340304212884e-06, 'epoch': 2.26}
{'loss': 2.7585, 'grad_norm': 0.5758021473884583, 'learning_rate': 8.819710205929312e-06, 'epoch': 2.26}
{'loss': 2.7259, 'grad_norm': 0.7390194535255432, 'learning_rate': 8.797102899418558e-06, 'epoch': 2.26}
{'loss': 2.6879, 'grad_norm': 0.8808749318122864, 'learning_rate': 8.774518416525594e-06, 'epoch': 2.26}
{'loss': 2.825, 'grad_norm': 1.15052330493927, 'learning_rate': 8.751956789063218e-06, 'epoch': 2.26}
{'loss': 2.9318, 'grad_norm': 1.4279546737670898, 'learning_rate': 8.729418048812034e-06, 'epoch': 2.26}
{'loss': 3.1894, 'grad_norm': 1.8149570226669312, 'learning_rate': 8.706902227520417e-06, 'epoch': 2.26}
{'loss': 3.4798, 'grad_norm': 2.0771560668945312, 'learning_rate': 8.68440935690446e-06, 'epoch': 2.26}
{'loss': 3.6387, 'grad_norm': 2.173884153366089, 'learning_rate': 8.661939468647911e-06, 'epoch': 2.26}
{'loss': 3.563, 'grad_norm': 2.090949296951294, 'learning_rate': 8.639492594402171e-06, 'epoch': 2.26}
{'loss': 3.5476, 'grad_norm': 0.3899441361427307, 'learning_rate': 8.6170687657862e-06, 'epoch': 2.27}
{'loss': 2.8497, 'grad_norm': 0.673534095287323, 'learning_rate': 8.594668014386497e-06, 'epoch': 2.27}
{'loss': 2.7909, 'grad_norm': 0.7696713209152222, 'learning_rate': 8.572290371757059e-06, 'epoch': 2.27}
{'loss': 2.7046, 'grad_norm': 0.9366106986999512, 'learning_rate': 8.549935869419349e-06, 'epoch': 2.27}
{'loss': 2.7387, 'grad_norm': 1.1376053094863892, 'learning_rate': 8.527604538862197e-06, 'epoch': 2.27}
{'loss': 2.8974, 'grad_norm': 1.5057646036148071, 'learning_rate': 8.505296411541835e-06, 'epoch': 2.27}
{'loss': 3.1195, 'grad_norm': 1.5007472038269043, 'learning_rate': 8.483011518881772e-06, 'epoch': 2.27}
{'loss': 3.4056, 'grad_norm': 2.0306830406188965, 'learning_rate': 8.460749892272832e-06, 'epoch': 2.27}
{'loss': 3.48, 'grad_norm': 2.2224931716918945, 'learning_rate': 8.438511563073006e-06, 'epoch': 2.27}
{'loss': 3.5211, 'grad_norm': 2.180274248123169, 'learning_rate': 8.416296562607529e-06, 'epoch': 2.27}
{'loss': 3.2878, 'grad_norm': 0.3643697500228882, 'learning_rate': 8.394104922168728e-06, 'epoch': 2.28}
{'loss': 2.8376, 'grad_norm': 0.6745567917823792, 'learning_rate': 8.371936673016061e-06, 'epoch': 2.28}
{'loss': 2.6284, 'grad_norm': 0.6964711546897888, 'learning_rate': 8.34979184637601e-06, 'epoch': 2.28}
{'loss': 2.6607, 'grad_norm': 0.962756335735321, 'learning_rate': 8.327670473442093e-06, 'epoch': 2.28}
{'loss': 2.843, 'grad_norm': 1.1671940088272095, 'learning_rate': 8.305572585374743e-06, 'epoch': 2.28}
{'loss': 2.855, 'grad_norm': 1.4716092348098755, 'learning_rate': 8.283498213301364e-06, 'epoch': 2.28}
{'loss': 3.067, 'grad_norm': 1.597700834274292, 'learning_rate': 8.261447388316194e-06, 'epoch': 2.28}
{'loss': 3.413, 'grad_norm': 1.9352104663848877, 'learning_rate': 8.23942014148034e-06, 'epoch': 2.28}
{'loss': 3.5696, 'grad_norm': 2.045387029647827, 'learning_rate': 8.217416503821657e-06, 'epoch': 2.28}
{'loss': 3.6994, 'grad_norm': 2.402583599090576, 'learning_rate': 8.195436506334783e-06, 'epoch': 2.28}
{'loss': 3.3915, 'grad_norm': 0.44755202531814575, 'learning_rate': 8.173480179981028e-06, 'epoch': 2.29}
{'loss': 2.7766, 'grad_norm': 0.6333345174789429, 'learning_rate': 8.151547555688364e-06, 'epoch': 2.29}
{'loss': 2.7083, 'grad_norm': 0.773002564907074, 'learning_rate': 8.129638664351374e-06, 'epoch': 2.29}
{'loss': 2.7542, 'grad_norm': 0.9857624173164368, 'learning_rate': 8.107753536831234e-06, 'epoch': 2.29}
{'loss': 2.8164, 'grad_norm': 1.2558649778366089, 'learning_rate': 8.085892203955606e-06, 'epoch': 2.29}
{'loss': 2.8719, 'grad_norm': 1.5071794986724854, 'learning_rate': 8.064054696518678e-06, 'epoch': 2.29}
{'loss': 3.2513, 'grad_norm': 1.891955018043518, 'learning_rate': 8.042241045281043e-06, 'epoch': 2.29}
{'loss': 3.4538, 'grad_norm': 2.311180591583252, 'learning_rate': 8.020451280969704e-06, 'epoch': 2.29}
{'loss': 3.3688, 'grad_norm': 1.983213186264038, 'learning_rate': 7.998685434278006e-06, 'epoch': 2.29}
{'loss': 3.3636, 'grad_norm': 2.107914447784424, 'learning_rate': 7.97694353586563e-06, 'epoch': 2.3}
{'loss': 3.2741, 'grad_norm': 0.40941449999809265, 'learning_rate': 7.955225616358486e-06, 'epoch': 2.3}
{'loss': 2.7846, 'grad_norm': 0.6331888437271118, 'learning_rate': 7.933531706348746e-06, 'epoch': 2.3}
{'loss': 2.7122, 'grad_norm': 0.802433431148529, 'learning_rate': 7.911861836394721e-06, 'epoch': 2.3}
{'loss': 2.651, 'grad_norm': 0.8821848034858704, 'learning_rate': 7.890216037020901e-06, 'epoch': 2.3}
{'loss': 2.65, 'grad_norm': 1.0330971479415894, 'learning_rate': 7.868594338717838e-06, 'epoch': 2.3}
{'loss': 2.7636, 'grad_norm': 1.404664397239685, 'learning_rate': 7.846996771942138e-06, 'epoch': 2.3}
{'loss': 3.1463, 'grad_norm': 1.9305448532104492, 'learning_rate': 7.825423367116442e-06, 'epoch': 2.3}
{'loss': 3.6777, 'grad_norm': 2.283646821975708, 'learning_rate': 7.803874154629321e-06, 'epoch': 2.3}
{'loss': 3.6566, 'grad_norm': 2.2000041007995605, 'learning_rate': 7.782349164835284e-06, 'epoch': 2.3}
{'loss': 3.5576, 'grad_norm': 2.301044464111328, 'learning_rate': 7.760848428054732e-06, 'epoch': 2.31}
{'loss': 3.2678, 'grad_norm': 0.4872317910194397, 'learning_rate': 7.739371974573886e-06, 'epoch': 2.31}
{'loss': 2.7334, 'grad_norm': 0.5865592360496521, 'learning_rate': 7.717919834644751e-06, 'epoch': 2.31}
{'loss': 2.6934, 'grad_norm': 1.353929042816162, 'learning_rate': 7.696492038485123e-06, 'epoch': 2.31}
{'loss': 2.6633, 'grad_norm': 0.866116464138031, 'learning_rate': 7.675088616278461e-06, 'epoch': 2.31}
{'loss': 2.7119, 'grad_norm': 1.1279983520507812, 'learning_rate': 7.653709598173931e-06, 'epoch': 2.31}
{'loss': 2.8118, 'grad_norm': 1.4057385921478271, 'learning_rate': 7.632355014286293e-06, 'epoch': 2.31}
{'loss': 3.1364, 'grad_norm': 1.826922059059143, 'learning_rate': 7.611024894695903e-06, 'epoch': 2.31}
{'loss': 3.4203, 'grad_norm': 2.1474859714508057, 'learning_rate': 7.589719269448644e-06, 'epoch': 2.31}
{'loss': 3.5994, 'grad_norm': 2.062356472015381, 'learning_rate': 7.568438168555919e-06, 'epoch': 2.31}
{'loss': 3.5712, 'grad_norm': 2.4564545154571533, 'learning_rate': 7.547181621994559e-06, 'epoch': 2.32}
{'loss': 3.2544, 'grad_norm': 0.43344035744667053, 'learning_rate': 7.525949659706832e-06, 'epoch': 2.32}
{'loss': 2.8434, 'grad_norm': 0.5723425149917603, 'learning_rate': 7.504742311600352e-06, 'epoch': 2.32}
{'loss': 2.6274, 'grad_norm': 0.7172561287879944, 'learning_rate': 7.483559607548096e-06, 'epoch': 2.32}
{'loss': 2.6856, 'grad_norm': 0.9028984904289246, 'learning_rate': 7.462401577388273e-06, 'epoch': 2.32}
{'loss': 2.8232, 'grad_norm': 1.251076102256775, 'learning_rate': 7.441268250924391e-06, 'epoch': 2.32}
{'loss': 2.9157, 'grad_norm': 1.3479984998703003, 'learning_rate': 7.420159657925119e-06, 'epoch': 2.32}
{'loss': 3.0604, 'grad_norm': 1.63308846950531, 'learning_rate': 7.399075828124324e-06, 'epoch': 2.32}
{'loss': 3.5258, 'grad_norm': 1.9458413124084473, 'learning_rate': 7.378016791220951e-06, 'epoch': 2.32}
{'loss': 3.4964, 'grad_norm': 1.9144953489303589, 'learning_rate': 7.356982576879065e-06, 'epoch': 2.32}
{'loss': 3.4818, 'grad_norm': 2.039945125579834, 'learning_rate': 7.335973214727715e-06, 'epoch': 2.33}
{'loss': 3.4186, 'grad_norm': 0.44189170002937317, 'learning_rate': 7.314988734360989e-06, 'epoch': 2.33}
{'loss': 2.8068, 'grad_norm': 0.6077117323875427, 'learning_rate': 7.294029165337893e-06, 'epoch': 2.33}
{'loss': 2.6939, 'grad_norm': 0.7270219922065735, 'learning_rate': 7.27309453718237e-06, 'epoch': 2.33}
{'loss': 2.7601, 'grad_norm': 0.9148128628730774, 'learning_rate': 7.252184879383206e-06, 'epoch': 2.33}
{'loss': 2.7604, 'grad_norm': 1.1200064420700073, 'learning_rate': 7.231300221394036e-06, 'epoch': 2.33}
{'loss': 2.832, 'grad_norm': 1.4270734786987305, 'learning_rate': 7.210440592633263e-06, 'epoch': 2.33}
{'loss': 2.8883, 'grad_norm': 1.5269739627838135, 'learning_rate': 7.189606022484041e-06, 'epoch': 2.33}
{'loss': 3.4477, 'grad_norm': 2.0976343154907227, 'learning_rate': 7.168796540294215e-06, 'epoch': 2.33}
{'loss': 3.5252, 'grad_norm': 2.1466851234436035, 'learning_rate': 7.1480121753763145e-06, 'epoch': 2.33}
{'loss': 3.572, 'grad_norm': 2.0267908573150635, 'learning_rate': 7.127252957007463e-06, 'epoch': 2.34}
{'loss': 3.3925, 'grad_norm': 0.4268759787082672, 'learning_rate': 7.106518914429383e-06, 'epoch': 2.34}
{'loss': 2.8248, 'grad_norm': 0.600017249584198, 'learning_rate': 7.0858100768483235e-06, 'epoch': 2.34}
{'loss': 2.7756, 'grad_norm': 0.7327286601066589, 'learning_rate': 7.065126473435022e-06, 'epoch': 2.34}
{'loss': 2.7282, 'grad_norm': 0.9452219009399414, 'learning_rate': 7.04446813332468e-06, 'epoch': 2.34}
{'loss': 2.8164, 'grad_norm': 1.1710197925567627, 'learning_rate': 7.02383508561692e-06, 'epoch': 2.34}
{'loss': 2.7749, 'grad_norm': 1.396876335144043, 'learning_rate': 7.003227359375719e-06, 'epoch': 2.34}
{'loss': 3.049, 'grad_norm': 1.8947402238845825, 'learning_rate': 6.982644983629411e-06, 'epoch': 2.34}
{'loss': 3.4557, 'grad_norm': 1.8332983255386353, 'learning_rate': 6.962087987370597e-06, 'epoch': 2.34}
{'loss': 3.5362, 'grad_norm': 2.0468029975891113, 'learning_rate': 6.941556399556134e-06, 'epoch': 2.35}
{'loss': 3.5307, 'grad_norm': 2.1524715423583984, 'learning_rate': 6.921050249107092e-06, 'epoch': 2.35}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.029177665710449, 'eval_runtime': 12.6872, 'eval_samples_per_second': 128.791, 'eval_steps_per_second': 16.158, 'epoch': 2.35}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.4862, 'grad_norm': 0.3900856375694275, 'learning_rate': 6.900569564908715e-06, 'epoch': 2.35}
{'loss': 2.937, 'grad_norm': 0.6124845147132874, 'learning_rate': 6.880114375810362e-06, 'epoch': 2.35}
{'loss': 2.6745, 'grad_norm': 0.9068475365638733, 'learning_rate': 6.859684710625494e-06, 'epoch': 2.35}
{'loss': 2.6315, 'grad_norm': 1.0340220928192139, 'learning_rate': 6.8392805981316035e-06, 'epoch': 2.35}
{'loss': 2.7726, 'grad_norm': 1.0642977952957153, 'learning_rate': 6.818902067070204e-06, 'epoch': 2.35}
{'loss': 2.8487, 'grad_norm': 1.3873926401138306, 'learning_rate': 6.798549146146762e-06, 'epoch': 2.35}
{'loss': 3.1338, 'grad_norm': 1.8899692296981812, 'learning_rate': 6.778221864030679e-06, 'epoch': 2.35}
{'loss': 3.6829, 'grad_norm': 1.9792851209640503, 'learning_rate': 6.757920249355224e-06, 'epoch': 2.35}
{'loss': 3.5338, 'grad_norm': 2.01369309425354, 'learning_rate': 6.73764433071754e-06, 'epoch': 2.36}
{'loss': 3.421, 'grad_norm': 2.2134616374969482, 'learning_rate': 6.717394136678542e-06, 'epoch': 2.36}
{'loss': 3.4023, 'grad_norm': 0.41615620255470276, 'learning_rate': 6.697169695762942e-06, 'epoch': 2.36}
{'loss': 2.7778, 'grad_norm': 2.145198106765747, 'learning_rate': 6.676971036459148e-06, 'epoch': 2.36}
{'loss': 2.6583, 'grad_norm': 0.7212198972702026, 'learning_rate': 6.656798187219254e-06, 'epoch': 2.36}
{'loss': 2.6692, 'grad_norm': 0.9275795817375183, 'learning_rate': 6.636651176459022e-06, 'epoch': 2.36}
{'loss': 2.7112, 'grad_norm': 1.1287950277328491, 'learning_rate': 6.616530032557791e-06, 'epoch': 2.36}
{'loss': 2.7694, 'grad_norm': 1.45880925655365, 'learning_rate': 6.596434783858471e-06, 'epoch': 2.36}
{'loss': 2.9598, 'grad_norm': 1.6879873275756836, 'learning_rate': 6.57636545866751e-06, 'epoch': 2.36}
{'loss': 3.5437, 'grad_norm': 1.9126837253570557, 'learning_rate': 6.55632208525482e-06, 'epoch': 2.36}
{'loss': 3.445, 'grad_norm': 2.107815742492676, 'learning_rate': 6.5363046918537615e-06, 'epoch': 2.37}
{'loss': 3.4356, 'grad_norm': 2.034600019454956, 'learning_rate': 6.516313306661112e-06, 'epoch': 2.37}
{'loss': 3.4143, 'grad_norm': 0.41210630536079407, 'learning_rate': 6.496347957836996e-06, 'epoch': 2.37}
{'loss': 2.8298, 'grad_norm': 0.7160335779190063, 'learning_rate': 6.47640867350488e-06, 'epoch': 2.37}
{'loss': 2.6423, 'grad_norm': 0.704264223575592, 'learning_rate': 6.456495481751495e-06, 'epoch': 2.37}
{'loss': 2.6798, 'grad_norm': 0.8282089829444885, 'learning_rate': 6.4366084106268545e-06, 'epoch': 2.37}
{'loss': 2.8021, 'grad_norm': 1.111873984336853, 'learning_rate': 6.416747488144118e-06, 'epoch': 2.37}
{'loss': 2.826, 'grad_norm': 1.4431415796279907, 'learning_rate': 6.39691274227967e-06, 'epoch': 2.37}
{'loss': 3.0012, 'grad_norm': 1.8817355632781982, 'learning_rate': 6.377104200972989e-06, 'epoch': 2.37}
{'loss': 3.5, 'grad_norm': 1.7239772081375122, 'learning_rate': 6.3573218921266605e-06, 'epoch': 2.37}
{'loss': 3.4916, 'grad_norm': 2.2489750385284424, 'learning_rate': 6.337565843606299e-06, 'epoch': 2.38}
{'loss': 3.2634, 'grad_norm': 2.1907455921173096, 'learning_rate': 6.31783608324055e-06, 'epoch': 2.38}
{'loss': 3.3189, 'grad_norm': 0.4335721731185913, 'learning_rate': 6.298132638821014e-06, 'epoch': 2.38}
{'loss': 2.8146, 'grad_norm': 0.5928646326065063, 'learning_rate': 6.278455538102226e-06, 'epoch': 2.38}
{'loss': 2.7261, 'grad_norm': 0.8039777874946594, 'learning_rate': 6.258804808801605e-06, 'epoch': 2.38}
{'loss': 2.6669, 'grad_norm': 0.916424036026001, 'learning_rate': 6.239180478599446e-06, 'epoch': 2.38}
{'loss': 2.8295, 'grad_norm': 1.232703685760498, 'learning_rate': 6.21958257513883e-06, 'epoch': 2.38}
{'loss': 2.9667, 'grad_norm': 1.4459086656570435, 'learning_rate': 6.2000111260256395e-06, 'epoch': 2.38}
{'loss': 3.1664, 'grad_norm': 1.6443829536437988, 'learning_rate': 6.180466158828471e-06, 'epoch': 2.38}
{'loss': 3.4918, 'grad_norm': 1.863312005996704, 'learning_rate': 6.160947701078632e-06, 'epoch': 2.38}
{'loss': 3.3706, 'grad_norm': 2.079399824142456, 'learning_rate': 6.141455780270072e-06, 'epoch': 2.39}
{'loss': 3.4285, 'grad_norm': 2.3242363929748535, 'learning_rate': 6.121990423859386e-06, 'epoch': 2.39}
{'loss': 3.4138, 'grad_norm': 0.40341293811798096, 'learning_rate': 6.1025516592657235e-06, 'epoch': 2.39}
{'loss': 2.7698, 'grad_norm': 0.6256332993507385, 'learning_rate': 6.083139513870803e-06, 'epoch': 2.39}
{'loss': 2.61, 'grad_norm': 0.7757515907287598, 'learning_rate': 6.063754015018822e-06, 'epoch': 2.39}
{'loss': 2.8193, 'grad_norm': 0.9010021686553955, 'learning_rate': 6.044395190016458e-06, 'epoch': 2.39}
{'loss': 2.6024, 'grad_norm': 1.146894097328186, 'learning_rate': 6.025063066132799e-06, 'epoch': 2.39}
{'loss': 2.8294, 'grad_norm': 1.2497837543487549, 'learning_rate': 6.005757670599352e-06, 'epoch': 2.39}
{'loss': 3.0251, 'grad_norm': 1.8744639158248901, 'learning_rate': 5.986479030609937e-06, 'epoch': 2.39}
{'loss': 3.5641, 'grad_norm': 2.272576093673706, 'learning_rate': 5.967227173320724e-06, 'epoch': 2.39}
{'loss': 3.639, 'grad_norm': 2.0586860179901123, 'learning_rate': 5.948002125850122e-06, 'epoch': 2.4}
{'loss': 3.4857, 'grad_norm': 2.072298288345337, 'learning_rate': 5.9288039152788075e-06, 'epoch': 2.4}
{'loss': 3.3694, 'grad_norm': 0.4592658579349518, 'learning_rate': 5.909632568649609e-06, 'epoch': 2.4}
{'loss': 2.9406, 'grad_norm': 0.612640917301178, 'learning_rate': 5.890488112967568e-06, 'epoch': 2.4}
{'loss': 2.7569, 'grad_norm': 0.7318547368049622, 'learning_rate': 5.871370575199803e-06, 'epoch': 2.4}
{'loss': 2.7074, 'grad_norm': 0.8436561226844788, 'learning_rate': 5.8522799822755494e-06, 'epoch': 2.4}
{'loss': 2.6578, 'grad_norm': 1.0410934686660767, 'learning_rate': 5.833216361086055e-06, 'epoch': 2.4}
{'loss': 2.8056, 'grad_norm': 1.4526495933532715, 'learning_rate': 5.814179738484618e-06, 'epoch': 2.4}
{'loss': 2.8887, 'grad_norm': 1.4892443418502808, 'learning_rate': 5.795170141286446e-06, 'epoch': 2.4}
{'loss': 3.3407, 'grad_norm': 2.0264904499053955, 'learning_rate': 5.776187596268737e-06, 'epoch': 2.41}
{'loss': 3.5478, 'grad_norm': 2.0598134994506836, 'learning_rate': 5.757232130170545e-06, 'epoch': 2.41}
{'loss': 3.5088, 'grad_norm': 2.129518747329712, 'learning_rate': 5.7383037696928045e-06, 'epoch': 2.41}
{'loss': 3.3233, 'grad_norm': 0.3979799449443817, 'learning_rate': 5.719402541498242e-06, 'epoch': 2.41}
{'loss': 2.8133, 'grad_norm': 0.5942718386650085, 'learning_rate': 5.7005284722114e-06, 'epoch': 2.41}
{'loss': 2.6988, 'grad_norm': 0.7559667825698853, 'learning_rate': 5.681681588418536e-06, 'epoch': 2.41}
{'loss': 2.6497, 'grad_norm': 0.8983208537101746, 'learning_rate': 5.662861916667611e-06, 'epoch': 2.41}
{'loss': 2.7411, 'grad_norm': 1.1171966791152954, 'learning_rate': 5.644069483468289e-06, 'epoch': 2.41}
{'loss': 2.8879, 'grad_norm': 1.2596938610076904, 'learning_rate': 5.62530431529183e-06, 'epoch': 2.41}
{'loss': 2.9541, 'grad_norm': 1.562258005142212, 'learning_rate': 5.606566438571095e-06, 'epoch': 2.41}
{'loss': 3.4509, 'grad_norm': 1.8504905700683594, 'learning_rate': 5.587855879700523e-06, 'epoch': 2.42}
{'loss': 3.5441, 'grad_norm': 1.9373449087142944, 'learning_rate': 5.569172665036052e-06, 'epoch': 2.42}
{'loss': 3.4072, 'grad_norm': 2.1031923294067383, 'learning_rate': 5.550516820895096e-06, 'epoch': 2.42}
{'loss': 3.3325, 'grad_norm': 0.4321836233139038, 'learning_rate': 5.5318883735565485e-06, 'epoch': 2.42}
{'loss': 2.8418, 'grad_norm': 0.719411313533783, 'learning_rate': 5.513287349260671e-06, 'epoch': 2.42}
{'loss': 2.6652, 'grad_norm': 0.7219605445861816, 'learning_rate': 5.4947137742091285e-06, 'epoch': 2.42}
{'loss': 2.6196, 'grad_norm': 0.834403395652771, 'learning_rate': 5.476167674564905e-06, 'epoch': 2.42}
{'loss': 2.8207, 'grad_norm': 1.2275121212005615, 'learning_rate': 5.457649076452273e-06, 'epoch': 2.42}
{'loss': 2.8437, 'grad_norm': 1.434012770652771, 'learning_rate': 5.4391580059567936e-06, 'epoch': 2.42}
{'loss': 3.0618, 'grad_norm': 1.681119680404663, 'learning_rate': 5.420694489125231e-06, 'epoch': 2.42}
{'loss': 3.5828, 'grad_norm': 1.9608333110809326, 'learning_rate': 5.402258551965534e-06, 'epoch': 2.43}
{'loss': 3.4714, 'grad_norm': 2.093271255493164, 'learning_rate': 5.383850220446823e-06, 'epoch': 2.43}
{'loss': 3.5728, 'grad_norm': 2.3476853370666504, 'learning_rate': 5.365469520499306e-06, 'epoch': 2.43}
{'loss': 3.3713, 'grad_norm': 0.37861233949661255, 'learning_rate': 5.3471164780142995e-06, 'epoch': 2.43}
{'loss': 2.7443, 'grad_norm': 0.6841650009155273, 'learning_rate': 5.328791118844137e-06, 'epoch': 2.43}
{'loss': 2.7688, 'grad_norm': 0.8181324601173401, 'learning_rate': 5.310493468802166e-06, 'epoch': 2.43}
{'loss': 2.7912, 'grad_norm': 1.0175771713256836, 'learning_rate': 5.2922235536626924e-06, 'epoch': 2.43}
{'loss': 2.8172, 'grad_norm': 1.2285438776016235, 'learning_rate': 5.273981399160977e-06, 'epoch': 2.43}
{'loss': 2.8458, 'grad_norm': 1.4650516510009766, 'learning_rate': 5.255767030993148e-06, 'epoch': 2.43}
{'loss': 3.1781, 'grad_norm': 1.8392157554626465, 'learning_rate': 5.237580474816225e-06, 'epoch': 2.43}
{'loss': 3.5646, 'grad_norm': 2.0869901180267334, 'learning_rate': 5.2194217562480274e-06, 'epoch': 2.44}
{'loss': 3.4396, 'grad_norm': 1.9643563032150269, 'learning_rate': 5.201290900867167e-06, 'epoch': 2.44}
{'loss': 3.3316, 'grad_norm': 2.3909218311309814, 'learning_rate': 5.183187934213002e-06, 'epoch': 2.44}
{'loss': 3.2404, 'grad_norm': 0.4207959473133087, 'learning_rate': 5.165112881785633e-06, 'epoch': 2.44}
{'loss': 2.763, 'grad_norm': 0.6156811714172363, 'learning_rate': 5.1470657690458e-06, 'epoch': 2.44}
{'loss': 2.6641, 'grad_norm': 0.8122045993804932, 'learning_rate': 5.129046621414927e-06, 'epoch': 2.44}
{'loss': 2.723, 'grad_norm': 1.1175875663757324, 'learning_rate': 5.111055464275008e-06, 'epoch': 2.44}
{'loss': 2.7081, 'grad_norm': 1.0740355253219604, 'learning_rate': 5.093092322968654e-06, 'epoch': 2.44}
{'loss': 2.8566, 'grad_norm': 1.5432499647140503, 'learning_rate': 5.075157222798954e-06, 'epoch': 2.44}
{'loss': 2.903, 'grad_norm': 1.6766072511672974, 'learning_rate': 5.0572501890295524e-06, 'epoch': 2.44}
{'loss': 3.3622, 'grad_norm': 2.119805335998535, 'learning_rate': 5.039371246884522e-06, 'epoch': 2.45}
{'loss': 3.5904, 'grad_norm': 2.1566238403320312, 'learning_rate': 5.021520421548395e-06, 'epoch': 2.45}
{'loss': 3.4563, 'grad_norm': 2.069577932357788, 'learning_rate': 5.003697738166072e-06, 'epoch': 2.45}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0272469520568848, 'eval_runtime': 12.5767, 'eval_samples_per_second': 129.923, 'eval_steps_per_second': 16.3, 'epoch': 2.45}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.3442, 'grad_norm': 0.42326322197914124, 'learning_rate': 4.985903221842839e-06, 'epoch': 2.45}
{'loss': 2.7244, 'grad_norm': 0.6700990200042725, 'learning_rate': 4.968136897644265e-06, 'epoch': 2.45}
{'loss': 2.6935, 'grad_norm': 0.7918915748596191, 'learning_rate': 4.950398790596253e-06, 'epoch': 2.45}
{'loss': 2.7746, 'grad_norm': 0.8698014616966248, 'learning_rate': 4.9326889256849225e-06, 'epoch': 2.45}
{'loss': 2.8314, 'grad_norm': 1.2972500324249268, 'learning_rate': 4.915007327856641e-06, 'epoch': 2.45}
{'loss': 2.8419, 'grad_norm': 1.19189453125, 'learning_rate': 4.897354022017933e-06, 'epoch': 2.45}
{'loss': 3.1872, 'grad_norm': 1.8856635093688965, 'learning_rate': 4.8797290330354905e-06, 'epoch': 2.46}
{'loss': 3.3117, 'grad_norm': 1.777178406715393, 'learning_rate': 4.862132385736104e-06, 'epoch': 2.46}
{'loss': 3.555, 'grad_norm': 2.0757298469543457, 'learning_rate': 4.844564104906646e-06, 'epoch': 2.46}
{'loss': 3.5909, 'grad_norm': 2.1439356803894043, 'learning_rate': 4.827024215294026e-06, 'epoch': 2.46}
{'loss': 3.488, 'grad_norm': 0.38143160939216614, 'learning_rate': 4.809512741605182e-06, 'epoch': 2.46}
{'loss': 2.7673, 'grad_norm': 0.6089022755622864, 'learning_rate': 4.792029708506998e-06, 'epoch': 2.46}
{'loss': 2.7414, 'grad_norm': 0.7850980758666992, 'learning_rate': 4.7745751406263165e-06, 'epoch': 2.46}
{'loss': 2.6521, 'grad_norm': 0.9649539589881897, 'learning_rate': 4.75714906254987e-06, 'epoch': 2.46}
{'loss': 2.7713, 'grad_norm': 1.233556866645813, 'learning_rate': 4.7397514988242714e-06, 'epoch': 2.46}
{'loss': 2.8635, 'grad_norm': 1.4569681882858276, 'learning_rate': 4.72238247395595e-06, 'epoch': 2.46}
{'loss': 3.0717, 'grad_norm': 1.5598702430725098, 'learning_rate': 4.705042012411159e-06, 'epoch': 2.47}
{'loss': 3.3639, 'grad_norm': 2.0524253845214844, 'learning_rate': 4.687730138615895e-06, 'epoch': 2.47}
{'loss': 3.5412, 'grad_norm': 2.04732084274292, 'learning_rate': 4.6704468769559084e-06, 'epoch': 2.47}
{'loss': 3.4344, 'grad_norm': 2.083437204360962, 'learning_rate': 4.653192251776617e-06, 'epoch': 2.47}
{'loss': 3.2997, 'grad_norm': 0.3703264892101288, 'learning_rate': 4.635966287383129e-06, 'epoch': 2.47}
{'loss': 2.6913, 'grad_norm': 0.6549732685089111, 'learning_rate': 4.618769008040166e-06, 'epoch': 2.47}
{'loss': 2.7754, 'grad_norm': 0.7988151907920837, 'learning_rate': 4.601600437972045e-06, 'epoch': 2.47}
{'loss': 2.8134, 'grad_norm': 0.9357365369796753, 'learning_rate': 4.5844606013626364e-06, 'epoch': 2.47}
{'loss': 2.6538, 'grad_norm': 1.071460247039795, 'learning_rate': 4.567349522355355e-06, 'epoch': 2.47}
{'loss': 2.8373, 'grad_norm': 1.2929332256317139, 'learning_rate': 4.550267225053084e-06, 'epoch': 2.47}
{'loss': 2.9507, 'grad_norm': 1.6122660636901855, 'learning_rate': 4.533213733518191e-06, 'epoch': 2.48}
{'loss': 3.3914, 'grad_norm': 1.9152121543884277, 'learning_rate': 4.516189071772445e-06, 'epoch': 2.48}
{'loss': 3.4889, 'grad_norm': 2.209096908569336, 'learning_rate': 4.499193263797003e-06, 'epoch': 2.48}
{'loss': 3.3487, 'grad_norm': 2.1515281200408936, 'learning_rate': 4.482226333532402e-06, 'epoch': 2.48}
{'loss': 3.3647, 'grad_norm': 0.4513043165206909, 'learning_rate': 4.465288304878479e-06, 'epoch': 2.48}
{'loss': 2.8066, 'grad_norm': 0.6819474101066589, 'learning_rate': 4.448379201694372e-06, 'epoch': 2.48}
{'loss': 2.7335, 'grad_norm': 0.8430533409118652, 'learning_rate': 4.431499047798471e-06, 'epoch': 2.48}
{'loss': 2.6465, 'grad_norm': 0.8984132409095764, 'learning_rate': 4.4146478669683855e-06, 'epoch': 2.48}
{'loss': 2.6484, 'grad_norm': 1.197125792503357, 'learning_rate': 4.3978256829409065e-06, 'epoch': 2.48}
{'loss': 2.8569, 'grad_norm': 1.5169322490692139, 'learning_rate': 4.3810325194119985e-06, 'epoch': 2.48}
{'loss': 3.1957, 'grad_norm': 1.8368903398513794, 'learning_rate': 4.364268400036728e-06, 'epoch': 2.49}
{'loss': 3.705, 'grad_norm': 1.9524866342544556, 'learning_rate': 4.347533348429269e-06, 'epoch': 2.49}
{'loss': 3.5944, 'grad_norm': 2.3334548473358154, 'learning_rate': 4.330827388162825e-06, 'epoch': 2.49}
{'loss': 3.3021, 'grad_norm': 2.2153468132019043, 'learning_rate': 4.3141505427696545e-06, 'epoch': 2.49}
{'loss': 3.4299, 'grad_norm': 0.5011171698570251, 'learning_rate': 4.297502835740963e-06, 'epoch': 2.49}
{'loss': 2.7384, 'grad_norm': 0.6474204063415527, 'learning_rate': 4.280884290526951e-06, 'epoch': 2.49}
{'loss': 2.6536, 'grad_norm': 0.7267234921455383, 'learning_rate': 4.264294930536708e-06, 'epoch': 2.49}
{'loss': 2.5875, 'grad_norm': 0.9920799136161804, 'learning_rate': 4.247734779138246e-06, 'epoch': 2.49}
{'loss': 2.7548, 'grad_norm': 1.2916302680969238, 'learning_rate': 4.2312038596584e-06, 'epoch': 2.49}
{'loss': 2.8387, 'grad_norm': 1.3010785579681396, 'learning_rate': 4.214702195382863e-06, 'epoch': 2.49}
{'loss': 3.2462, 'grad_norm': 1.7715578079223633, 'learning_rate': 4.198229809556081e-06, 'epoch': 2.5}
{'loss': 3.4084, 'grad_norm': 1.9917082786560059, 'learning_rate': 4.181786725381287e-06, 'epoch': 2.5}
{'loss': 3.5197, 'grad_norm': 2.112494468688965, 'learning_rate': 4.165372966020426e-06, 'epoch': 2.5}
{'loss': 3.5062, 'grad_norm': 2.425354242324829, 'learning_rate': 4.148988554594144e-06, 'epoch': 2.5}
{'loss': 3.2741, 'grad_norm': 0.4337572455406189, 'learning_rate': 4.132633514181733e-06, 'epoch': 2.5}
{'loss': 2.8154, 'grad_norm': 0.6448802351951599, 'learning_rate': 4.116307867821137e-06, 'epoch': 2.5}
{'loss': 2.6099, 'grad_norm': 1.1390149593353271, 'learning_rate': 4.100011638508869e-06, 'epoch': 2.5}
{'loss': 2.6989, 'grad_norm': 0.8981062173843384, 'learning_rate': 4.083744849200019e-06, 'epoch': 2.5}
{'loss': 2.7958, 'grad_norm': 1.2300400733947754, 'learning_rate': 4.067507522808195e-06, 'epoch': 2.5}
{'loss': 2.9367, 'grad_norm': 1.2440251111984253, 'learning_rate': 4.051299682205528e-06, 'epoch': 2.51}
{'loss': 3.0145, 'grad_norm': 1.8275504112243652, 'learning_rate': 4.035121350222587e-06, 'epoch': 2.51}
{'loss': 3.4778, 'grad_norm': 1.9403526782989502, 'learning_rate': 4.018972549648398e-06, 'epoch': 2.51}
{'loss': 3.368, 'grad_norm': 2.3923776149749756, 'learning_rate': 4.002853303230369e-06, 'epoch': 2.51}
{'loss': 3.5179, 'grad_norm': 2.087097644805908, 'learning_rate': 3.986763633674293e-06, 'epoch': 2.51}
{'loss': 3.3049, 'grad_norm': 0.4162743091583252, 'learning_rate': 3.970703563644279e-06, 'epoch': 2.51}
{'loss': 2.7101, 'grad_norm': 0.566519558429718, 'learning_rate': 3.954673115762777e-06, 'epoch': 2.51}
{'loss': 2.7487, 'grad_norm': 0.9059116840362549, 'learning_rate': 3.938672312610475e-06, 'epoch': 2.51}
{'loss': 2.6856, 'grad_norm': 0.9828047156333923, 'learning_rate': 3.922701176726329e-06, 'epoch': 2.51}
{'loss': 2.8373, 'grad_norm': 1.100378394126892, 'learning_rate': 3.906759730607484e-06, 'epoch': 2.51}
{'loss': 2.8352, 'grad_norm': 1.5251843929290771, 'learning_rate': 3.890847996709293e-06, 'epoch': 2.52}
{'loss': 3.2323, 'grad_norm': 1.750165581703186, 'learning_rate': 3.874965997445215e-06, 'epoch': 2.52}
{'loss': 3.5952, 'grad_norm': 2.371838092803955, 'learning_rate': 3.859113755186861e-06, 'epoch': 2.52}
{'loss': 3.5331, 'grad_norm': 1.9305604696273804, 'learning_rate': 3.8432912922639036e-06, 'epoch': 2.52}
{'loss': 3.702, 'grad_norm': 2.3263299465179443, 'learning_rate': 3.827498630964082e-06, 'epoch': 2.52}
{'loss': 3.3962, 'grad_norm': 0.40711095929145813, 'learning_rate': 3.811735793533147e-06, 'epoch': 2.52}
{'loss': 2.7744, 'grad_norm': 0.7043519020080566, 'learning_rate': 3.7960028021748475e-06, 'epoch': 2.52}
{'loss': 2.647, 'grad_norm': 0.7213107347488403, 'learning_rate': 3.7802996790508832e-06, 'epoch': 2.52}
{'loss': 2.6533, 'grad_norm': 2.1104586124420166, 'learning_rate': 3.7646264462808807e-06, 'epoch': 2.52}
{'loss': 2.7404, 'grad_norm': 1.1971081495285034, 'learning_rate': 3.7489831259423624e-06, 'epoch': 2.52}
{'loss': 2.8513, 'grad_norm': 1.400149941444397, 'learning_rate': 3.7333697400707273e-06, 'epoch': 2.53}
{'loss': 3.0642, 'grad_norm': 1.6052786111831665, 'learning_rate': 3.7177863106591935e-06, 'epoch': 2.53}
{'loss': 3.5523, 'grad_norm': 2.0244264602661133, 'learning_rate': 3.702232859658794e-06, 'epoch': 2.53}
{'loss': 3.5206, 'grad_norm': 2.1724917888641357, 'learning_rate': 3.6867094089783256e-06, 'epoch': 2.53}
{'loss': 3.5472, 'grad_norm': 2.1079161167144775, 'learning_rate': 3.6712159804843216e-06, 'epoch': 2.53}
{'loss': 3.2778, 'grad_norm': 0.40664219856262207, 'learning_rate': 3.6557525960010467e-06, 'epoch': 2.53}
{'loss': 2.7694, 'grad_norm': 0.5721880197525024, 'learning_rate': 3.640319277310425e-06, 'epoch': 2.53}
{'loss': 2.6913, 'grad_norm': 0.6916342973709106, 'learning_rate': 3.624916046152027e-06, 'epoch': 2.53}
{'loss': 2.7905, 'grad_norm': 0.8474966287612915, 'learning_rate': 3.6095429242230638e-06, 'epoch': 2.53}
{'loss': 2.7384, 'grad_norm': 1.0849686861038208, 'learning_rate': 3.5941999331783135e-06, 'epoch': 2.53}
{'loss': 2.9007, 'grad_norm': 1.2599060535430908, 'learning_rate': 3.5788870946301177e-06, 'epoch': 2.54}
{'loss': 3.0066, 'grad_norm': 1.6332470178604126, 'learning_rate': 3.5636044301483513e-06, 'epoch': 2.54}
{'loss': 3.5307, 'grad_norm': 2.2206461429595947, 'learning_rate': 3.548351961260374e-06, 'epoch': 2.54}
{'loss': 3.424, 'grad_norm': 1.9634181261062622, 'learning_rate': 3.5331297094510245e-06, 'epoch': 2.54}
{'loss': 3.5528, 'grad_norm': 2.1803719997406006, 'learning_rate': 3.5179376961625627e-06, 'epoch': 2.54}
{'loss': 3.367, 'grad_norm': 0.4662444293498993, 'learning_rate': 3.5027759427946756e-06, 'epoch': 2.54}
{'loss': 2.7466, 'grad_norm': 0.585026204586029, 'learning_rate': 3.487644470704393e-06, 'epoch': 2.54}
{'loss': 2.6334, 'grad_norm': 0.7519397735595703, 'learning_rate': 3.4725433012061213e-06, 'epoch': 2.54}
{'loss': 2.7369, 'grad_norm': 0.9093805551528931, 'learning_rate': 3.4574724555715643e-06, 'epoch': 2.54}
{'loss': 2.8562, 'grad_norm': 1.4155553579330444, 'learning_rate': 3.4424319550297235e-06, 'epoch': 2.54}
{'loss': 2.7657, 'grad_norm': 1.4675503969192505, 'learning_rate': 3.427421820766841e-06, 'epoch': 2.55}
{'loss': 3.1887, 'grad_norm': 1.6791653633117676, 'learning_rate': 3.412442073926403e-06, 'epoch': 2.55}
{'loss': 3.4013, 'grad_norm': 1.766640543937683, 'learning_rate': 3.3974927356090756e-06, 'epoch': 2.55}
{'loss': 3.4512, 'grad_norm': 2.153831958770752, 'learning_rate': 3.382573826872698e-06, 'epoch': 2.55}
{'loss': 3.4502, 'grad_norm': 2.2822048664093018, 'learning_rate': 3.3676853687322374e-06, 'epoch': 2.55}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.027690887451172, 'eval_runtime': 12.6562, 'eval_samples_per_second': 129.106, 'eval_steps_per_second': 16.198, 'epoch': 2.55}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.2861, 'grad_norm': 0.4284617304801941, 'learning_rate': 3.352827382159787e-06, 'epoch': 2.55}
{'loss': 2.7345, 'grad_norm': 0.6582229137420654, 'learning_rate': 3.3379998880844975e-06, 'epoch': 2.55}
{'loss': 2.7826, 'grad_norm': 0.7305985689163208, 'learning_rate': 3.3232029073925835e-06, 'epoch': 2.55}
{'loss': 2.6279, 'grad_norm': 0.9066538214683533, 'learning_rate': 3.308436460927264e-06, 'epoch': 2.55}
{'loss': 2.7287, 'grad_norm': 1.0709424018859863, 'learning_rate': 3.293700569488753e-06, 'epoch': 2.56}
{'loss': 2.8188, 'grad_norm': 1.3309522867202759, 'learning_rate': 3.2789952538342215e-06, 'epoch': 2.56}
{'loss': 3.0242, 'grad_norm': 1.621570348739624, 'learning_rate': 3.264320534677787e-06, 'epoch': 2.56}
{'loss': 3.5088, 'grad_norm': 1.9885002374649048, 'learning_rate': 3.2496764326904378e-06, 'epoch': 2.56}
{'loss': 3.4809, 'grad_norm': 2.070864200592041, 'learning_rate': 3.235062968500066e-06, 'epoch': 2.56}
{'loss': 3.4594, 'grad_norm': 2.156583309173584, 'learning_rate': 3.220480162691383e-06, 'epoch': 2.56}
{'loss': 3.256, 'grad_norm': 0.409054696559906, 'learning_rate': 3.205928035805941e-06, 'epoch': 2.56}
{'loss': 2.737, 'grad_norm': 0.5975592136383057, 'learning_rate': 3.1914066083420373e-06, 'epoch': 2.56}
{'loss': 2.8571, 'grad_norm': 0.7596002221107483, 'learning_rate': 3.176915900754768e-06, 'epoch': 2.56}
{'loss': 2.7885, 'grad_norm': 0.9112855792045593, 'learning_rate': 3.162455933455924e-06, 'epoch': 2.56}
{'loss': 2.8689, 'grad_norm': 1.2246131896972656, 'learning_rate': 3.1480267268140185e-06, 'epoch': 2.57}
{'loss': 3.0194, 'grad_norm': 1.8145984411239624, 'learning_rate': 3.1336283011542176e-06, 'epoch': 2.57}
{'loss': 2.9822, 'grad_norm': 1.6424896717071533, 'learning_rate': 3.1192606767583537e-06, 'epoch': 2.57}
{'loss': 3.4195, 'grad_norm': 1.7265692949295044, 'learning_rate': 3.104923873864829e-06, 'epoch': 2.57}
{'loss': 3.5047, 'grad_norm': 1.8308995962142944, 'learning_rate': 3.090617912668675e-06, 'epoch': 2.57}
{'loss': 3.3364, 'grad_norm': 2.2070670127868652, 'learning_rate': 3.076342813321448e-06, 'epoch': 2.57}
{'loss': 3.3922, 'grad_norm': 0.404030978679657, 'learning_rate': 3.0620985959312564e-06, 'epoch': 2.57}
{'loss': 2.8829, 'grad_norm': 0.5931008458137512, 'learning_rate': 3.047885280562682e-06, 'epoch': 2.57}
{'loss': 2.6951, 'grad_norm': 0.7645406126976013, 'learning_rate': 3.033702887236811e-06, 'epoch': 2.57}
{'loss': 2.6982, 'grad_norm': 0.9608631730079651, 'learning_rate': 3.0195514359311285e-06, 'epoch': 2.57}
{'loss': 2.7364, 'grad_norm': 1.2535384893417358, 'learning_rate': 3.0054309465795767e-06, 'epoch': 2.58}
{'loss': 2.8778, 'grad_norm': 1.3054306507110596, 'learning_rate': 2.991341439072451e-06, 'epoch': 2.58}
{'loss': 2.9759, 'grad_norm': 1.6922099590301514, 'learning_rate': 2.9772829332564368e-06, 'epoch': 2.58}
{'loss': 3.5468, 'grad_norm': 2.0482566356658936, 'learning_rate': 2.963255448934518e-06, 'epoch': 2.58}
{'loss': 3.5518, 'grad_norm': 2.001643657684326, 'learning_rate': 2.949259005866009e-06, 'epoch': 2.58}
{'loss': 3.5215, 'grad_norm': 2.3122165203094482, 'learning_rate': 2.93529362376648e-06, 'epoch': 2.58}
{'loss': 3.3142, 'grad_norm': 0.3922271430492401, 'learning_rate': 2.9213593223077594e-06, 'epoch': 2.58}
{'loss': 2.7497, 'grad_norm': 0.6680291891098022, 'learning_rate': 2.9074561211178825e-06, 'epoch': 2.58}
{'loss': 2.7028, 'grad_norm': 0.7189566493034363, 'learning_rate': 2.893584039781097e-06, 'epoch': 2.58}
{'loss': 2.6702, 'grad_norm': 0.9546238780021667, 'learning_rate': 2.8797430978377894e-06, 'epoch': 2.58}
{'loss': 2.7903, 'grad_norm': 1.1526883840560913, 'learning_rate': 2.8659333147845053e-06, 'epoch': 2.59}
{'loss': 2.8015, 'grad_norm': 1.3874260187149048, 'learning_rate': 2.85215471007389e-06, 'epoch': 2.59}
{'loss': 3.2271, 'grad_norm': 1.6247081756591797, 'learning_rate': 2.8384073031146626e-06, 'epoch': 2.59}
{'loss': 3.5424, 'grad_norm': 2.1788644790649414, 'learning_rate': 2.824691113271613e-06, 'epoch': 2.59}
{'loss': 3.4967, 'grad_norm': 2.0864908695220947, 'learning_rate': 2.8110061598655426e-06, 'epoch': 2.59}
{'loss': 3.503, 'grad_norm': 2.433070659637451, 'learning_rate': 2.7973524621732668e-06, 'epoch': 2.59}
{'loss': 3.287, 'grad_norm': 0.4457760155200958, 'learning_rate': 2.7837300394275652e-06, 'epoch': 2.59}
{'loss': 2.8136, 'grad_norm': 0.7097207307815552, 'learning_rate': 2.7701389108171605e-06, 'epoch': 2.59}
{'loss': 2.6543, 'grad_norm': 0.7720293998718262, 'learning_rate': 2.7565790954867027e-06, 'epoch': 2.59}
{'loss': 2.7184, 'grad_norm': 0.8784360885620117, 'learning_rate': 2.7430506125367297e-06, 'epoch': 2.59}
{'loss': 2.7837, 'grad_norm': 1.1404787302017212, 'learning_rate': 2.729553481023639e-06, 'epoch': 2.6}
{'loss': 2.8493, 'grad_norm': 1.2919950485229492, 'learning_rate': 2.7160877199596758e-06, 'epoch': 2.6}
{'loss': 2.9597, 'grad_norm': 1.7702751159667969, 'learning_rate': 2.7026533483128865e-06, 'epoch': 2.6}
{'loss': 3.4892, 'grad_norm': 2.123859167098999, 'learning_rate': 2.689250385007114e-06, 'epoch': 2.6}
{'loss': 3.559, 'grad_norm': 2.1151859760284424, 'learning_rate': 2.675878848921948e-06, 'epoch': 2.6}
{'loss': 3.4834, 'grad_norm': 2.0869486331939697, 'learning_rate': 2.662538758892713e-06, 'epoch': 2.6}
{'loss': 3.3518, 'grad_norm': 0.419546514749527, 'learning_rate': 2.649230133710434e-06, 'epoch': 2.6}
{'loss': 2.7307, 'grad_norm': 0.6396788954734802, 'learning_rate': 2.635952992121826e-06, 'epoch': 2.6}
{'loss': 2.7307, 'grad_norm': 0.7818530797958374, 'learning_rate': 2.62270735282924e-06, 'epoch': 2.6}
{'loss': 2.7144, 'grad_norm': 1.0688273906707764, 'learning_rate': 2.609493234490673e-06, 'epoch': 2.61}
{'loss': 2.7908, 'grad_norm': 1.155074954032898, 'learning_rate': 2.5963106557197025e-06, 'epoch': 2.61}
{'loss': 2.7841, 'grad_norm': 1.455009937286377, 'learning_rate': 2.583159635085483e-06, 'epoch': 2.61}
{'loss': 3.2728, 'grad_norm': 1.7530956268310547, 'learning_rate': 2.5700401911127157e-06, 'epoch': 2.61}
{'loss': 3.4953, 'grad_norm': 1.8878669738769531, 'learning_rate': 2.556952342281635e-06, 'epoch': 2.61}
{'loss': 3.5307, 'grad_norm': 1.9181890487670898, 'learning_rate': 2.5438961070279515e-06, 'epoch': 2.61}
{'loss': 3.4784, 'grad_norm': 2.1896286010742188, 'learning_rate': 2.53087150374286e-06, 'epoch': 2.61}
{'loss': 3.432, 'grad_norm': 0.3891462981700897, 'learning_rate': 2.5178785507729815e-06, 'epoch': 2.61}
{'loss': 2.9312, 'grad_norm': 0.6304534673690796, 'learning_rate': 2.5049172664203783e-06, 'epoch': 2.61}
{'loss': 2.6749, 'grad_norm': 0.7412362098693848, 'learning_rate': 2.491987668942472e-06, 'epoch': 2.61}
{'loss': 2.7281, 'grad_norm': 0.8500754237174988, 'learning_rate': 2.4790897765520765e-06, 'epoch': 2.62}
{'loss': 2.7515, 'grad_norm': 1.0872185230255127, 'learning_rate': 2.466223607417331e-06, 'epoch': 2.62}
{'loss': 2.9339, 'grad_norm': 1.3151763677597046, 'learning_rate': 2.4533891796617025e-06, 'epoch': 2.62}
{'loss': 2.9437, 'grad_norm': 1.5962467193603516, 'learning_rate': 2.4405865113639263e-06, 'epoch': 2.62}
{'loss': 3.4486, 'grad_norm': 1.764910340309143, 'learning_rate': 2.427815620558027e-06, 'epoch': 2.62}
{'loss': 3.5249, 'grad_norm': 1.8560584783554077, 'learning_rate': 2.415076525233234e-06, 'epoch': 2.62}
{'loss': 3.3762, 'grad_norm': 2.278970718383789, 'learning_rate': 2.4023692433340185e-06, 'epoch': 2.62}
{'loss': 3.3674, 'grad_norm': 0.41972973942756653, 'learning_rate': 2.3896937927600254e-06, 'epoch': 2.62}
{'loss': 2.7371, 'grad_norm': 0.5930010080337524, 'learning_rate': 2.3770501913660677e-06, 'epoch': 2.62}
{'loss': 2.7331, 'grad_norm': 0.7138460874557495, 'learning_rate': 2.364438456962084e-06, 'epoch': 2.62}
{'loss': 2.6926, 'grad_norm': 0.8567675352096558, 'learning_rate': 2.35185860731314e-06, 'epoch': 2.63}
{'loss': 2.7494, 'grad_norm': 0.9996001720428467, 'learning_rate': 2.339310660139379e-06, 'epoch': 2.63}
{'loss': 2.8753, 'grad_norm': 1.3900010585784912, 'learning_rate': 2.3267946331160056e-06, 'epoch': 2.63}
{'loss': 3.0364, 'grad_norm': 1.6452341079711914, 'learning_rate': 2.3143105438732578e-06, 'epoch': 2.63}
{'loss': 3.531, 'grad_norm': 2.1495158672332764, 'learning_rate': 2.301858409996402e-06, 'epoch': 2.63}
{'loss': 3.5062, 'grad_norm': 1.9811875820159912, 'learning_rate': 2.2894382490256687e-06, 'epoch': 2.63}
{'loss': 3.5928, 'grad_norm': 2.3091938495635986, 'learning_rate': 2.2770500784562748e-06, 'epoch': 2.63}
{'loss': 3.504, 'grad_norm': 0.4102659821510315, 'learning_rate': 2.264693915738356e-06, 'epoch': 2.63}
{'loss': 2.6497, 'grad_norm': 0.6034186482429504, 'learning_rate': 2.2523697782769722e-06, 'epoch': 2.63}
{'loss': 2.7464, 'grad_norm': 0.8229964375495911, 'learning_rate': 2.2400776834320596e-06, 'epoch': 2.63}
{'loss': 2.679, 'grad_norm': 0.8223975896835327, 'learning_rate': 2.2278176485184406e-06, 'epoch': 2.64}
{'loss': 2.7898, 'grad_norm': 1.1076204776763916, 'learning_rate': 2.21558969080575e-06, 'epoch': 2.64}
{'loss': 2.7721, 'grad_norm': 1.3123419284820557, 'learning_rate': 2.2033938275184685e-06, 'epoch': 2.64}
{'loss': 3.0069, 'grad_norm': 1.8961681127548218, 'learning_rate': 2.19123007583584e-06, 'epoch': 2.64}
{'loss': 3.3305, 'grad_norm': 2.1226115226745605, 'learning_rate': 2.179098452891895e-06, 'epoch': 2.64}
{'loss': 3.3871, 'grad_norm': 2.0297696590423584, 'learning_rate': 2.1669989757754e-06, 'epoch': 2.64}
{'loss': 3.5625, 'grad_norm': 2.0764400959014893, 'learning_rate': 2.1549316615298355e-06, 'epoch': 2.64}
{'loss': 3.2704, 'grad_norm': 0.4505436420440674, 'learning_rate': 2.142896527153382e-06, 'epoch': 2.64}
{'loss': 2.7346, 'grad_norm': 0.6322351098060608, 'learning_rate': 2.1308935895988978e-06, 'epoch': 2.64}
{'loss': 2.5858, 'grad_norm': 0.7238617539405823, 'learning_rate': 2.118922865773873e-06, 'epoch': 2.64}
{'loss': 2.7258, 'grad_norm': 0.8383551836013794, 'learning_rate': 2.1069843725404383e-06, 'epoch': 2.65}
{'loss': 2.8597, 'grad_norm': 0.997917115688324, 'learning_rate': 2.095078126715308e-06, 'epoch': 2.65}
{'loss': 2.8949, 'grad_norm': 1.428162932395935, 'learning_rate': 2.083204145069778e-06, 'epoch': 2.65}
{'loss': 2.949, 'grad_norm': 1.5412038564682007, 'learning_rate': 2.071362444329708e-06, 'epoch': 2.65}
{'loss': 3.4683, 'grad_norm': 2.0519025325775146, 'learning_rate': 2.0595530411754633e-06, 'epoch': 2.65}
{'loss': 3.511, 'grad_norm': 2.2382137775421143, 'learning_rate': 2.047775952241937e-06, 'epoch': 2.65}
{'loss': 3.45, 'grad_norm': 2.0446574687957764, 'learning_rate': 2.0360311941184926e-06, 'epoch': 2.65}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0250167846679688, 'eval_runtime': 12.4772, 'eval_samples_per_second': 130.959, 'eval_steps_per_second': 16.43, 'epoch': 2.65}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.3921, 'grad_norm': 0.4113795757293701, 'learning_rate': 2.0243187833489525e-06, 'epoch': 2.65}
{'loss': 2.8561, 'grad_norm': 0.6239816546440125, 'learning_rate': 2.0126387364315712e-06, 'epoch': 2.65}
{'loss': 2.6835, 'grad_norm': 0.8094081878662109, 'learning_rate': 2.000991069819025e-06, 'epoch': 2.66}
{'loss': 2.5749, 'grad_norm': 0.8900561928749084, 'learning_rate': 1.9893757999183666e-06, 'epoch': 2.66}
{'loss': 2.8227, 'grad_norm': 1.0922905206680298, 'learning_rate': 1.977792943091031e-06, 'epoch': 2.66}
{'loss': 2.7654, 'grad_norm': 1.2946422100067139, 'learning_rate': 1.966242515652775e-06, 'epoch': 2.66}
{'loss': 3.0242, 'grad_norm': 1.67947256565094, 'learning_rate': 1.9547245338736946e-06, 'epoch': 2.66}
{'loss': 3.405, 'grad_norm': 1.9041258096694946, 'learning_rate': 1.9432390139781624e-06, 'epoch': 2.66}
{'loss': 3.3596, 'grad_norm': 2.0516576766967773, 'learning_rate': 1.931785972144845e-06, 'epoch': 2.66}
{'loss': 3.4691, 'grad_norm': 2.442112684249878, 'learning_rate': 1.920365424506643e-06, 'epoch': 2.66}
{'loss': 3.3917, 'grad_norm': 0.4256654679775238, 'learning_rate': 1.9089773871507012e-06, 'epoch': 2.66}
{'loss': 2.7848, 'grad_norm': 0.679050087928772, 'learning_rate': 1.8976218761183496e-06, 'epoch': 2.66}
{'loss': 2.6482, 'grad_norm': 0.7104932069778442, 'learning_rate': 1.8862989074051318e-06, 'epoch': 2.67}
{'loss': 2.6713, 'grad_norm': 0.8909152150154114, 'learning_rate': 1.8750084969607117e-06, 'epoch': 2.67}
{'loss': 2.7281, 'grad_norm': 1.0367578268051147, 'learning_rate': 1.863750660688926e-06, 'epoch': 2.67}
{'loss': 2.8788, 'grad_norm': 1.5160285234451294, 'learning_rate': 1.8525254144477062e-06, 'epoch': 2.67}
{'loss': 3.0344, 'grad_norm': 1.6274443864822388, 'learning_rate': 1.8413327740490927e-06, 'epoch': 2.67}
{'loss': 3.5038, 'grad_norm': 2.1865251064300537, 'learning_rate': 1.8301727552591796e-06, 'epoch': 2.67}
{'loss': 3.5503, 'grad_norm': 2.030513286590576, 'learning_rate': 1.8190453737981283e-06, 'epoch': 2.67}
{'loss': 3.5161, 'grad_norm': 1.9533133506774902, 'learning_rate': 1.8079506453401118e-06, 'epoch': 2.67}
{'loss': 3.3783, 'grad_norm': 0.41449660062789917, 'learning_rate': 1.7968885855133122e-06, 'epoch': 2.67}
{'loss': 2.7078, 'grad_norm': 0.6228586435317993, 'learning_rate': 1.7858592098998927e-06, 'epoch': 2.67}
{'loss': 2.6777, 'grad_norm': 0.7375922203063965, 'learning_rate': 1.7748625340359898e-06, 'epoch': 2.68}
{'loss': 2.6488, 'grad_norm': 0.9043416380882263, 'learning_rate': 1.763898573411657e-06, 'epoch': 2.68}
{'loss': 2.8312, 'grad_norm': 1.1445187330245972, 'learning_rate': 1.7529673434708821e-06, 'epoch': 2.68}
{'loss': 2.926, 'grad_norm': 1.372206449508667, 'learning_rate': 1.7420688596115426e-06, 'epoch': 2.68}
{'loss': 3.0929, 'grad_norm': 1.699818730354309, 'learning_rate': 1.7312031371853887e-06, 'epoch': 2.68}
{'loss': 3.5128, 'grad_norm': 2.12225079536438, 'learning_rate': 1.7203701914980135e-06, 'epoch': 2.68}
{'loss': 3.4475, 'grad_norm': 1.8485504388809204, 'learning_rate': 1.7095700378088637e-06, 'epoch': 2.68}
{'loss': 3.415, 'grad_norm': 2.1940696239471436, 'learning_rate': 1.6988026913311667e-06, 'epoch': 2.68}
{'loss': 3.2273, 'grad_norm': 0.4125578701496124, 'learning_rate': 1.6880681672319625e-06, 'epoch': 2.68}
{'loss': 2.8181, 'grad_norm': 0.640386700630188, 'learning_rate': 1.6773664806320337e-06, 'epoch': 2.68}
{'loss': 2.6153, 'grad_norm': 0.6937308311462402, 'learning_rate': 1.6666976466059358e-06, 'epoch': 2.69}
{'loss': 2.6631, 'grad_norm': 0.9107484221458435, 'learning_rate': 1.656061680181914e-06, 'epoch': 2.69}
{'loss': 2.7743, 'grad_norm': 1.0820326805114746, 'learning_rate': 1.645458596341945e-06, 'epoch': 2.69}
{'loss': 2.8134, 'grad_norm': 1.3755592107772827, 'learning_rate': 1.6348884100216649e-06, 'epoch': 2.69}
{'loss': 3.3147, 'grad_norm': 1.939042568206787, 'learning_rate': 1.6243511361103886e-06, 'epoch': 2.69}
{'loss': 3.3351, 'grad_norm': 2.0598344802856445, 'learning_rate': 1.6138467894510538e-06, 'epoch': 2.69}
{'loss': 3.5447, 'grad_norm': 2.1614503860473633, 'learning_rate': 1.6033753848402362e-06, 'epoch': 2.69}
{'loss': 3.4069, 'grad_norm': 2.4091973304748535, 'learning_rate': 1.5929369370280777e-06, 'epoch': 2.69}
{'loss': 3.3946, 'grad_norm': 0.39007920026779175, 'learning_rate': 1.5825314607183312e-06, 'epoch': 2.69}
{'loss': 2.8232, 'grad_norm': 0.6179215312004089, 'learning_rate': 1.57215897056828e-06, 'epoch': 2.69}
{'loss': 2.6922, 'grad_norm': 1.0716124773025513, 'learning_rate': 1.5618194811887616e-06, 'epoch': 2.7}
{'loss': 2.7138, 'grad_norm': 0.9885107278823853, 'learning_rate': 1.5515130071441093e-06, 'epoch': 2.7}
{'loss': 2.8576, 'grad_norm': 1.1486178636550903, 'learning_rate': 1.5412395629521708e-06, 'epoch': 2.7}
{'loss': 2.7101, 'grad_norm': 1.318761944770813, 'learning_rate': 1.5309991630842513e-06, 'epoch': 2.7}
{'loss': 3.1585, 'grad_norm': 1.6707439422607422, 'learning_rate': 1.5207918219651146e-06, 'epoch': 2.7}
{'loss': 3.3777, 'grad_norm': 1.9717192649841309, 'learning_rate': 1.5106175539729595e-06, 'epoch': 2.7}
{'loss': 3.5748, 'grad_norm': 2.159283399581909, 'learning_rate': 1.5004763734393967e-06, 'epoch': 2.7}
{'loss': 3.5298, 'grad_norm': 2.0979576110839844, 'learning_rate': 1.4903682946494275e-06, 'epoch': 2.7}
{'loss': 3.2885, 'grad_norm': 0.5252081155776978, 'learning_rate': 1.4802933318414286e-06, 'epoch': 2.7}
{'loss': 2.7866, 'grad_norm': 0.6487510800361633, 'learning_rate': 1.4702514992071259e-06, 'epoch': 2.71}
{'loss': 2.7653, 'grad_norm': 0.79007887840271, 'learning_rate': 1.4602428108915793e-06, 'epoch': 2.71}
{'loss': 2.8061, 'grad_norm': 0.8885322213172913, 'learning_rate': 1.4502672809931616e-06, 'epoch': 2.71}
{'loss': 2.6947, 'grad_norm': 1.136048674583435, 'learning_rate': 1.4403249235635379e-06, 'epoch': 2.71}
{'loss': 2.8715, 'grad_norm': 1.2745766639709473, 'learning_rate': 1.4304157526076473e-06, 'epoch': 2.71}
{'loss': 3.0709, 'grad_norm': 1.7971594333648682, 'learning_rate': 1.4205397820836798e-06, 'epoch': 2.71}
{'loss': 3.6115, 'grad_norm': 2.062687873840332, 'learning_rate': 1.41069702590306e-06, 'epoch': 2.71}
{'loss': 3.5921, 'grad_norm': 1.9189120531082153, 'learning_rate': 1.4008874979304226e-06, 'epoch': 2.71}
{'loss': 3.5133, 'grad_norm': 2.5311191082000732, 'learning_rate': 1.3911112119836062e-06, 'epoch': 2.71}
{'loss': 3.3862, 'grad_norm': 0.3862076699733734, 'learning_rate': 1.3813681818336089e-06, 'epoch': 2.71}
{'loss': 2.8031, 'grad_norm': 0.6433511972427368, 'learning_rate': 1.3716584212046052e-06, 'epoch': 2.72}
{'loss': 2.6277, 'grad_norm': 0.7873314023017883, 'learning_rate': 1.3619819437738824e-06, 'epoch': 2.72}
{'loss': 2.708, 'grad_norm': 0.9300251603126526, 'learning_rate': 1.3523387631718647e-06, 'epoch': 2.72}
{'loss': 2.6529, 'grad_norm': 1.1875211000442505, 'learning_rate': 1.3427288929820647e-06, 'epoch': 2.72}
{'loss': 2.825, 'grad_norm': 1.3699404001235962, 'learning_rate': 1.3331523467410705e-06, 'epoch': 2.72}
{'loss': 2.9734, 'grad_norm': 1.5733197927474976, 'learning_rate': 1.3236091379385306e-06, 'epoch': 2.72}
{'loss': 3.3656, 'grad_norm': 1.8311357498168945, 'learning_rate': 1.3140992800171448e-06, 'epoch': 2.72}
{'loss': 3.5341, 'grad_norm': 1.9306026697158813, 'learning_rate': 1.3046227863726196e-06, 'epoch': 2.72}
{'loss': 3.3013, 'grad_norm': 2.239922523498535, 'learning_rate': 1.2951796703536772e-06, 'epoch': 2.72}
{'loss': 3.3821, 'grad_norm': 0.39781779050827026, 'learning_rate': 1.2857699452620137e-06, 'epoch': 2.72}
{'loss': 2.7922, 'grad_norm': 0.6455236077308655, 'learning_rate': 1.2763936243522928e-06, 'epoch': 2.73}
{'loss': 2.6702, 'grad_norm': 0.6865489482879639, 'learning_rate': 1.2670507208321219e-06, 'epoch': 2.73}
{'loss': 2.6633, 'grad_norm': 0.8975198864936829, 'learning_rate': 1.2577412478620488e-06, 'epoch': 2.73}
{'loss': 2.7898, 'grad_norm': 1.1076126098632812, 'learning_rate': 1.2484652185555168e-06, 'epoch': 2.73}
{'loss': 2.8177, 'grad_norm': 1.2773561477661133, 'learning_rate': 1.2392226459788658e-06, 'epoch': 2.73}
{'loss': 2.8761, 'grad_norm': 1.5685620307922363, 'learning_rate': 1.2300135431513038e-06, 'epoch': 2.73}
{'loss': 3.3848, 'grad_norm': 1.9756488800048828, 'learning_rate': 1.2208379230449095e-06, 'epoch': 2.73}
{'loss': 3.5498, 'grad_norm': 2.156453847885132, 'learning_rate': 1.2116957985845666e-06, 'epoch': 2.73}
{'loss': 3.3831, 'grad_norm': 2.385702133178711, 'learning_rate': 1.2025871826480046e-06, 'epoch': 2.73}
{'loss': 3.4422, 'grad_norm': 0.3660002648830414, 'learning_rate': 1.1935120880657347e-06, 'epoch': 2.73}
{'loss': 2.7879, 'grad_norm': 0.6174371838569641, 'learning_rate': 1.1844705276210649e-06, 'epoch': 2.74}
{'loss': 2.6294, 'grad_norm': 0.7670345902442932, 'learning_rate': 1.175462514050052e-06, 'epoch': 2.74}
{'loss': 2.8094, 'grad_norm': 0.8590852618217468, 'learning_rate': 1.166488060041515e-06, 'epoch': 2.74}
{'loss': 2.6848, 'grad_norm': 1.0348857641220093, 'learning_rate': 1.1575471782369729e-06, 'epoch': 2.74}
{'loss': 2.8534, 'grad_norm': 1.260799527168274, 'learning_rate': 1.1486398812306842e-06, 'epoch': 2.74}
{'loss': 2.9711, 'grad_norm': 1.705040693283081, 'learning_rate': 1.1397661815695821e-06, 'epoch': 2.74}
{'loss': 3.434, 'grad_norm': 2.0009419918060303, 'learning_rate': 1.1309260917532816e-06, 'epoch': 2.74}
{'loss': 3.4322, 'grad_norm': 2.0233263969421387, 'learning_rate': 1.1221196242340444e-06, 'epoch': 2.74}
{'loss': 3.7024, 'grad_norm': 2.370905876159668, 'learning_rate': 1.1133467914167917e-06, 'epoch': 2.74}
{'loss': 3.2785, 'grad_norm': 0.4060119092464447, 'learning_rate': 1.1046076056590415e-06, 'epoch': 2.74}
{'loss': 2.8111, 'grad_norm': 0.6529538035392761, 'learning_rate': 1.095902079270933e-06, 'epoch': 2.75}
{'loss': 2.6598, 'grad_norm': 0.7490863800048828, 'learning_rate': 1.0872302245151816e-06, 'epoch': 2.75}
{'loss': 2.7158, 'grad_norm': 0.933667004108429, 'learning_rate': 1.0785920536070859e-06, 'epoch': 2.75}
{'loss': 2.8152, 'grad_norm': 1.4512470960617065, 'learning_rate': 1.0699875787144847e-06, 'epoch': 2.75}
{'loss': 2.9037, 'grad_norm': 1.5231802463531494, 'learning_rate': 1.0614168119577577e-06, 'epoch': 2.75}
{'loss': 2.9788, 'grad_norm': 1.6939046382904053, 'learning_rate': 1.0528797654098033e-06, 'epoch': 2.75}
{'loss': 3.4406, 'grad_norm': 1.9145309925079346, 'learning_rate': 1.0443764510960158e-06, 'epoch': 2.75}
{'loss': 3.4916, 'grad_norm': 2.0499045848846436, 'learning_rate': 1.0359068809942778e-06, 'epoch': 2.75}
{'loss': 3.4479, 'grad_norm': 2.232036828994751, 'learning_rate': 1.0274710670349436e-06, 'epoch': 2.75}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.026625156402588, 'eval_runtime': 12.61, 'eval_samples_per_second': 129.58, 'eval_steps_per_second': 16.257, 'epoch': 2.75}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.2454, 'grad_norm': 0.4087276756763458, 'learning_rate': 1.0190690211008103e-06, 'epoch': 2.76}
{'loss': 2.761, 'grad_norm': 0.6907650232315063, 'learning_rate': 1.0107007550271135e-06, 'epoch': 2.76}
{'loss': 2.7163, 'grad_norm': 0.8491553664207458, 'learning_rate': 1.0023662806015022e-06, 'epoch': 2.76}
{'loss': 2.7341, 'grad_norm': 0.9589468836784363, 'learning_rate': 9.940656095640293e-07, 'epoch': 2.76}
{'loss': 2.9049, 'grad_norm': 1.2047446966171265, 'learning_rate': 9.857987536071367e-07, 'epoch': 2.76}
{'loss': 2.7039, 'grad_norm': 1.3075252771377563, 'learning_rate': 9.775657243756203e-07, 'epoch': 2.76}
{'loss': 3.1793, 'grad_norm': 1.7472530603408813, 'learning_rate': 9.693665334666423e-07, 'epoch': 2.76}
{'loss': 3.3289, 'grad_norm': 1.8654053211212158, 'learning_rate': 9.612011924296893e-07, 'epoch': 2.76}
{'loss': 3.505, 'grad_norm': 2.488895893096924, 'learning_rate': 9.530697127665689e-07, 'epoch': 2.76}
{'loss': 3.4163, 'grad_norm': 2.2268242835998535, 'learning_rate': 9.449721059313965e-07, 'epoch': 2.76}
{'loss': 3.3166, 'grad_norm': 0.4426323175430298, 'learning_rate': 9.369083833305648e-07, 'epoch': 2.77}
{'loss': 2.6692, 'grad_norm': 0.7023659944534302, 'learning_rate': 9.288785563227404e-07, 'epoch': 2.77}
{'loss': 2.5495, 'grad_norm': 0.7628336548805237, 'learning_rate': 9.208826362188533e-07, 'epoch': 2.77}
{'loss': 2.7616, 'grad_norm': 1.094346284866333, 'learning_rate': 9.129206342820551e-07, 'epoch': 2.77}
{'loss': 2.8023, 'grad_norm': 1.1847327947616577, 'learning_rate': 9.049925617277327e-07, 'epoch': 2.77}
{'loss': 2.6905, 'grad_norm': 1.29604172706604, 'learning_rate': 8.970984297234753e-07, 'epoch': 2.77}
{'loss': 2.9305, 'grad_norm': 1.6506527662277222, 'learning_rate': 8.8923824938906e-07, 'epoch': 2.77}
{'loss': 3.3362, 'grad_norm': 1.9507440328598022, 'learning_rate': 8.814120317964414e-07, 'epoch': 2.77}
{'loss': 3.4858, 'grad_norm': 2.3123044967651367, 'learning_rate': 8.736197879697428e-07, 'epoch': 2.77}
{'loss': 3.3604, 'grad_norm': 2.2900984287261963, 'learning_rate': 8.658615288852117e-07, 'epoch': 2.77}
{'loss': 3.391, 'grad_norm': 0.4181267023086548, 'learning_rate': 8.581372654712427e-07, 'epoch': 2.78}
{'loss': 2.8241, 'grad_norm': 0.6030500531196594, 'learning_rate': 8.504470086083349e-07, 'epoch': 2.78}
{'loss': 2.7614, 'grad_norm': 0.8084372878074646, 'learning_rate': 8.427907691290926e-07, 'epoch': 2.78}
{'loss': 2.6882, 'grad_norm': 0.8753621578216553, 'learning_rate': 8.351685578181834e-07, 'epoch': 2.78}
{'loss': 2.8694, 'grad_norm': 1.2678459882736206, 'learning_rate': 8.275803854123714e-07, 'epoch': 2.78}
{'loss': 2.9457, 'grad_norm': 1.6306251287460327, 'learning_rate': 8.200262626004457e-07, 'epoch': 2.78}
{'loss': 3.0361, 'grad_norm': 1.750042200088501, 'learning_rate': 8.125062000232553e-07, 'epoch': 2.78}
{'loss': 3.4401, 'grad_norm': 1.817753791809082, 'learning_rate': 8.050202082736518e-07, 'epoch': 2.78}
{'loss': 3.4477, 'grad_norm': 1.927496314048767, 'learning_rate': 7.975682978965115e-07, 'epoch': 2.78}
{'loss': 3.5762, 'grad_norm': 2.702213764190674, 'learning_rate': 7.901504793886877e-07, 'epoch': 2.78}
{'loss': 3.4625, 'grad_norm': 0.38070228695869446, 'learning_rate': 7.827667631990249e-07, 'epoch': 2.79}
{'loss': 2.7171, 'grad_norm': 0.622275710105896, 'learning_rate': 7.754171597283199e-07, 'epoch': 2.79}
{'loss': 2.7261, 'grad_norm': 0.7173842191696167, 'learning_rate': 7.681016793293277e-07, 'epoch': 2.79}
{'loss': 2.6235, 'grad_norm': 0.8728833198547363, 'learning_rate': 7.608203323067248e-07, 'epoch': 2.79}
{'loss': 2.916, 'grad_norm': 1.7225340604782104, 'learning_rate': 7.535731289171261e-07, 'epoch': 2.79}
{'loss': 2.8873, 'grad_norm': 1.3560612201690674, 'learning_rate': 7.46360079369024e-07, 'epoch': 2.79}
{'loss': 3.1575, 'grad_norm': 1.5141303539276123, 'learning_rate': 7.391811938228271e-07, 'epoch': 2.79}
{'loss': 3.5713, 'grad_norm': 2.377169609069824, 'learning_rate': 7.320364823908049e-07, 'epoch': 2.79}
{'loss': 3.2145, 'grad_norm': 2.111274480819702, 'learning_rate': 7.249259551370985e-07, 'epoch': 2.79}
{'loss': 3.4739, 'grad_norm': 2.058133363723755, 'learning_rate': 7.178496220776848e-07, 'epoch': 2.79}
{'loss': 3.4051, 'grad_norm': 0.49462345242500305, 'learning_rate': 7.108074931803876e-07, 'epoch': 2.8}
{'loss': 2.6624, 'grad_norm': 0.6384592652320862, 'learning_rate': 7.037995783648415e-07, 'epoch': 2.8}
{'loss': 2.6919, 'grad_norm': 0.7471908926963806, 'learning_rate': 6.968258875024891e-07, 'epoch': 2.8}
{'loss': 2.8171, 'grad_norm': 1.0016552209854126, 'learning_rate': 6.898864304165642e-07, 'epoch': 2.8}
{'loss': 2.7386, 'grad_norm': 1.1788536310195923, 'learning_rate': 6.82981216882081e-07, 'epoch': 2.8}
{'loss': 2.8999, 'grad_norm': 1.6427934169769287, 'learning_rate': 6.761102566258115e-07, 'epoch': 2.8}
{'loss': 3.0661, 'grad_norm': 1.7234103679656982, 'learning_rate': 6.692735593262861e-07, 'epoch': 2.8}
{'loss': 3.4648, 'grad_norm': 2.0581252574920654, 'learning_rate': 6.624711346137679e-07, 'epoch': 2.8}
{'loss': 3.4804, 'grad_norm': 2.1504478454589844, 'learning_rate': 6.557029920702451e-07, 'epoch': 2.8}
{'loss': 3.3867, 'grad_norm': 2.2478575706481934, 'learning_rate': 6.48969141229408e-07, 'epoch': 2.81}
{'loss': 3.2973, 'grad_norm': 0.3910038471221924, 'learning_rate': 6.422695915766524e-07, 'epoch': 2.81}
{'loss': 2.8142, 'grad_norm': 0.6143030524253845, 'learning_rate': 6.356043525490546e-07, 'epoch': 2.81}
{'loss': 2.6092, 'grad_norm': 1.2625350952148438, 'learning_rate': 6.289734335353598e-07, 'epoch': 2.81}
{'loss': 2.7875, 'grad_norm': 1.0595507621765137, 'learning_rate': 6.223768438759631e-07, 'epoch': 2.81}
{'loss': 2.7278, 'grad_norm': 1.0908867120742798, 'learning_rate': 6.158145928629205e-07, 'epoch': 2.81}
{'loss': 2.7728, 'grad_norm': 1.4196760654449463, 'learning_rate': 6.092866897398991e-07, 'epoch': 2.81}
{'loss': 3.0572, 'grad_norm': 1.5788767337799072, 'learning_rate': 6.027931437021905e-07, 'epoch': 2.81}
{'loss': 3.5254, 'grad_norm': 2.109649658203125, 'learning_rate': 5.963339638966892e-07, 'epoch': 2.81}
{'loss': 3.5608, 'grad_norm': 2.450901746749878, 'learning_rate': 5.899091594218892e-07, 'epoch': 2.81}
{'loss': 3.4851, 'grad_norm': 2.0637950897216797, 'learning_rate': 5.835187393278486e-07, 'epoch': 2.82}
{'loss': 3.3953, 'grad_norm': 0.40930137038230896, 'learning_rate': 5.771627126162027e-07, 'epoch': 2.82}
{'loss': 2.7881, 'grad_norm': 2.476517677307129, 'learning_rate': 5.708410882401372e-07, 'epoch': 2.82}
{'loss': 2.7426, 'grad_norm': 0.766137957572937, 'learning_rate': 5.645538751043733e-07, 'epoch': 2.82}
{'loss': 2.6624, 'grad_norm': 1.0136743783950806, 'learning_rate': 5.583010820651658e-07, 'epoch': 2.82}
{'loss': 2.8067, 'grad_norm': 1.1371526718139648, 'learning_rate': 5.520827179302802e-07, 'epoch': 2.82}
{'loss': 2.8047, 'grad_norm': 1.4626950025558472, 'learning_rate': 5.458987914589902e-07, 'epoch': 2.82}
{'loss': 2.9868, 'grad_norm': 1.6126047372817993, 'learning_rate': 5.397493113620561e-07, 'epoch': 2.82}
{'loss': 3.5116, 'grad_norm': 2.0390665531158447, 'learning_rate': 5.336342863017208e-07, 'epoch': 2.82}
{'loss': 3.4913, 'grad_norm': 2.149040699005127, 'learning_rate': 5.275537248916829e-07, 'epoch': 2.82}
{'loss': 3.5876, 'grad_norm': 2.3434674739837646, 'learning_rate': 5.215076356971105e-07, 'epoch': 2.83}
{'loss': 3.3995, 'grad_norm': 0.457306832075119, 'learning_rate': 5.154960272346022e-07, 'epoch': 2.83}
{'loss': 2.8729, 'grad_norm': 0.6204109191894531, 'learning_rate': 5.095189079721924e-07, 'epoch': 2.83}
{'loss': 2.6841, 'grad_norm': 0.7432669401168823, 'learning_rate': 5.035762863293269e-07, 'epoch': 2.83}
{'loss': 2.7143, 'grad_norm': 0.8661308288574219, 'learning_rate': 4.976681706768682e-07, 'epoch': 2.83}
{'loss': 2.7727, 'grad_norm': 1.1374112367630005, 'learning_rate': 4.917945693370618e-07, 'epoch': 2.83}
{'loss': 2.837, 'grad_norm': 1.3888198137283325, 'learning_rate': 4.859554905835423e-07, 'epoch': 2.83}
{'loss': 3.0911, 'grad_norm': 1.8050601482391357, 'learning_rate': 4.801509426413165e-07, 'epoch': 2.83}
{'loss': 3.3809, 'grad_norm': 1.9008327722549438, 'learning_rate': 4.743809336867466e-07, 'epoch': 2.83}
{'loss': 3.617, 'grad_norm': 2.3385415077209473, 'learning_rate': 4.686454718475453e-07, 'epoch': 2.83}
{'loss': 3.3911, 'grad_norm': 2.195796489715576, 'learning_rate': 4.629445652027636e-07, 'epoch': 2.84}
{'loss': 3.4291, 'grad_norm': 0.40208718180656433, 'learning_rate': 4.57278221782767e-07, 'epoch': 2.84}
{'loss': 2.7763, 'grad_norm': 0.6195453405380249, 'learning_rate': 4.516464495692485e-07, 'epoch': 2.84}
{'loss': 2.6288, 'grad_norm': 0.7521632313728333, 'learning_rate': 4.460492564951957e-07, 'epoch': 2.84}
{'loss': 2.7163, 'grad_norm': 0.9141581058502197, 'learning_rate': 4.4048665044489343e-07, 'epoch': 2.84}
{'loss': 2.7927, 'grad_norm': 1.152574062347412, 'learning_rate': 4.3495863925389904e-07, 'epoch': 2.84}
{'loss': 2.8823, 'grad_norm': 2.3622000217437744, 'learning_rate': 4.2946523070904466e-07, 'epoch': 2.84}
{'loss': 3.1307, 'grad_norm': 1.8627873659133911, 'learning_rate': 4.240064325484211e-07, 'epoch': 2.84}
{'loss': 3.6108, 'grad_norm': 2.009606122970581, 'learning_rate': 4.1858225246136084e-07, 'epoch': 2.84}
{'loss': 3.5172, 'grad_norm': 2.3769943714141846, 'learning_rate': 4.131926980884382e-07, 'epoch': 2.84}
{'loss': 3.5337, 'grad_norm': 2.163273572921753, 'learning_rate': 4.0783777702144975e-07, 'epoch': 2.85}
{'loss': 3.445, 'grad_norm': 0.3907628059387207, 'learning_rate': 4.0251749680341176e-07, 'epoch': 2.85}
{'loss': 2.7824, 'grad_norm': 0.620439887046814, 'learning_rate': 3.9723186492854335e-07, 'epoch': 2.85}
{'loss': 2.7598, 'grad_norm': 0.7677792906761169, 'learning_rate': 3.919808888422527e-07, 'epoch': 2.85}
{'loss': 2.7297, 'grad_norm': 0.9556735754013062, 'learning_rate': 3.86764575941137e-07, 'epoch': 2.85}
{'loss': 2.7278, 'grad_norm': 1.0983835458755493, 'learning_rate': 3.8158293357296316e-07, 'epoch': 2.85}
{'loss': 2.8174, 'grad_norm': 1.5608011484146118, 'learning_rate': 3.76435969036662e-07, 'epoch': 2.85}
{'loss': 2.8963, 'grad_norm': 1.6719573736190796, 'learning_rate': 3.713236895823174e-07, 'epoch': 2.85}
{'loss': 3.5358, 'grad_norm': 1.9506971836090088, 'learning_rate': 3.66246102411158e-07, 'epoch': 2.85}
{'loss': 3.4766, 'grad_norm': 2.106975555419922, 'learning_rate': 3.6120321467553455e-07, 'epoch': 2.86}
{'loss': 3.3981, 'grad_norm': 2.260268211364746, 'learning_rate': 3.5619503347893155e-07, 'epoch': 2.86}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0259995460510254, 'eval_runtime': 12.5979, 'eval_samples_per_second': 129.705, 'eval_steps_per_second': 16.273, 'epoch': 2.86}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.1978, 'grad_norm': 0.5972654223442078, 'learning_rate': 3.512215658759366e-07, 'epoch': 2.86}
{'loss': 2.7464, 'grad_norm': 0.809242308139801, 'learning_rate': 3.462828188722456e-07, 'epoch': 2.86}
{'loss': 2.7128, 'grad_norm': 0.7314842343330383, 'learning_rate': 3.413787994246409e-07, 'epoch': 2.86}
{'loss': 2.679, 'grad_norm': 0.8556939959526062, 'learning_rate': 3.365095144409913e-07, 'epoch': 2.86}
{'loss': 2.753, 'grad_norm': 1.1313424110412598, 'learning_rate': 3.3167497078023803e-07, 'epoch': 2.86}
{'loss': 2.7653, 'grad_norm': 1.4103071689605713, 'learning_rate': 3.268751752523835e-07, 'epoch': 2.86}
{'loss': 2.9002, 'grad_norm': 1.710875391960144, 'learning_rate': 3.221101346184807e-07, 'epoch': 2.86}
{'loss': 3.6347, 'grad_norm': 1.9210461378097534, 'learning_rate': 3.1737985559063264e-07, 'epoch': 2.86}
{'loss': 3.4922, 'grad_norm': 2.2704319953918457, 'learning_rate': 3.126843448319733e-07, 'epoch': 2.87}
{'loss': 3.4443, 'grad_norm': 2.054180383682251, 'learning_rate': 3.080236089566646e-07, 'epoch': 2.87}
{'loss': 3.2658, 'grad_norm': 0.38789454102516174, 'learning_rate': 3.033976545298745e-07, 'epoch': 2.87}
{'loss': 2.8641, 'grad_norm': 0.6680648326873779, 'learning_rate': 2.988064880677932e-07, 'epoch': 2.87}
{'loss': 2.6784, 'grad_norm': 0.7747741937637329, 'learning_rate': 2.942501160375921e-07, 'epoch': 2.87}
{'loss': 2.7176, 'grad_norm': 0.8178638219833374, 'learning_rate': 2.897285448574427e-07, 'epoch': 2.87}
{'loss': 2.6815, 'grad_norm': 1.1175150871276855, 'learning_rate': 2.8524178089648355e-07, 'epoch': 2.87}
{'loss': 2.9724, 'grad_norm': 1.4012951850891113, 'learning_rate': 2.8078983047483966e-07, 'epoch': 2.87}
{'loss': 3.2697, 'grad_norm': 1.8208502531051636, 'learning_rate': 2.7637269986358347e-07, 'epoch': 2.87}
{'loss': 3.3705, 'grad_norm': 2.1469151973724365, 'learning_rate': 2.7199039528474356e-07, 'epoch': 2.87}
{'loss': 3.4366, 'grad_norm': 2.2177438735961914, 'learning_rate': 2.676429229112959e-07, 'epoch': 2.88}
{'loss': 3.3971, 'grad_norm': 2.1744918823242188, 'learning_rate': 2.6333028886714183e-07, 'epoch': 2.88}
{'loss': 3.3079, 'grad_norm': 0.4471186101436615, 'learning_rate': 2.5905249922712203e-07, 'epoch': 2.88}
{'loss': 2.8268, 'grad_norm': 0.6186230182647705, 'learning_rate': 2.5480956001698585e-07, 'epoch': 2.88}
{'loss': 2.739, 'grad_norm': 0.7516307830810547, 'learning_rate': 2.5060147721339687e-07, 'epoch': 2.88}
{'loss': 2.688, 'grad_norm': 0.9441301226615906, 'learning_rate': 2.464282567439108e-07, 'epoch': 2.88}
{'loss': 2.6631, 'grad_norm': 1.0131151676177979, 'learning_rate': 2.422899044869892e-07, 'epoch': 2.88}
{'loss': 2.894, 'grad_norm': 1.4062684774398804, 'learning_rate': 2.3818642627196354e-07, 'epoch': 2.88}
{'loss': 3.0192, 'grad_norm': 1.9962183237075806, 'learning_rate': 2.3411782787905179e-07, 'epoch': 2.88}
{'loss': 3.4058, 'grad_norm': 1.7086368799209595, 'learning_rate': 2.3008411503933624e-07, 'epoch': 2.88}
{'loss': 3.4437, 'grad_norm': 1.974130630493164, 'learning_rate': 2.2608529343475793e-07, 'epoch': 2.89}
{'loss': 3.4395, 'grad_norm': 2.039252996444702, 'learning_rate': 2.2212136869811107e-07, 'epoch': 2.89}
{'loss': 3.3221, 'grad_norm': 0.40233585238456726, 'learning_rate': 2.18192346413032e-07, 'epoch': 2.89}
{'loss': 2.75, 'grad_norm': 0.6301714777946472, 'learning_rate': 2.1429823211399637e-07, 'epoch': 2.89}
{'loss': 2.6412, 'grad_norm': 0.7336150407791138, 'learning_rate': 2.1043903128630526e-07, 'epoch': 2.89}
{'loss': 2.626, 'grad_norm': 2.5241596698760986, 'learning_rate': 2.0661474936607694e-07, 'epoch': 2.89}
{'loss': 2.7517, 'grad_norm': 1.0182329416275024, 'learning_rate': 2.0282539174025228e-07, 'epoch': 2.89}
{'loss': 2.9655, 'grad_norm': 1.5128722190856934, 'learning_rate': 1.990709637465643e-07, 'epoch': 2.89}
{'loss': 3.139, 'grad_norm': 2.3535208702087402, 'learning_rate': 1.9535147067355485e-07, 'epoch': 2.89}
{'loss': 3.4318, 'grad_norm': 1.898108959197998, 'learning_rate': 1.9166691776055235e-07, 'epoch': 2.89}
{'loss': 3.5118, 'grad_norm': 2.0391361713409424, 'learning_rate': 1.8801731019766622e-07, 'epoch': 2.9}
{'loss': 3.5009, 'grad_norm': 2.323301076889038, 'learning_rate': 1.8440265312577865e-07, 'epoch': 2.9}
{'loss': 3.2845, 'grad_norm': 0.43105775117874146, 'learning_rate': 1.8082295163655006e-07, 'epoch': 2.9}
{'loss': 2.7118, 'grad_norm': 0.6320093274116516, 'learning_rate': 1.7727821077239137e-07, 'epoch': 2.9}
{'loss': 2.71, 'grad_norm': 0.7611609101295471, 'learning_rate': 1.7376843552647792e-07, 'epoch': 2.9}
{'loss': 2.7721, 'grad_norm': 0.8143300414085388, 'learning_rate': 1.7029363084271889e-07, 'epoch': 2.9}
{'loss': 2.77, 'grad_norm': 1.1731213331222534, 'learning_rate': 1.668538016157739e-07, 'epoch': 2.9}
{'loss': 2.7972, 'grad_norm': 1.3030800819396973, 'learning_rate': 1.6344895269103379e-07, 'epoch': 2.9}
{'loss': 3.1172, 'grad_norm': 1.8798468112945557, 'learning_rate': 1.60079088864612e-07, 'epoch': 2.9}
{'loss': 3.4472, 'grad_norm': 2.318138360977173, 'learning_rate': 1.567442148833448e-07, 'epoch': 2.91}
{'loss': 3.6183, 'grad_norm': 1.8698995113372803, 'learning_rate': 1.5344433544478288e-07, 'epoch': 2.91}
{'loss': 3.5718, 'grad_norm': 2.4026739597320557, 'learning_rate': 1.5017945519717468e-07, 'epoch': 2.91}
{'loss': 3.3852, 'grad_norm': 0.38540416955947876, 'learning_rate': 1.4694957873948034e-07, 'epoch': 2.91}
{'loss': 2.8246, 'grad_norm': 0.6070020794868469, 'learning_rate': 1.4375471062134384e-07, 'epoch': 2.91}
{'loss': 2.6739, 'grad_norm': 1.2094287872314453, 'learning_rate': 1.4059485534309858e-07, 'epoch': 2.91}
{'loss': 2.7868, 'grad_norm': 0.9269360899925232, 'learning_rate': 1.3747001735576192e-07, 'epoch': 2.91}
{'loss': 2.8651, 'grad_norm': 5.812455177307129, 'learning_rate': 1.3438020106101846e-07, 'epoch': 2.91}
{'loss': 2.9334, 'grad_norm': 1.4810019731521606, 'learning_rate': 1.3132541081122828e-07, 'epoch': 2.91}
{'loss': 3.1852, 'grad_norm': 1.7869772911071777, 'learning_rate': 1.283056509094105e-07, 'epoch': 2.91}
{'loss': 3.4642, 'grad_norm': 2.122741460800171, 'learning_rate': 1.253209256092347e-07, 'epoch': 2.92}
{'loss': 3.4923, 'grad_norm': 2.252810001373291, 'learning_rate': 1.2237123911502945e-07, 'epoch': 2.92}
{'loss': 3.4262, 'grad_norm': 2.1017839908599854, 'learning_rate': 1.1945659558175726e-07, 'epoch': 2.92}
{'loss': 3.3776, 'grad_norm': 0.4132927656173706, 'learning_rate': 1.1657699911503117e-07, 'epoch': 2.92}
{'loss': 2.7284, 'grad_norm': 0.6455089449882507, 'learning_rate': 1.1373245377108432e-07, 'epoch': 2.92}
{'loss': 2.6962, 'grad_norm': 0.7739376425743103, 'learning_rate': 1.1092296355678377e-07, 'epoch': 2.92}
{'loss': 2.8968, 'grad_norm': 0.9099778532981873, 'learning_rate': 1.0814853242961387e-07, 'epoch': 2.92}
{'loss': 2.7848, 'grad_norm': 1.2668625116348267, 'learning_rate': 1.0540916429767622e-07, 'epoch': 2.92}
{'loss': 2.7964, 'grad_norm': 1.3489882946014404, 'learning_rate': 1.0270486301967864e-07, 'epoch': 2.92}
{'loss': 3.0511, 'grad_norm': 1.7640496492385864, 'learning_rate': 1.0003563240494063e-07, 'epoch': 2.92}
{'loss': 3.5146, 'grad_norm': 1.992382287979126, 'learning_rate': 9.740147621337404e-08, 'epoch': 2.93}
{'loss': 3.6333, 'grad_norm': 2.072512626647949, 'learning_rate': 9.480239815548575e-08, 'epoch': 2.93}
{'loss': 3.5372, 'grad_norm': 2.1935815811157227, 'learning_rate': 9.22384018923722e-08, 'epoch': 2.93}
{'loss': 3.4388, 'grad_norm': 0.4040200412273407, 'learning_rate': 8.97094910357138e-08, 'epoch': 2.93}
{'loss': 2.783, 'grad_norm': 1.5204923152923584, 'learning_rate': 8.721566914776657e-08, 'epoch': 2.93}
{'loss': 2.6897, 'grad_norm': 0.8086034059524536, 'learning_rate': 8.4756939741365e-08, 'epoch': 2.93}
{'loss': 2.6213, 'grad_norm': 0.9470457434654236, 'learning_rate': 8.233330627990254e-08, 'epoch': 2.93}
{'loss': 2.7758, 'grad_norm': 1.2958945035934448, 'learning_rate': 7.99447721773483e-08, 'epoch': 2.93}
{'loss': 2.8163, 'grad_norm': 1.416689157485962, 'learning_rate': 7.75913407982165e-08, 'epoch': 2.93}
{'loss': 3.0575, 'grad_norm': 1.699905514717102, 'learning_rate': 7.52730154575887e-08, 'epoch': 2.93}
{'loss': 3.6154, 'grad_norm': 2.1879425048828125, 'learning_rate': 7.298979942108319e-08, 'epoch': 2.94}
{'loss': 3.5997, 'grad_norm': 2.1177642345428467, 'learning_rate': 7.074169590487179e-08, 'epoch': 2.94}
{'loss': 3.3633, 'grad_norm': 1.9578770399093628, 'learning_rate': 6.85287080756658e-08, 'epoch': 2.94}
{'loss': 3.2924, 'grad_norm': 0.4096378684043884, 'learning_rate': 6.635083905070505e-08, 'epoch': 2.94}
{'loss': 2.7679, 'grad_norm': 0.606465756893158, 'learning_rate': 6.42080918977661e-08, 'epoch': 2.94}
{'loss': 2.6553, 'grad_norm': 0.7389510273933411, 'learning_rate': 6.210046963515126e-08, 'epoch': 2.94}
{'loss': 2.7441, 'grad_norm': 0.9474761486053467, 'learning_rate': 6.002797523168568e-08, 'epoch': 2.94}
{'loss': 2.7017, 'grad_norm': 1.3181962966918945, 'learning_rate': 5.799061160671471e-08, 'epoch': 2.94}
{'loss': 2.7179, 'grad_norm': 1.4579428434371948, 'learning_rate': 5.598838163009268e-08, 'epoch': 2.94}
{'loss': 3.0075, 'grad_norm': 1.5335994958877563, 'learning_rate': 5.402128812218854e-08, 'epoch': 2.94}
{'loss': 3.3199, 'grad_norm': 2.1444339752197266, 'learning_rate': 5.2089333853877484e-08, 'epoch': 2.95}
{'loss': 3.4681, 'grad_norm': 2.095348834991455, 'learning_rate': 5.0192521546538197e-08, 'epoch': 2.95}
{'loss': 3.4172, 'grad_norm': 2.065207004547119, 'learning_rate': 4.833085387204173e-08, 'epoch': 2.95}
{'loss': 3.268, 'grad_norm': 0.4159283936023712, 'learning_rate': 4.6504333452759844e-08, 'epoch': 2.95}
{'loss': 2.8129, 'grad_norm': 0.6454635262489319, 'learning_rate': 4.471296286155391e-08, 'epoch': 2.95}
{'loss': 2.6901, 'grad_norm': 0.7332189083099365, 'learning_rate': 4.295674462176935e-08, 'epoch': 2.95}
{'loss': 2.6913, 'grad_norm': 1.0031925439834595, 'learning_rate': 4.123568120724397e-08, 'epoch': 2.95}
{'loss': 2.72, 'grad_norm': 1.1303677558898926, 'learning_rate': 3.9549775042288515e-08, 'epoch': 2.95}
{'loss': 2.9057, 'grad_norm': 1.752272129058838, 'learning_rate': 3.7899028501695024e-08, 'epoch': 2.95}
{'loss': 3.0222, 'grad_norm': 1.6278636455535889, 'learning_rate': 3.6283443910722917e-08, 'epoch': 2.96}
{'loss': 3.5186, 'grad_norm': 2.0357015132904053, 'learning_rate': 3.470302354510735e-08, 'epoch': 2.96}
{'loss': 3.4658, 'grad_norm': 2.0589709281921387, 'learning_rate': 3.315776963105088e-08, 'epoch': 2.96}
{'loss': 3.313, 'grad_norm': 2.089790105819702, 'learning_rate': 3.16476843452207e-08, 'epoch': 2.96}
  warnings.warn(                                                                                                                                                       
{'eval_loss': 3.0248377323150635, 'eval_runtime': 12.5846, 'eval_samples_per_second': 129.841, 'eval_steps_per_second': 16.29, 'epoch': 2.96}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
                                                                                                                                                                       
{'loss': 3.5075, 'grad_norm': 0.42198118567466736, 'learning_rate': 3.017276981473749e-08, 'epoch': 2.96}
{'loss': 2.8335, 'grad_norm': 0.6511578559875488, 'learning_rate': 2.873302811719214e-08, 'epoch': 2.96}
{'loss': 2.6675, 'grad_norm': 0.813700258731842, 'learning_rate': 2.732846128062072e-08, 'epoch': 2.96}
{'loss': 2.7776, 'grad_norm': 1.0120428800582886, 'learning_rate': 2.5959071283512827e-08, 'epoch': 2.96}
{'loss': 2.8082, 'grad_norm': 1.3101094961166382, 'learning_rate': 2.4624860054814346e-08, 'epoch': 2.96}
{'loss': 2.8234, 'grad_norm': 1.6498945951461792, 'learning_rate': 2.332582947391082e-08, 'epoch': 2.96}
{'loss': 3.2014, 'grad_norm': 1.831413984298706, 'learning_rate': 2.2061981370632977e-08, 'epoch': 2.97}
{'loss': 3.5242, 'grad_norm': 2.026432752609253, 'learning_rate': 2.0833317525256745e-08, 'epoch': 2.97}
{'loss': 3.5362, 'grad_norm': 2.058422327041626, 'learning_rate': 1.9639839668489368e-08, 'epoch': 2.97}
{'loss': 3.478, 'grad_norm': 2.225541830062866, 'learning_rate': 1.848154948148606e-08, 'epoch': 2.97}
{'loss': 3.2817, 'grad_norm': 0.4253772497177124, 'learning_rate': 1.7358448595827803e-08, 'epoch': 2.97}
{'loss': 2.7063, 'grad_norm': 0.5967633128166199, 'learning_rate': 1.6270538593526897e-08, 'epoch': 2.97}
{'loss': 2.6784, 'grad_norm': 0.7366650104522705, 'learning_rate': 1.5217821007029732e-08, 'epoch': 2.97}
{'loss': 2.5895, 'grad_norm': 0.9039450883865356, 'learning_rate': 1.4200297319211242e-08, 'epoch': 2.97}
{'loss': 2.6575, 'grad_norm': 1.1902492046356201, 'learning_rate': 1.3217968963363803e-08, 'epoch': 2.97}
{'loss': 2.9017, 'grad_norm': 1.3796974420547485, 'learning_rate': 1.2270837323211104e-08, 'epoch': 2.97}
{'loss': 3.0213, 'grad_norm': 1.9276220798492432, 'learning_rate': 1.1358903732897052e-08, 'epoch': 2.98}
{'loss': 3.6292, 'grad_norm': 2.28999662399292, 'learning_rate': 1.0482169476980219e-08, 'epoch': 2.98}
{'loss': 3.5447, 'grad_norm': 2.007678508758545, 'learning_rate': 9.64063579043939e-09, 'epoch': 2.98}
{'loss': 3.5104, 'grad_norm': nan, 'learning_rate': 8.834303858673564e-09, 'epoch': 2.98}
{'loss': 3.4051, 'grad_norm': 0.41024354100227356, 'learning_rate': 8.214584341628696e-09, 'epoch': 2.98}
{'loss': 2.8401, 'grad_norm': 0.6519701480865479, 'learning_rate': 7.471618397375534e-09, 'epoch': 2.98}
{'loss': 2.7882, 'grad_norm': 0.8051152229309082, 'learning_rate': 6.7638572632033885e-09, 'epoch': 2.98}
{'loss': 2.6712, 'grad_norm': 0.9379277229309082, 'learning_rate': 6.091301936070326e-09, 'epoch': 2.98}
{'loss': 2.6756, 'grad_norm': 1.0263670682907104, 'learning_rate': 5.45395336335186e-09, 'epoch': 2.98}
{'loss': 2.7121, 'grad_norm': 1.252500057220459, 'learning_rate': 4.851812442827064e-09, 'epoch': 2.98}
{'loss': 2.8278, 'grad_norm': 1.522854208946228, 'learning_rate': 4.284880022673021e-09, 'epoch': 2.99}
{'loss': 3.3326, 'grad_norm': 1.9554133415222168, 'learning_rate': 3.753156901484256e-09, 'epoch': 2.99}
{'loss': 3.587, 'grad_norm': 2.218583822250366, 'learning_rate': 3.256643828247752e-09, 'epoch': 2.99}
{'loss': 3.5229, 'grad_norm': 2.4688186645507812, 'learning_rate': 2.7953415023651563e-09, 'epoch': 2.99}
{'loss': 3.4617, 'grad_norm': 0.4414197504520416, 'learning_rate': 2.3692505736278013e-09, 'epoch': 2.99}
{'loss': 2.7182, 'grad_norm': 0.5794845819473267, 'learning_rate': 1.9783716422333566e-09, 'epoch': 2.99}
{'loss': 2.7117, 'grad_norm': 0.6582310199737549, 'learning_rate': 1.6227052587830528e-09, 'epoch': 2.99}
{'loss': 2.7103, 'grad_norm': 3.022827386856079, 'learning_rate': 1.3022519242678055e-09, 'epoch': 2.99}
{'loss': 2.6496, 'grad_norm': 1.0207817554473877, 'learning_rate': 1.0170120900876435e-09, 'epoch': 2.99}
{'loss': 2.9211, 'grad_norm': 1.3766545057296753, 'learning_rate': 7.66986158032279e-10, 'epoch': 2.99}
{'loss': 2.8981, 'grad_norm': 1.6345382928848267, 'learning_rate': 5.521744802949868e-10, 'epoch': 3.0}
{'loss': 3.3412, 'grad_norm': 2.3453593254089355, 'learning_rate': 3.7257735945595007e-10, 'epoch': 3.0}
{'loss': 3.4228, 'grad_norm': 1.9113271236419678, 'learning_rate': 2.2819504850446482e-10, 'epoch': 3.0}
{'loss': 3.4483, 'grad_norm': 1.9455238580703735, 'learning_rate': 1.1902775081396034e-10, 'epoch': 3.0}
{'loss': 3.5524, 'grad_norm': 0.7167147397994995, 'learning_rate': 4.50756201642033e-11, 'epoch': 3.0}
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14706/14706 [6:22:12<00:00,  1.56s/it]
{'train_runtime': 22932.8971, 'train_samples_per_second': 10.259, 'train_steps_per_second': 0.641, 'train_loss': 4.748793095240832, 'epoch': 3.0}
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 205/205 [00:12<00:00, 16.68it/s]
[FINAL] eval_loss=3.0258 | ppl=20.61
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
/mnt/dados/gpt2_chatbot/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
