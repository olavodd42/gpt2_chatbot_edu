# Chatbot Educacional com GPT-2 + LoRA + 8-bit

Fine-tuning do GPTâ€‘2 (com LoRA + 8â€‘bit via bitsandbytes) para diÃ¡logo educacional, usando transformers, datasets, peft e accelerate.

- **LoRA** (Low-Rank Adaptation) para treinamento eficiente
- **QuantizaÃ§Ã£o em 8 bits** via **BitsAndBytes**
- **Hugging Face Transformers/Datasets**
- **W&B** (Weights & Biases) para logging de mÃ©tricas e amostras
- Suporte a datasets de **perguntas e respostas** no formato de **diÃ¡logo** (`<|user|>` e `<|assistant|>`)

O objetivo Ã© treinar um modelo rÃ¡pido, leve e personalizÃ¡vel para uso em ambientes educacionais e pesquisa.

LicenÃ§a: ver [LICENSE.md](LICENSE.md)

---

## SumÃ¡rio

- [ğŸ“‚ Estrutura do Projeto](#-estrutura-do-projeto)
- [âš™ï¸ Funcionalidades](#ï¸-funcionalidades)
- [ğŸ“¦ InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [ğŸ“Š Estrutura de ConfiguraÃ§Ã£o](#-estrutura-de-configuraÃ§Ã£o-train_smallyaml)
- [ğŸš€ Fluxo de ExecuÃ§Ã£o](#-fluxo-de-execuÃ§Ã£o)
- [ğŸ§ª Estrutura de TokenizaÃ§Ã£o](#-estrutura-de-tokenizaÃ§Ã£o)
- [ğŸ“ˆ IntegraÃ§Ã£o com Weights & Biases](#-integraÃ§Ã£o-com-weights--biases)
- [ğŸ“œ Exemplo de Dataset no Formato Final](#-exemplo-de-dataset-no-formato-final)
- [ğŸ› ï¸ Requisitos](#ï¸-requisitos)
- [ğŸ“š PossÃ­veis ExtensÃµes para Artigo](#-possÃ­veis-extensÃµes-para-artigo)
- [ğŸ“Š Metodologia e Resultados](#-metodologia-e-resultados)
- [ğŸ“„ Citar este trabalho](#-citar-este-trabalho)
- [ğŸ“œ LicenÃ§a](#-licenÃ§a)

---

## ğŸ“‚ Estrutura do Projeto
```
gpt2_chatbot_edu/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ train_small.yaml
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_prepare_datasets.py
â”‚   â”œâ”€â”€ 02_train.py
â”‚   â””â”€â”€ 03_inference.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â”œâ”€â”€ train_loop.py
â”‚   â”œâ”€â”€ chat_loop.py
â”‚   â”œâ”€â”€ templates.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ LICENSE.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Funcionalidades

- **Treinamento eficiente** com LoRA + quantizaÃ§Ã£o 8-bit
- **Formato de diÃ¡logo padronizado**:
  ```txt
  <|user|> Pergunta
  <|assistant|> Resposta
  ```
- **Limpeza de texto**: remoÃ§Ã£o de HTML, caracteres estranhos e normalizaÃ§Ã£o de espaÃ§os
- **ConversÃ£o de datasets externos** (Natural Questions, SciQ, Dolly, Alpaca)
- **Logging no W&B** de mÃ©tricas, amostras e hiperparÃ¢metros
- **InferÃªncia interativa** com stopping criteria customizado

## ğŸ“¦ InstalaÃ§Ã£o
```bash
git clone https://github.com/SEU-USUARIO/gpt2_chatbot_edu.git
cd gpt2_chatbot_edu
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## ğŸ“Š Estrutura de ConfiguraÃ§Ã£o (train_small.yaml)
*(Veja exemplo completo no repositÃ³rio)*

```yaml
project_name: chatbot-edu-gpt2
model:
  base_ckpt: gpt2
lora:
  enabled: True
train:
  epochs: 3
  lr: 5e-5
logging:
  report_to: "wandb"
```

## ğŸš€ Fluxo de ExecuÃ§Ã£o
1. **Preparar dataset**
```bash
python scripts/01_prepare_datasets.py
```
2. **Treinar modelo**
```bash
python scripts/02_train.py
```
3. **Rodar inferÃªncia**
```bash
python scripts/03_inference.py
```

## ğŸ§ª Estrutura de TokenizaÃ§Ã£o
```python
tokenized_dataset = tokenize_dataset(tokenizer, dataset)
print(tokenized_dataset["train"].column_names)  # ['input_ids', 'attention_mask']
```

## ğŸ“ˆ IntegraÃ§Ã£o com Weights & Biases
```yaml
logging:
  report_to: "wandb"
```
```python
from src.train_loop import log_samples_wandb
log_samples_wandb(trainer, tokenizer, prompts)
```

## ğŸ“œ Exemplo de Dataset no Formato Final
```json
{"dialogue": [
  {"role": "user", "content": "What is the capital of France?"},
  {"role": "assistant", "content": "Paris."}
]}
```
Formato texto:
```
<|user|> What is the capital of France?
<|assistant|> Paris.
```

## ğŸ› ï¸ Requisitos
- Python 3.10+
- transformers
- datasets
- peft
- bitsandbytes
- wandb

## ğŸ“š PossÃ­veis ExtensÃµes para Artigo
- Comparar LoRA vs Full Fine-tuning
- QuantizaÃ§Ã£o 8-bit vs 16-bit
- Avaliar em diferentes domÃ­nios
- Analisar custo computacional vs qualidade

## ğŸ“Š Metodologia e Resultados
**Metodologia:**
1. Coleta e conversÃ£o de datasets QA para formato de diÃ¡logo
2. Fine-tuning do GPT-2 com LoRA + 8-bit
3. Uso de W&B para logging e anÃ¡lise
4. ImplementaÃ§Ã£o de StoppingCriteria para evitar vazamentos

**Resultados:**
- **Perda (eval_loss)** e **Perplexidade (PPL)** melhoraram em relaÃ§Ã£o ao modelo base
- Respostas mais concisas e contextualizadas
- ReduÃ§Ã£o de custo de GPU com LoRA + 8-bit

## ğŸ“„ Citar este trabalho
```
Olavo Dalberto (2025). gpt2_chatbot_edu: Fine-tuning GPT-2 with LoRA+8-bit for dialog.
https://github.com/olavodd42/gpt2_chatbot_edu
```

## ğŸ“œ LicenÃ§a
Este projeto estÃ¡ licenciado sob a MIT License. Veja [LICENSE.md](LICENSE.md) para mais detalhes.